{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class.\", category=UserWarning, module=r\"torch\\._utils\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"promote has been superseded by promote_options='default'\", category=FutureWarning, module=r\"datasets\\.table\")\n",
    "\n",
    "# Standard library imports\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import signal\n",
    "import time\n",
    "from collections import Counter\n",
    "import traceback\n",
    "import math\n",
    "from multiprocessing import Value\n",
    "from queue import Empty\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader, DistributedSampler, TensorDataset\n",
    "import torch.optim as optim\n",
    "from torch.distributed.optim import ZeroRedundancyOptimizer\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import classification_report\n",
    "import sst\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Custom utility imports\n",
    "from utils import (\n",
    "    setup_environment, \n",
    "    prepare_device, \n",
    "    fix_random_seeds,\n",
    "    convert_numeric_to_labels, \n",
    "    convert_labels_to_tensor,\n",
    "    format_time, \n",
    "    convert_sst_label, \n",
    "    get_activation,\n",
    "    set_threads,\n",
    "    signal_handler,\n",
    "    cleanup_and_exit,\n",
    "    get_optimizer,\n",
    "    get_shape_color,\n",
    "    print_rank_memory_summary,\n",
    "    #get_scheduler\n",
    ")\n",
    "from torch_ddp_finetune_neural_classifier import TorchDDPNeuralClassifier, SentimentDataset\n",
    "from colors import *\n",
    "\n",
    "# Suppress Hugging Face library warnings\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"datasets\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"huggingface_hub.repocard\").setLevel(logging.ERROR)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from multiprocessing import Queue, Pipe, set_start_method\n",
    "\n",
    "def save_data_archive(X_train, X_dev, y_train, y_dev, X_dev_sent, world_size, device_type, data_dir):\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    # Create filename with appropriate suffix and timestamp\n",
    "    suffix = f'_{world_size}_gpu' if device_type == 'cuda' else '_1_cpu'\n",
    "    timestamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "    filename = f'data{suffix}_{timestamp}.npz'\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "    # Save data to archive file\n",
    "    np.savez_compressed(filepath, X_train=X_train, X_dev=X_dev, y_train=y_train, y_dev=y_dev, X_dev_sent=X_dev_sent)\n",
    "    print(f\"\\nData saved to: {filepath}\")\n",
    "\n",
    "def load_data_archive(data_file, device, rank):\n",
    "    load_archive_start = time.time()\n",
    "    \n",
    "    # Check if the archive file path is provided\n",
    "    if data_file is None:\n",
    "        raise ValueError(\"No archive file provided to load data from\")\n",
    "    \n",
    "    # Check if the archive file exists\n",
    "    if not os.path.exists(data_file):\n",
    "        raise FileNotFoundError(f\"Archive file not found: {data_file}\")\n",
    "    \n",
    "    # Attempt to load the data from the archive file\n",
    "    try:\n",
    "        print(f\"\\nLoading archived data from: {data_file}...\") if rank == 0 else None\n",
    "        with np.load(data_file, allow_pickle=True) as data:\n",
    "            X_train = data['X_train']\n",
    "            X_dev = data['X_dev']\n",
    "            y_train = data['y_train']\n",
    "            y_dev = data['y_dev']\n",
    "            X_dev_sent = data['X_dev_sent']\n",
    "        print(f\"Archived data loaded ({format_time(time.time() - load_archive_start)})\") if rank == 0 else None\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load data from archive file {data_file}: {str(e)}\")\n",
    "    \n",
    "    return X_train, X_dev, y_train, y_dev, X_dev_sent\n",
    "\n",
    "def initialize_bert_model(weights_name, device, rank, debug):\n",
    "    model_init_start = time.time()\n",
    "    print(f\"\\nInitializing '{weights_name}' tokenizer and model...\") if rank == 0 else None\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(weights_name)\n",
    "    bert_model = BertModel.from_pretrained(weights_name).to(device)\n",
    "    # if device.type == 'cuda':\n",
    "    #     bert_model = DDP(bert_model, device_ids=[rank], output_device=rank, static_graph=True)\n",
    "    # else:\n",
    "    #     bert_model = DDP(bert_model, device_ids=None, output_device=None, static_graph=True)\n",
    "    dist.barrier()\n",
    "    if rank == 0:\n",
    "        if debug:\n",
    "            print(f\"Bert Tokenizer:\\n{bert_tokenizer}\")\n",
    "            print(f\"Bert Model:\\n{bert_model}\")\n",
    "        print(f\"Tokenizer and model initialized ({format_time(time.time() - model_init_start)})\")\n",
    "    return bert_tokenizer, bert_model\n",
    "\n",
    "def load_data(dataset, eval_dataset, sample_percent, world_size, rank, debug):\n",
    "    data_load_start = time.time()\n",
    "\n",
    "    # Function to get a subset of data based on split name\n",
    "    def get_split(data, split):\n",
    "            if split == 'train':\n",
    "                data_split = data['train'].to_pandas()\n",
    "            elif split == 'dev':\n",
    "                data_split = data['validation'].to_pandas()\n",
    "            elif split == 'test':\n",
    "                data_split = data['test'].to_pandas()\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown split: {split}\")\n",
    "            return data_split\n",
    "    \n",
    "    def print_label_dist(dataset, label_name='label'):\n",
    "        dist = sorted(Counter(dataset[label_name]).items())\n",
    "        for k, v in dist:\n",
    "            print(f\"\\t{k.capitalize():>14s}: {v}\")\n",
    "\n",
    "    # Function to load data from Hugging Face or local based on ID and split name\n",
    "    def get_data(id, split, rank, debug):\n",
    "        # Identify the dataset and path from the ID\n",
    "        dataset_source = 'Hugging Face'\n",
    "        dataset_subset = None\n",
    "        dataset_url = None\n",
    "        if id == 'sst_local':\n",
    "            dataset_name = 'Stanford Sentiment Treebank (SST)'\n",
    "            dataset_source = 'Local'\n",
    "            dataset_path = os.path.join('data', 'sentiment')\n",
    "        elif id == 'sst':\n",
    "            dataset_name = 'Stanford Sentiment Treebank (SST)'\n",
    "            dataset_url = 'https://huggingface.co/datasets/gimmaru/SetFit-sst5'\n",
    "            dataset_path = 'SetFit/sst5'\n",
    "        elif id in ['dynasent', 'dynasent_r1']:\n",
    "            dataset_name = 'DynaSent Round 1'\n",
    "            dataset_url = 'https://huggingface.co/datasets/dynabench/dynasent'\n",
    "            dataset_path = 'dynabench/dynasent'\n",
    "            dataset_subset = 'dynabench.dynasent.r1.all'\n",
    "        elif id == 'dynasent_r2':\n",
    "            dataset_name = 'DynaSent Round 2'\n",
    "            dataset_url = 'https://huggingface.co/datasets/dynabench/dynasent'\n",
    "            dataset_path = 'dynabench/dynasent'\n",
    "            dataset_subset = 'dynabench.dynasent.r2.all'\n",
    "        elif id == 'mteb_tweet':\n",
    "            dataset_name = 'MTEB Tweet Sentiment Extraction'\n",
    "            dataset_url = 'https://huggingface.co/datasets/mteb/tweet_sentiment_extraction'\n",
    "            dataset_path = 'mteb/tweet_sentiment_extraction'\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {id}\")\n",
    "        print(f\"{split.capitalize()} Data: {dataset_name} from {dataset_source}: '{dataset_path}'\") if rank == 0 else None\n",
    "        print(f\"Dataset URL: {dataset_url}\") if dataset_url is not None and rank == 0 else None\n",
    "\n",
    "        # Load the dataset, do any pre-processing, and select appropriate split\n",
    "        if id == 'sst_local':\n",
    "            data_split = sst.train_reader(dataset_path) if split == 'train' else sst.dev_reader(dataset_path)\n",
    "        elif id == 'sst':\n",
    "            data = load_dataset(dataset_path)\n",
    "            data = data.rename_column('label', 'label_orig') \n",
    "            for split_name in ('train', 'validation', 'test'):\n",
    "                dis = [convert_sst_label(s) for s in data[split_name]['label_text']]\n",
    "                data[split_name] = data[split_name].add_column('label', dis)\n",
    "                data[split_name] = data[split_name].add_column('sentence', data[split_name]['text'])\n",
    "            data_split = get_split(data, split)\n",
    "        elif id in ['dynasent', 'dynasent_r1']:\n",
    "            data = load_dataset(dataset_path, dataset_subset)\n",
    "            data = data.rename_column('gold_label', 'label')\n",
    "            data_split = get_split(data, split)\n",
    "        elif id == 'dynasent_r2':\n",
    "            data = load_dataset(dataset_path, dataset_subset)\n",
    "            data = data.rename_column('gold_label', 'label')\n",
    "            data_split = get_split(data, split)\n",
    "        elif id == 'mteb_tweet':\n",
    "            data = load_dataset(dataset_path)\n",
    "            data = data.rename_column('label', 'label_orig') \n",
    "            data = data.rename_column('label_text', 'label')\n",
    "            data = data.rename_column('text', 'sentence')\n",
    "            split = 'test' if split == 'dev' else split\n",
    "            data_split = get_split(data, split)\n",
    "\n",
    "        return data_split\n",
    "\n",
    "    if rank == 0:\n",
    "        print(f\"\\nLoading data...\")\n",
    "        if eval_dataset is not None:\n",
    "            print(\"Using different datasets for training and evaluation\")\n",
    "        else:\n",
    "            eval_dataset = dataset\n",
    "            print(\"Using the same dataset for training and evaluation\")\n",
    "        \n",
    "        train = get_data(dataset, 'train', rank, debug)\n",
    "        dev = get_data(eval_dataset, 'dev', rank, debug)\n",
    "        \n",
    "        print(f\"Train size: {len(train)}, Dev size: {len(dev)}\")\n",
    "\n",
    "        if sample_percent is not None:\n",
    "            print(f\"Sampling {sample_percent:.0%} of data...\")\n",
    "            train = train.sample(frac=sample_percent)\n",
    "            dev = dev.sample(frac=sample_percent)\n",
    "            print(f\"Sampled Train size: {len(train)}, Sampled Dev size: {len(dev)}\")\n",
    "\n",
    "    else:\n",
    "        train = None\n",
    "        dev = None\n",
    "\n",
    "    # Broadcast the data to all ranks\n",
    "    if world_size > 1:\n",
    "        object_list = [train, dev]\n",
    "        dist.broadcast_object_list(object_list, src=0)\n",
    "        train, dev = object_list\n",
    "\n",
    "        dist.barrier()\n",
    "        print(f\"Data broadcasted to all ranks\") if rank == 0 and debug else None\n",
    "        print(f\"Rank {rank}: Train size: {len(train)}, Dev size: {len(dev)}\") if debug else None\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\"Train label distribution:\")\n",
    "        print_label_dist(train)\n",
    "        print(\"Dev label distribution:\")\n",
    "        print_label_dist(dev)\n",
    "        print(f\"Data loaded ({format_time(time.time() - data_load_start)})\")\n",
    "    dist.barrier()\n",
    "        \n",
    "    return train, dev\n",
    "\n",
    "def process_data(bert_tokenizer, bert_model, pooling, world_size, train, dev, device, batch_size, rank, debug, save_archive,\n",
    "                 save_dir, num_workers, prefetch, empty_cache, finetune_bert):\n",
    "    data_process_start = time.time()\n",
    "\n",
    "    print(f\"\\nProcessing data (Batch size: {batch_size}, Pooling: {pooling.upper() if pooling == 'cls' else pooling.capitalize()}, Fine Tune BERT: {finetune_bert})...\") if rank == 0 else None\n",
    "    print(f\"Extracting sentences and labels...\") if rank == 0 else None\n",
    "    \n",
    "    # Extract y labels\n",
    "    y_train = train.label.values\n",
    "    y_dev = dev.label.values\n",
    "    \n",
    "    # Extract X sentences\n",
    "    X_train_sent = train.sentence.values\n",
    "    X_dev_sent = dev.sentence.values\n",
    "\n",
    "    if rank == 0:\n",
    "        # Generate random indices\n",
    "        train_indices = np.random.choice(len(X_train_sent), 3, replace=False)\n",
    "        dev_indices = np.random.choice(len(X_dev_sent), 3, replace=False)\n",
    "        \n",
    "        # Collect sample sentences\n",
    "        train_samples = []\n",
    "        dev_samples = []\n",
    "        for i in train_indices:\n",
    "            train_samples.append((f'Train[{i}]: ', X_train_sent[i], f' - {y_train[i].upper()}'))\n",
    "        for i in dev_indices:\n",
    "            dev_samples.append((f'Dev[{i}]: ', X_dev_sent[i], f' - {y_dev[i].upper()}'))\n",
    "    else:\n",
    "        train_samples = None\n",
    "        dev_samples = None\n",
    "    \n",
    "    # Process X sentences (tokenize and encode with BERT) if we're not fine-tuning BERT\n",
    "    if finetune_bert:\n",
    "        # For fine-tuning, we just return the sentences\n",
    "        X_train = X_train_sent\n",
    "        X_dev = X_dev_sent\n",
    "    else:\n",
    "        # Process X sentences (tokenize and encode with BERT) for non-fine-tuning workflow\n",
    "        X_train = bert_phi(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size, train_samples, rank,\n",
    "                           debug, split='train', num_workers=num_workers, prefetch=prefetch,\n",
    "                           empty_cache=empty_cache).cpu().numpy()\n",
    "        X_dev = bert_phi(X_dev_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size, dev_samples, rank,\n",
    "                         debug, split='dev', num_workers=num_workers, prefetch=prefetch,\n",
    "                         empty_cache=empty_cache).cpu().numpy()\n",
    "    \n",
    "    # Data integrity check, make sure the sizes are consistent across ranks\n",
    "    if not finetune_bert and device.type == 'cuda' and world_size > 1:\n",
    "        # Gather sizes from all ranks\n",
    "        train_sizes = [torch.tensor(X_train.shape[0], device=device) for _ in range(world_size)]\n",
    "        dev_sizes = [torch.tensor(X_dev.shape[0], device=device) for _ in range(world_size)]\n",
    "        \n",
    "        dist.all_gather(train_sizes, train_sizes[rank])\n",
    "        dist.all_gather(dev_sizes, dev_sizes[rank])\n",
    "\n",
    "        if rank == 0:\n",
    "            # Convert to CPU for easier handling\n",
    "            train_sizes = [size.cpu().item() for size in train_sizes]\n",
    "            dev_sizes = [size.cpu().item() for size in dev_sizes]\n",
    "\n",
    "            if debug:\n",
    "                print(\"\\nDataset size summary:\")\n",
    "                print(f\"Train sizes across ranks: {train_sizes}\")\n",
    "                print(f\"Dev sizes across ranks: {dev_sizes}\")\n",
    "                \n",
    "                if len(set(train_sizes)) > 1 or len(set(dev_sizes)) > 1:\n",
    "                    print(\"WARNING: Mismatch in dataset sizes across ranks!\")\n",
    "                    print(f\"Train size mismatch: {max(train_sizes) - min(train_sizes)}\")\n",
    "                    print(f\"Dev size mismatch: {max(dev_sizes) - min(dev_sizes)}\")\n",
    "                else:\n",
    "                    print(\"All ranks have consistent dataset sizes.\")\n",
    "                \n",
    "                print(f\"Total train samples: {sum(train_sizes)}\")\n",
    "                print(f\"Total dev samples: {sum(dev_sizes)}\")\n",
    "\n",
    "            # Check for significant mismatch and raise error if necessary\n",
    "            max_mismatch = max(max(train_sizes) - min(train_sizes), max(dev_sizes) - min(dev_sizes))\n",
    "            if max_mismatch > world_size:  # Allow for small mismatches due to uneven division\n",
    "                raise ValueError(f\"Significant mismatch in dataset sizes across ranks. Max difference: {max_mismatch}\")\n",
    "\n",
    "    if save_archive and rank == 0:\n",
    "        save_data_archive(X_train, X_dev, y_train, y_dev, X_dev_sent, world_size, device.type, save_dir)\n",
    "\n",
    "    dist.barrier()\n",
    "    if rank == 0:\n",
    "        print(f\"X Train shape: {list(np.shape(X_train))}, X Dev shape: {list(np.shape(X_dev))}\")\n",
    "        print(f\"y Train shape: {list(np.shape(y_train))}, y Dev shape: {list(np.shape(y_dev))}\")\n",
    "        print(f\"Data processed ({format_time(time.time() - data_process_start)})\")\n",
    "    \n",
    "    return X_train, X_dev, y_train, y_dev, X_dev_sent\n",
    "\n",
    "def bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split, num_workers, prefetch, empty_cache):\n",
    "    encoding_start = time.time()\n",
    "    total_texts = len(texts)\n",
    "    embeddings = []\n",
    "\n",
    "    def tokenize(texts, tokenizer, device):\n",
    "        encoded = tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoded['input_ids'].to(device)\n",
    "        attention_mask = encoded['attention_mask'].to(device)\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "    def pool(last_hidden_state, attention_mask, pooling):\n",
    "        if pooling == 'cls':\n",
    "            return last_hidden_state[:, 0, :]\n",
    "        elif pooling == 'mean':\n",
    "            return (last_hidden_state * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1)\n",
    "        elif pooling == 'max':\n",
    "            return torch.max(last_hidden_state * attention_mask.unsqueeze(-1), dim=1)[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling method: {pooling}\")\n",
    "\n",
    "    # Process and display sample texts first\n",
    "    def display_sample_texts(sample_texts):\n",
    "        for text in sample_texts:\n",
    "            # Tokenize the text and get the tokens\n",
    "            tokens = tokenizer.tokenize(text[1])\n",
    "            print(f\"{text[0]}{text[1]}{text[2]}\")\n",
    "            print(f\"Tokens: {tokens}\")\n",
    "            \n",
    "            # Encode the text (including special tokens) and get embeddings\n",
    "            input_ids, attention_mask = tokenize([text[1]], tokenizer, device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            embedding = pool(outputs.last_hidden_state, attention_mask, pooling)\n",
    "            \n",
    "            print(f\"Embedding: {embedding[0, :6].cpu().numpy()} ...\")\n",
    "            print()\n",
    "\n",
    "            if device.type == 'cuda':\n",
    "                del input_ids, attention_mask, outputs, embedding\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Use DDP to distribute the encoding process across multiple GPUs\n",
    "    if device.type == 'cuda' and world_size > 1: \n",
    "        if rank == 0:\n",
    "\n",
    "            # Display sample texts\n",
    "            print(f\"\\nDisplaying samples from {split.capitalize()} data:\")\n",
    "            display_sample_texts(sample_texts)\n",
    "\n",
    "            print(f\"\\nEncoding {split.capitalize()} data of {total_texts} texts distributed across {world_size} GPUs...\")\n",
    "            print(f\"Batch Size: {batch_size}, Pooling: {pooling.upper() if pooling == 'cls' else pooling.capitalize()}, Empty Cache: {empty_cache}\")\n",
    "\n",
    "        dist.barrier()\n",
    "        # Calculate the number of texts that make the dataset evenly divisible by world_size\n",
    "        texts_per_rank = math.ceil(total_texts / world_size)\n",
    "        padded_total = texts_per_rank * world_size\n",
    "        \n",
    "        if padded_total > total_texts:\n",
    "            print(f\"Padding {split.capitalize()} data to {padded_total} texts for even distribution across {world_size} ranks...\") if rank == 0 else None\n",
    "        \n",
    "        # Calculate number of padding texts needed\n",
    "        padding_texts = padded_total - total_texts\n",
    "\n",
    "        # Create padding texts using [PAD] token\n",
    "        pad_text = tokenizer.pad_token * 10  # Arbitrary length, will be truncated if too long\n",
    "        texts_with_padding = list(texts) + [pad_text] * padding_texts  # Convert texts to list and then concatenate\n",
    "\n",
    "        # Distribute texts evenly across ranks\n",
    "        start_idx = rank * texts_per_rank\n",
    "        end_idx = start_idx + texts_per_rank\n",
    "        local_texts = texts_with_padding[start_idx:end_idx]\n",
    "        local_batch_count = len(local_texts) // batch_size + 1\n",
    "\n",
    "        batch_count = len(texts) // batch_size + 1\n",
    "        total_batches = batch_count\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"Texts per rank: {texts_per_rank}, Total batches: {total_batches}\")\n",
    "        \n",
    "        dist.barrier()\n",
    "        print(f\"Rank {rank}: Processing {len(local_texts)} texts (indices {start_idx} to {end_idx-1}) in {local_batch_count} batches...\")\n",
    "        \n",
    "        dist.barrier()\n",
    "        model.eval()\n",
    "\n",
    "        for i in range(0, len(local_texts), batch_size):\n",
    "            batch_start = time.time()\n",
    "            batch_texts = local_texts[i:i + batch_size]\n",
    "            input_ids, attention_mask = tokenize(batch_texts, tokenizer, device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            batch_embeddings = pool(outputs.last_hidden_state, attention_mask, pooling)\n",
    "            \n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "            batch_shape = list(batch_embeddings.shape)\n",
    "\n",
    "            if empty_cache:\n",
    "                # Delete the unused objects\n",
    "                del outputs, input_ids, attention_mask, batch_embeddings\n",
    "                # Empty CUDA cache\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            shape_color = get_shape_color(batch_size, batch_shape)\n",
    "            print(f\"Rank {bright_white}{bold}{rank}{reset}: Batch {bright_white}{bold}{(i // batch_size) + 1:2d}{reset} / {local_batch_count}, Shape: {shape_color}{bold}{batch_shape}{reset}, Time: {format_time(time.time() - batch_start)}\")\n",
    "\n",
    "            if rank == 0:\n",
    "                if (i // batch_size) % 5 == 0:\n",
    "                    print_rank_memory_summary(world_size, rank, all_local=True, verbose=False)\n",
    "\n",
    "        local_embeddings = torch.cat(embeddings, dim=0)\n",
    "\n",
    "        #dist.barrier()\n",
    "\n",
    "        # Gather embeddings from all processes\n",
    "        if world_size > 1:\n",
    "            gathered_embeddings = [torch.zeros_like(local_embeddings) for _ in range(world_size)]\n",
    "            dist.all_gather(gathered_embeddings, local_embeddings)\n",
    "            all_embeddings = torch.cat(gathered_embeddings, dim=0)\n",
    "            \n",
    "            if rank == 0:  # Only one process needs to do this check\n",
    "                total_embeddings = all_embeddings.shape[0]\n",
    "                padding_embeddings = total_embeddings - total_texts\n",
    "                \n",
    "                print(f\"Total embeddings: {total_embeddings}\")\n",
    "                print(f\"Original texts: {total_texts}\")\n",
    "                print(f\"Expected padding: {padding_texts}\")\n",
    "                print(f\"Actual padding: {padding_embeddings}\")\n",
    "                \n",
    "                if padding_embeddings != padding_texts:\n",
    "                    print(\"WARNING: Mismatch in padding count!\")\n",
    "                \n",
    "                if padding_embeddings > 0:\n",
    "                    # Get the embeddings of the padded texts\n",
    "                    padding_embeds = all_embeddings[-padding_embeddings:]\n",
    "                    \n",
    "                    # Calculate the maximum difference between any two padding embeddings\n",
    "                    max_diff = torch.max(torch.pdist(padding_embeds))\n",
    "                    \n",
    "                    print(f\"Maximum difference between padding embeddings: {max_diff}\")\n",
    "                    \n",
    "                    # You can adjust this threshold based on your observations\n",
    "                    if max_diff > 1e-6:\n",
    "                        print(\"WARNING: Padding embeddings are not similar.\")\n",
    "                    else:\n",
    "                        print(\"Padding embeddings verified as very similar.\")\n",
    "\n",
    "            # Now slice off the padding\n",
    "            final_embeddings = all_embeddings[:total_texts]\n",
    "        else:\n",
    "            final_embeddings = local_embeddings\n",
    "\n",
    "        dist.barrier()\n",
    "        if rank == 0:\n",
    "            print(f\"Final embeddings shape: {list(final_embeddings.shape)}\") if debug else None\n",
    "            print(f\"Encoding completed ({format_time(time.time() - encoding_start)})\")\n",
    "\n",
    "    else:\n",
    "        # Take a more straightforward approach for CPU or single GPU\n",
    "        if rank == 0:\n",
    "            device_string = 'GPU' if device.type == 'cuda' else 'CPU'\n",
    "            print(f\"\\nEncoding {split.capitalize()} data of {total_texts} texts on a single {device_string}...\")\n",
    "            print(f\"Batch Size: {batch_size}, Pooling: {pooling.upper() if pooling == 'cls' else pooling.capitalize()}, Empty Cache: {empty_cache}\")\n",
    "\n",
    "            # Display sample texts\n",
    "            print(f\"\\nDisplaying samples from {split.capitalize()} data:\")\n",
    "            display_sample_texts(sample_texts)\n",
    "\n",
    "        total_batches = len(texts) // batch_size + 1\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_start = time.time()\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            input_ids, attention_mask = tokenize(batch_texts, tokenizer, device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Perform only the selected pooling strategy for all batches\n",
    "            batch_embeddings = pool(outputs.last_hidden_state, attention_mask, pooling)\n",
    "            \n",
    "            embeddings.append(batch_embeddings)\n",
    "            final_embeddings = torch.cat(embeddings, dim=0)\n",
    "            print(f\"Batch {(i // batch_size) + 1:2d} / {total_batches}, Shape: {list(batch_embeddings.shape)}, Time: {format_time(time.time() - batch_start)}\")\n",
    "    \n",
    "            if empty_cache and device.type == 'cuda':\n",
    "                # Delete the unused objects\n",
    "                del outputs, input_ids, attention_mask, batch_embeddings\n",
    "                # Empty CUDA cache\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    return final_embeddings\n",
    "\n",
    "\n",
    "def initialize_classifier(bert_model, bert_tokenizer, finetune_bert, finetune_layers, num_layers, hidden_dim, batch_size,\n",
    "                          epochs, lr, early_stop, hidden_activation, n_iter_no_change, tol, rank, world_size, device, debug,\n",
    "                          checkpoint_dir, checkpoint_interval, resume_from_checkpoint, filename=None, use_saved_params=True,\n",
    "                          optimizer_name=None, scheduler_name=None, pooling='cls', target_score=None, interactive=False,\n",
    "                          response_pipe=None, accumulation_steps=1):\n",
    "    class_init_start = time.time()\n",
    "    print(f\"\\nInitializing DDP Neural Classifier...\") if rank == 0 else None\n",
    "    hidden_activation = get_activation(hidden_activation)\n",
    "    optimizer_class = get_optimizer(optimizer_name, device, rank, world_size)\n",
    "    #scheduler_class = get_scheduler(scheduler_name, device, rank, world_size)\n",
    "    print(f\"Layers: {num_layers}, Hidden Dim: {hidden_dim}, Hidden Act: {hidden_activation.__class__.__name__}, Optimizer: {optimizer_class.__name__}, Pooling: {pooling.upper()}, Batch Size: {batch_size}, Gradient Accumulation Steps: {accumulation_steps}\") if rank == 0 else None\n",
    "    print(f\"Max Epochs: {epochs}, LR: {lr}, Early Stop: {early_stop}, Fine-tune BERT: {finetune_bert}, Fine-tune Layers: {finetune_layers}, Target Score: {target_score}, Interactive: {interactive}\") if rank == 0 else None\n",
    "    \n",
    "    classifier = TorchDDPNeuralClassifier(\n",
    "        bert_model=bert_model,\n",
    "        bert_tokenizer=bert_tokenizer,\n",
    "        finetune_bert=finetune_bert,\n",
    "        finetune_layers=finetune_layers,\n",
    "        pooling=pooling,\n",
    "        num_layers=num_layers,\n",
    "        early_stopping=early_stop,\n",
    "        hidden_dim=hidden_dim,\n",
    "        hidden_activation=hidden_activation,\n",
    "        batch_size=batch_size,\n",
    "        max_iter=epochs,\n",
    "        n_iter_no_change=n_iter_no_change,\n",
    "        tol=tol,\n",
    "        eta=lr,\n",
    "        rank=rank,\n",
    "        debug=debug,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        checkpoint_interval=checkpoint_interval,\n",
    "        resume_from_checkpoint=resume_from_checkpoint,\n",
    "        device=device,\n",
    "        optimizer_class=optimizer_class,\n",
    "        target_score=target_score,\n",
    "        interactive=interactive,\n",
    "        response_pipe=response_pipe,\n",
    "        gradient_accumulation_steps=accumulation_steps\n",
    "    )\n",
    "\n",
    "    if filename is not None:\n",
    "        print(f\"Loading model from: {checkpoint_dir}/{filename}...\") if rank == 0 else None\n",
    "        start_epoch, model_state_dict, optimizer_state_dict = classifier.load_model(directory=checkpoint_dir, filename=filename, pattern=None, use_saved_params=use_saved_params, rank=rank, debug=debug)\n",
    "    elif resume_from_checkpoint:\n",
    "        print(\"Resuming training from the latest checkpoint...\") if rank == 0 else None\n",
    "        start_epoch, model_state_dict, optimizer_state_dict = classifier.load_model(directory=checkpoint_dir, filename=None, pattern='checkpoint_epoch', use_saved_params=use_saved_params, rank=rank, debug=debug)\n",
    "    else:\n",
    "        start_epoch = 1\n",
    "        model_state_dict = None\n",
    "        optimizer_state_dict = None\n",
    "\n",
    "    dist.barrier()\n",
    "    if rank == 0:\n",
    "        print(classifier) if debug else None\n",
    "        print(f\"Classifier initialized ({format_time(time.time() - class_init_start)})\")\n",
    "\n",
    "    return classifier, start_epoch, model_state_dict, optimizer_state_dict\n",
    "\n",
    "def evaluate_model(model, bert_tokenizer, X_dev, y_dev, label_dict, numeric_dict, world_size, device, rank, debug, save_preds,\n",
    "                   save_dir, X_dev_sent):\n",
    "    eval_start = time.time()\n",
    "    print(\"\\nEvaluating model...\") if rank == 0 else None\n",
    "    model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print(\"Making predictions...\") if rank == 0 and debug else None\n",
    "        if model.finetune_bert:\n",
    "            dataset = SentimentDataset(X_dev, [0] * len(X_dev), bert_tokenizer)  # Dummy labels\n",
    "            dataloader = DataLoader(dataset, batch_size=model.batch_size, shuffle=False)\n",
    "            preds = []\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                outputs = model.model(input_ids, attention_mask=attention_mask)\n",
    "                preds.append(outputs)\n",
    "            preds = torch.cat(preds, dim=0)\n",
    "        else:\n",
    "            if not torch.is_tensor(X_dev):\n",
    "                X_dev = torch.tensor(X_dev, device=device)\n",
    "            preds = model.model(X_dev)\n",
    "        all_preds = [torch.zeros_like(preds) for _ in range(world_size)]\n",
    "        dist.all_gather(all_preds, preds)\n",
    "        if rank == 0:\n",
    "            all_preds = torch.cat(all_preds, dim=0)[:len(y_dev)]\n",
    "            preds_labels = convert_numeric_to_labels(all_preds.argmax(dim=1).cpu().numpy(), numeric_dict)\n",
    "            print(f\"Predictions: {len(preds_labels)}, True labels: {len(y_dev)}\") if debug else None\n",
    "            # Save predictions if requested\n",
    "            if save_preds:\n",
    "                df = pd.DataFrame({\n",
    "                    'X_dev_sent': X_dev_sent,\n",
    "                    'y_dev': y_dev,\n",
    "                    'preds_labels': preds_labels\n",
    "                })\n",
    "                # Create a save directory if it doesn't exist\n",
    "                if not os.path.exists(save_dir):\n",
    "                    print(f\"Creating save directory: {save_dir}\")\n",
    "                    os.makedirs(save_dir)\n",
    "                # Create a filename timestamp\n",
    "                timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "                save_path = os.path.join(save_dir, f'predictions_{timestamp}.csv')\n",
    "                df.to_csv(save_path, index=False)\n",
    "                print(f\"Saved predictions: {save_dir}/predictions_{timestamp}.csv\")\n",
    "            print(\"\\nClassification report:\")\n",
    "            print(classification_report(y_dev, preds_labels, digits=3, zero_division=0))\n",
    "            macro_f1_score = model.score(X_dev, y_dev, device, debug)\n",
    "            print(f\"Macro F1 Score: {macro_f1_score:.2f}\")\n",
    "            print(f\"\\nEvaluation completed ({format_time(time.time() - eval_start)})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = prepare_device(0, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_environment(rank, world_size, backend, device, debug, port='12355'):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = port\n",
    "    dist.init_process_group(backend=backend, rank=rank, world_size=world_size)\n",
    "    print(f\"Rank {rank} - Device: {device}\")\n",
    "    dist.barrier()\n",
    "    if rank == 0:\n",
    "        print(f\"{world_size} process groups initialized with '{backend}' backend on {os.environ['MASTER_ADDR']}:{os.environ['MASTER_PORT']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 - Device: cpu\n",
      "1 process groups initialized with 'gloo' backend on localhost:12356\n"
     ]
    }
   ],
   "source": [
    "setup_environment(0, 1, 'gloo', device, False, '12356')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "Using the same dataset for training and evaluation\n",
      "Train Data: DynaSent Round 1 from Hugging Face: 'dynabench/dynasent'\n",
      "Dataset URL: https://huggingface.co/datasets/dynabench/dynasent\n",
      "Dev Data: DynaSent Round 1 from Hugging Face: 'dynabench/dynasent'\n",
      "Dataset URL: https://huggingface.co/datasets/dynabench/dynasent\n",
      "Train size: 80488, Dev size: 3600\n",
      "Train label distribution:\n",
      "\t      Negative: 14021\n",
      "\t       Neutral: 45076\n",
      "\t      Positive: 21391\n",
      "Dev label distribution:\n",
      "\t      Negative: 1200\n",
      "\t       Neutral: 1200\n",
      "\t      Positive: 1200\n",
      "Data loaded (2s)\n"
     ]
    }
   ],
   "source": [
    "train_dyn1, dev_dyn1 = load_data('dynasent_r1', None, None, 1, 0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "Using the same dataset for training and evaluation\n",
      "Train Data: Stanford Sentiment Treebank (SST) from Local: 'data/sentiment'\n",
      "Dev Data: Stanford Sentiment Treebank (SST) from Local: 'data/sentiment'\n",
      "Train size: 8544, Dev size: 1101\n",
      "Train label distribution:\n",
      "\t      Negative: 3310\n",
      "\t       Neutral: 1624\n",
      "\t      Positive: 3610\n",
      "Dev label distribution:\n",
      "\t      Negative: 428\n",
      "\t       Neutral: 229\n",
      "\t      Positive: 444\n",
      "Data loaded (392ms)\n"
     ]
    }
   ],
   "source": [
    "train_sst, dev_sst = load_data('sst_local', None, None, 1, 0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "Using the same dataset for training and evaluation\n",
      "Train Data: DynaSent Round 2 from Hugging Face: 'dynabench/dynasent'\n",
      "Dataset URL: https://huggingface.co/datasets/dynabench/dynasent\n",
      "Dev Data: DynaSent Round 2 from Hugging Face: 'dynabench/dynasent'\n",
      "Dataset URL: https://huggingface.co/datasets/dynabench/dynasent\n",
      "Train size: 13065, Dev size: 720\n",
      "Train label distribution:\n",
      "\t      Negative: 4579\n",
      "\t       Neutral: 2448\n",
      "\t      Positive: 6038\n",
      "Dev label distribution:\n",
      "\t      Negative: 240\n",
      "\t       Neutral: 240\n",
      "\t      Positive: 240\n",
      "Data loaded (1s)\n"
     ]
    }
   ],
   "source": [
    "train_dyn2, dev_dyn2 = load_data('dynasent_r2', None, None, 1, 0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dyn1['source'] = 'dynasent_r1'\n",
    "train_dyn2['source'] = 'dynasent_r2'\n",
    "train_sst['source'] = 'sst_local'\n",
    "dev_dyn1['source'] = 'dynasent_r1'\n",
    "dev_dyn2['source'] = 'dynasent_r2'\n",
    "dev_sst['source'] = 'sst_local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Roto-Rooter is always good when you need someo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's so worth the price of cox service over he...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I placed my order of \"sticky ribs\" as an appet...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There is mandatory valet parking, so make sure...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My wife and I couldn't finish it.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label       source\n",
       "0  Roto-Rooter is always good when you need someo...  positive  dynasent_r1\n",
       "1  It's so worth the price of cox service over he...  positive  dynasent_r1\n",
       "2  I placed my order of \"sticky ribs\" as an appet...   neutral  dynasent_r1\n",
       "3  There is mandatory valet parking, so make sure...   neutral  dynasent_r1\n",
       "4                  My wife and I couldn't finish it.   neutral  dynasent_r1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dyn1[['sentence', 'label', 'source']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We enjoyed our first and last meal in Toronto ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I tried a new place. I can't wait to return an...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The buffalo chicken was not good, but very cos...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The hotel offered complimentary breakfast.</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It work very well</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label       source\n",
       "0  We enjoyed our first and last meal in Toronto ...  positive  dynasent_r2\n",
       "1  I tried a new place. I can't wait to return an...  positive  dynasent_r2\n",
       "2  The buffalo chicken was not good, but very cos...  negative  dynasent_r2\n",
       "3         The hotel offered complimentary breakfast.  positive  dynasent_r2\n",
       "4                                  It work very well  positive  dynasent_r2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dyn2[['sentence', 'label', 'source']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence     label     source\n",
       "0    The Rock is destined to be the 21st Century 's...  positive  sst_local\n",
       "71   The gorgeously elaborate continuation of `` Th...  positive  sst_local\n",
       "144  Singer\\/composer Bryan Adams contributes a sle...  positive  sst_local\n",
       "221  You 'd think by now America would have had eno...   neutral  sst_local\n",
       "258               Yet the act is still charming here .  positive  sst_local"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sst[['sentence', 'label', 'source']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.concat([train_dyn1[['sentence', 'label', 'source']], train_dyn2[['sentence', 'label', 'source']], train_sst[['sentence', 'label', 'source']]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78907</th>\n",
       "      <td>I took the car to Audi Charlotte the next day.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56638</th>\n",
       "      <td>One highlight was that Iove the color I picked...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91210</th>\n",
       "      <td>There was one server, and he became less atten...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71599</th>\n",
       "      <td>Guess jeans are the only way to go.</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100656</th>\n",
       "      <td>Every joke is repeated at least four times .</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54270</th>\n",
       "      <td>The food was actually better than the food i h...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11674</th>\n",
       "      <td>I really didn't want to buy a brand new tire, ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>I just thought I was doomed forever.</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95959</th>\n",
       "      <td>The performances of the children , untrained i...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28011</th>\n",
       "      <td>Went here on a quick lunch break.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence     label  \\\n",
       "78907      I took the car to Audi Charlotte the next day.   neutral   \n",
       "56638   One highlight was that Iove the color I picked...  positive   \n",
       "91210   There was one server, and he became less atten...  negative   \n",
       "71599                 Guess jeans are the only way to go.  positive   \n",
       "100656       Every joke is repeated at least four times .  negative   \n",
       "54270   The food was actually better than the food i h...  positive   \n",
       "11674   I really didn't want to buy a brand new tire, ...   neutral   \n",
       "877                  I just thought I was doomed forever.  negative   \n",
       "95959   The performances of the children , untrained i...  positive   \n",
       "28011                   Went here on a quick lunch break.   neutral   \n",
       "\n",
       "             source  \n",
       "78907   dynasent_r1  \n",
       "56638   dynasent_r1  \n",
       "91210   dynasent_r2  \n",
       "71599   dynasent_r1  \n",
       "100656    sst_local  \n",
       "54270   dynasent_r1  \n",
       "11674   dynasent_r1  \n",
       "877     dynasent_r1  \n",
       "95959     sst_local  \n",
       "28011   dynasent_r1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_all = pd.concat([dev_dyn1[['sentence', 'label', 'source']], dev_dyn2[['sentence', 'label', 'source']], dev_sst[['sentence', 'label', 'source']]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>Went to this venue for an adult game night eve...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>Was told there'd be a ten minute but we weren'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>There I was with an overheated an engine one M...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>After doing a bit of research I found this pla...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>Flies inside especially near the deli section ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3117</th>\n",
       "      <td>I never have written a review before on Yelp b...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4032</th>\n",
       "      <td>I had to order the pulled chicken and some sid...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2789</th>\n",
       "      <td>Had the windshield on the Cadillac replaced by...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>It may not look like much from the outside but...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>I searched on Yelp for a reasonable and reliab...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence     label       source\n",
       "3261  Went to this venue for an adult game night eve...  positive  dynasent_r1\n",
       "1207  Was told there'd be a ten minute but we weren'...  positive  dynasent_r1\n",
       "1315  There I was with an overheated an engine one M...  negative  dynasent_r1\n",
       "1245  After doing a bit of research I found this pla...   neutral  dynasent_r1\n",
       "1749  Flies inside especially near the deli section ...  negative  dynasent_r1\n",
       "3117  I never have written a review before on Yelp b...  negative  dynasent_r1\n",
       "4032  I had to order the pulled chicken and some sid...  negative  dynasent_r2\n",
       "2789  Had the windshield on the Cadillac replaced by...   neutral  dynasent_r1\n",
       "1108  It may not look like much from the outside but...  positive  dynasent_r1\n",
       "1765  I searched on Yelp for a reasonable and reliab...   neutral  dynasent_r1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_all.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102097, 3) (5421, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_all.shape, dev_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Roto-Rooter is always good when you need someo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's so worth the price of cox service over he...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I placed my order of \"sticky ribs\" as an appet...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There is mandatory valet parking, so make sure...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My wife and I couldn't finish it.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label       source\n",
       "0  Roto-Rooter is always good when you need someo...  positive  dynasent_r1\n",
       "1  It's so worth the price of cox service over he...  positive  dynasent_r1\n",
       "2  I placed my order of \"sticky ribs\" as an appet...   neutral  dynasent_r1\n",
       "3  There is mandatory valet parking, so make sure...   neutral  dynasent_r1\n",
       "4                  My wife and I couldn't finish it.   neutral  dynasent_r1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102092</th>\n",
       "      <td>A real snooze .</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102093</th>\n",
       "      <td>No surprises .</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102094</th>\n",
       "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102095</th>\n",
       "      <td>Her fans walked out muttering words like `` ho...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102096</th>\n",
       "      <td>In this case zero .</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence     label     source\n",
       "102092                                    A real snooze .  negative  sst_local\n",
       "102093                                     No surprises .  negative  sst_local\n",
       "102094  We 've seen the hippie-turned-yuppie plot befo...  positive  sst_local\n",
       "102095  Her fans walked out muttering words like `` ho...  negative  sst_local\n",
       "102096                                In this case zero .  negative  sst_local"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle the training data and dev data\n",
    "train_all = train_all.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "dev_all = dev_all.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Those 2 drinks are part of the HK culture and ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was told by the repair company that was doin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is there to give them a good time .</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Like leafing through an album of photos accomp...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Johnny was a talker and liked to have fun.</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label       source\n",
       "0  Those 2 drinks are part of the HK culture and ...  negative  dynasent_r2\n",
       "1  I was told by the repair company that was doin...  negative  dynasent_r1\n",
       "2             It is there to give them a good time .   neutral    sst_local\n",
       "3  Like leafing through an album of photos accomp...  negative    sst_local\n",
       "4         Johnny was a talker and liked to have fun.  positive  dynasent_r1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102092</th>\n",
       "      <td>I thought this place was supposed to be good.</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102093</th>\n",
       "      <td>They claim it's because people didn't like it ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102094</th>\n",
       "      <td>There is also another marbled-out full bathroo...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102095</th>\n",
       "      <td>You put in your cell phone number &amp; select a d...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102096</th>\n",
       "      <td>I came in for a second opinion on a crown I wa...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence     label  \\\n",
       "102092      I thought this place was supposed to be good.  negative   \n",
       "102093  They claim it's because people didn't like it ...  negative   \n",
       "102094  There is also another marbled-out full bathroo...   neutral   \n",
       "102095  You put in your cell phone number & select a d...   neutral   \n",
       "102096  I came in for a second opinion on a crown I wa...   neutral   \n",
       "\n",
       "             source  \n",
       "102092  dynasent_r1  \n",
       "102093  dynasent_r1  \n",
       "102094  dynasent_r1  \n",
       "102095  dynasent_r1  \n",
       "102096  dynasent_r1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Found Thai Spoon on the Vegan Pittsburgh website.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our bill came out to around $27 and we ate lik...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>State Farm broke down the costs for me of the ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The only con for this resto is the wait to get...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We could hear the people above us stomping aro...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label       source\n",
       "0  Found Thai Spoon on the Vegan Pittsburgh website.   neutral  dynasent_r1\n",
       "1  Our bill came out to around $27 and we ate lik...  positive  dynasent_r1\n",
       "2  State Farm broke down the costs for me of the ...   neutral  dynasent_r1\n",
       "3  The only con for this resto is the wait to get...  negative  dynasent_r1\n",
       "4  We could hear the people above us stomping aro...  negative  dynasent_r1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5416</th>\n",
       "      <td>I think it's really a matter of mastering the ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5417</th>\n",
       "      <td>A bloated gasbag thesis grotesquely impressed ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5418</th>\n",
       "      <td>Its story may be a thousand years old , but wh...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5419</th>\n",
       "      <td>I felt sad for Lise not so much because of wha...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sst_local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5420</th>\n",
       "      <td>We always eat in the restaurant, so I can't co...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence     label       source\n",
       "5416  I think it's really a matter of mastering the ...   neutral  dynasent_r2\n",
       "5417  A bloated gasbag thesis grotesquely impressed ...  negative    sst_local\n",
       "5418  Its story may be a thousand years old , but wh...  negative    sst_local\n",
       "5419  I felt sad for Lise not so much because of wha...   neutral    sst_local\n",
       "5420  We always eat in the restaurant, so I can't co...   neutral  dynasent_r1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined training and dev data to CSV files\n",
    "train_all.to_csv('data/merged/train_all.csv', index=False)\n",
    "dev_all.to_csv('data/merged/dev_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
