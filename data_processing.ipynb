{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"resume_download is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use force_download=True.\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class.\", category=UserWarning, module=r\"torch\\._utils\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"promote has been superseded by promote_options='default'\", category=FutureWarning, module=r\"datasets\\.table\")\n",
    "\n",
    "# Standard library imports\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import signal\n",
    "import time\n",
    "from collections import Counter\n",
    "import traceback\n",
    "import math\n",
    "from multiprocessing import Value\n",
    "from queue import Empty\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader, DistributedSampler, TensorDataset\n",
    "import torch.optim as optim\n",
    "from torch.distributed.optim import ZeroRedundancyOptimizer\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import sst\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "\n",
    "# Custom utility imports\n",
    "from utils import (\n",
    "    setup_environment, \n",
    "    prepare_device, \n",
    "    fix_random_seeds,\n",
    "    convert_numeric_to_labels, \n",
    "    convert_labels_to_tensor,\n",
    "    format_time, \n",
    "    convert_sst_label, \n",
    "    get_activation,\n",
    "    set_threads,\n",
    "    signal_handler,\n",
    "    cleanup_and_exit,\n",
    "    get_optimizer,\n",
    "    get_shape_color,\n",
    "    print_rank_memory_summary,\n",
    "    tensor_to_numpy,\n",
    "    print_label_dist,\n",
    "    get_scheduler,\n",
    "    parse_dict\n",
    ")\n",
    "from datawaza_funcs import eval_model\n",
    "from torch_ddp_finetune_neural_classifier import TorchDDPNeuralClassifier, SentimentDataset\n",
    "from colors import *\n",
    "\n",
    "# Suppress Hugging Face library warnings\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"datasets\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"huggingface_hub.repocard\").setLevel(logging.ERROR)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from multiprocessing import Queue, Pipe, set_start_method\n",
    "\n",
    "def load_label_dicts(label_template):\n",
    "    if label_template == 'neg_neu_pos':\n",
    "        label_dict = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "        numeric_dict = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "    elif label_template == 'bin_neu':\n",
    "        label_dict = {'non-neutral': 0, 'neutral': 1}\n",
    "        numeric_dict = {0: 'non-neutral', 1: 'neutral'}\n",
    "    elif label_template == 'bin_pos':\n",
    "        label_dict = {'non-positive': 0, 'positive': 1}\n",
    "        numeric_dict = {0: 'non-positive', 1: 'positive'}\n",
    "    elif label_template == 'bin_neg':\n",
    "        label_dict = {'non-negative': 0, 'negative': 1}\n",
    "        numeric_dict = {0: 'non-negative', 1: 'negative'}\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label template: {label_template}. Options are: 'neg_neu_pos', 'bin_neu', 'bin_pos', 'bin_neg'\")\n",
    "    \n",
    "    return label_dict, numeric_dict\n",
    "\n",
    "def save_data_archive(X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent, world_size, device_type, data_dir):\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    # Create filename with appropriate suffix and timestamp\n",
    "    suffix = f'_{world_size}_gpu' if device_type == 'cuda' else '_1_cpu'\n",
    "    timestamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "    filename = f'data{suffix}_{timestamp}.npz'\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "    # Convert tensors to NumPy arrays if necessary\n",
    "    X_train = tensor_to_numpy(X_train)\n",
    "    X_val = tensor_to_numpy(X_val) if X_val is not None else None\n",
    "    X_test = tensor_to_numpy(X_test)\n",
    "    y_train = tensor_to_numpy(y_train)\n",
    "    y_val = tensor_to_numpy(y_val) if y_val is not None else None\n",
    "    y_test = tensor_to_numpy(y_test)\n",
    "\n",
    "    # Save data to archive file\n",
    "    if X_val is not None:\n",
    "        np.savez_compressed(filepath, X_train=X_train, X_val=X_val, X_test=X_test, y_train=y_train, y_val=y_val, y_test=y_test, X_test_sent=X_test_sent)\n",
    "    else:\n",
    "        np.savez_compressed(filepath, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, X_test_sent=X_test_sent)\n",
    "    print(f\"\\nData saved to: {filepath}\")\n",
    "\n",
    "def load_data_archive(data_file, device, rank, sample_percent=None):\n",
    "    load_archive_start = time.time()\n",
    "    \n",
    "    # Check if the archive file path is provided\n",
    "    if data_file is None:\n",
    "        raise ValueError(f\"{red}No archive file provided to load data from{reset}\")\n",
    "    \n",
    "    # Check if the archive file exists\n",
    "    if not os.path.exists(data_file):\n",
    "        raise FileNotFoundError(f\"{red}Archive file not found: {data_file}{reset}\")\n",
    "    \n",
    "    # Attempt to load the data from the archive file\n",
    "    try:\n",
    "        print(f\"\\n{sky_blue}Loading archived data from: {data_file}...{reset}\") if rank == 0 else None\n",
    "        with np.load(data_file, allow_pickle=True) as data:\n",
    "            X_train = data['X_train']\n",
    "            X_val = data['X_val'] if 'X_val' in data else None\n",
    "            X_test = data['X_test']\n",
    "            y_train = data['y_train']\n",
    "            y_val = data['y_val'] if 'y_val' in data else None\n",
    "            y_test = data['y_test']\n",
    "            X_test_sent = data['X_test_sent']\n",
    "        \n",
    "        # Sample data if sample_percent is provided\n",
    "        if sample_percent is not None:\n",
    "            print(f\"Sampling {sample_percent:.0%} of data...\") if rank == 0 else None\n",
    "            num_train_samples = int(len(X_train) * sample_percent)\n",
    "            num_val_samples = int(len(X_val) * sample_percent) if X_val is not None else None\n",
    "            num_test_samples = int(len(X_test) * sample_percent)\n",
    "            \n",
    "            # Create a permutation of indices\n",
    "            train_indices = np.random.permutation(len(X_train))[:num_train_samples]\n",
    "            val_indices = np.random.permutation(len(X_val))[:num_val_samples] if X_val is not None else None\n",
    "            test_indices = np.random.permutation(len(X_test))[:num_test_samples]\n",
    "            \n",
    "            # Sample the data\n",
    "            X_train = X_train[train_indices]\n",
    "            y_train = y_train[train_indices]\n",
    "            X_val = X_val[val_indices] if X_val is not None else None\n",
    "            y_val = y_val[val_indices] if y_val is not None else None\n",
    "            X_test = X_test[test_indices]\n",
    "            y_test = y_test[test_indices]\n",
    "            X_test_sent = X_test_sent[test_indices]\n",
    "            \n",
    "            if X_val is not None:\n",
    "                print(f\"Sampled Train size: {len(X_train)}, Sampled Validation size: {len(X_val)}, Sampled Evaluation size: {len(X_test)}\") if rank == 0 else None\n",
    "            else:\n",
    "                print(f\"Sampled Train size: {len(X_train)}, Sampled Evaluation size: {len(X_test)}\") if rank == 0 else None\n",
    "        \n",
    "        if rank == 0:\n",
    "            # Print a summary of the loaded data\n",
    "            print(f\"X Train shape: {list(X_train.shape)}, y Train shape: {list(y_train.shape)}\")\n",
    "            print(f\"X Validation shape: {list(X_val.shape)}, y Validation shape: {list(y_val.shape)}\") if X_val is not None else None\n",
    "            print(f\"X Test shape: {list(X_test.shape)}, y Dev shape: {list(y_test.shape)}\")\n",
    "            print(f\"X Test Sentences shape: {list(X_test_sent.shape)}\")\n",
    "            # Print label distributions\n",
    "            print(\"Train label distribution:\")\n",
    "            print_label_dist(y_train)\n",
    "            if X_val is not None:\n",
    "                print(\"Validation label distribution:\")\n",
    "                print_label_dist(y_val)\n",
    "            print(\"Test label distribution:\")\n",
    "            print_label_dist(y_test)\n",
    "        print(f\"Archived data loaded ({time.time() - load_archive_start:.2f}s)\") if rank == 0 else None\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load data from archive file {data_file}: {str(e)}\")\n",
    "        \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent\n",
    "\n",
    "def initialize_bert_model(weights_name, device, rank, debug):\n",
    "    model_init_start = time.time()\n",
    "    print(f\"\\nInitializing '{weights_name}' tokenizer and model...\") if rank == 0 else None\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(weights_name)\n",
    "    bert_model = BertModel.from_pretrained(weights_name).to(device)\n",
    "    # if device.type == 'cuda':\n",
    "    #     bert_model = DDP(bert_model, device_ids=[rank], output_device=rank, static_graph=True)\n",
    "    # else:\n",
    "    #     bert_model = DDP(bert_model, device_ids=None, output_device=None, static_graph=True)\n",
    "    dist.barrier()\n",
    "    if rank == 0:\n",
    "        if debug:\n",
    "            print(f\"Bert Tokenizer:\\n{bert_tokenizer}\")\n",
    "            print(f\"Bert Model:\\n{bert_model}\")\n",
    "        print(f\"Tokenizer and model initialized ({format_time(time.time() - model_init_start)})\")\n",
    "    return bert_tokenizer, bert_model\n",
    "\n",
    "def initialize_transformer_model(weights_name, device, rank, debug):\n",
    "    model_init_start = time.time()\n",
    "    print(f\"\\n{sky_blue}Initializing '{weights_name}' tokenizer and model...{reset}\") if rank == 0 else None\n",
    "    \n",
    "    max_retries = 3\n",
    "    retry_delay = 5\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Only rank 0 checks and downloads files\n",
    "            if rank == 0:\n",
    "                # Try loading with local_files_only first to check if files exist\n",
    "                try:\n",
    "                    print(f\"Checking for local files...\")\n",
    "                    _ = AutoTokenizer.from_pretrained(weights_name, local_files_only=True)\n",
    "                    _ = AutoModel.from_pretrained(weights_name, local_files_only=True)\n",
    "                    print(f\"Found all files in cache, skipping download\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Some files not found locally, downloading...\")\n",
    "                    # Download tokenizer files\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(weights_name, local_files_only=False)\n",
    "                    # Download model files\n",
    "                    _ = AutoModel.from_pretrained(weights_name, local_files_only=False)\n",
    "                    print(f\"Download complete\")\n",
    "            \n",
    "            # Wait for rank 0 to finish checking/downloading\n",
    "            dist.barrier()\n",
    "            \n",
    "            # Now all ranks can load from local files\n",
    "            if rank == 0:\n",
    "                print(f\"All ranks loading tokenizer from local files...\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(weights_name, local_files_only=True)\n",
    "            \n",
    "            if rank == 0:\n",
    "                print(f\"All ranks loading model from local files...\")\n",
    "            model = AutoModel.from_pretrained(weights_name, local_files_only=True).to(device)\n",
    "            \n",
    "            # Final sync point\n",
    "            dist.barrier()\n",
    "            \n",
    "            if rank == 0:\n",
    "                if debug:\n",
    "                    print(f\"Tokenizer:\\n{tokenizer}\")\n",
    "                    print(f\"Model:\\n{model}\")\n",
    "                print(f\"Tokenizer and model initialized ({format_time(time.time() - model_init_start)})\")\n",
    "            return tokenizer, model\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                if rank == 0:\n",
    "                    print(f\"\\n{yellow}Attempt {attempt + 1} failed. Retrying in {retry_delay} seconds...{reset}\")\n",
    "                    print(f\"Error: {str(e)}\")\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2  # Exponential backoff\n",
    "            else:\n",
    "                if rank == 0:\n",
    "                    print(f\"\\n{red}Failed to initialize tokenizer/model after {max_retries} attempts{reset}\")\n",
    "                raise e\n",
    "\n",
    "    raise RuntimeError(\"Failed to initialize transformer model and tokenizer\")\n",
    "\n",
    "def load_data(dataset, eval_dataset, sample_percent, eval_split, use_val_split, val_percent,\n",
    "              world_size, rank, debug):\n",
    "    data_load_start = time.time()\n",
    "\n",
    "    # Function to get a subset of data based on split name\n",
    "    def get_split(data, split):\n",
    "            split = 'validation' if split == 'dev' else split\n",
    "            if split in ['train', 'validation', 'test']:\n",
    "                data_split = data[split].to_pandas()\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown split: {split}\")\n",
    "            return data_split\n",
    "    \n",
    "    # Function to load data from Hugging Face or local based on ID and split name\n",
    "    def get_data(id, split, purpose, rank, debug):\n",
    "        # Identify the dataset and path from the ID\n",
    "        dataset_source = 'Hugging Face'\n",
    "        dataset_subset = None\n",
    "        dataset_url = None\n",
    "        if id == 'sst_local':\n",
    "            dataset_name = 'Stanford Sentiment Treebank (SST)'\n",
    "            dataset_source = 'Local'\n",
    "            dataset_path = os.path.join('data', 'sentiment')\n",
    "        elif id == 'sst':\n",
    "            dataset_name = 'Stanford Sentiment Treebank (SST)'\n",
    "            dataset_url = 'https://huggingface.co/datasets/gimmaru/SetFit-sst5'\n",
    "            dataset_path = 'SetFit/sst5'\n",
    "        elif id in ['dynasent', 'dynasent_r1']:\n",
    "            dataset_name = 'DynaSent Round 1'\n",
    "            dataset_url = 'https://huggingface.co/datasets/dynabench/dynasent'\n",
    "            dataset_path = 'dynabench/dynasent'\n",
    "            dataset_subset = 'dynabench.dynasent.r1.all'\n",
    "        elif id == 'dynasent_r2':\n",
    "            dataset_name = 'DynaSent Round 2'\n",
    "            dataset_url = 'https://huggingface.co/datasets/dynabench/dynasent'\n",
    "            dataset_path = 'dynabench/dynasent'\n",
    "            dataset_subset = 'dynabench.dynasent.r2.all'\n",
    "        elif id == 'mteb_tweet':\n",
    "            dataset_name = 'MTEB Tweet Sentiment Extraction'\n",
    "            dataset_url = 'https://huggingface.co/datasets/mteb/tweet_sentiment_extraction'\n",
    "            dataset_path = 'mteb/tweet_sentiment_extraction'\n",
    "        elif id == 'merged_local':\n",
    "            dataset_name = 'Merged DynaSent Round 1, Round 2 and SST'\n",
    "            dataset_source = 'Local'\n",
    "            dataset_path = os.path.join('data', 'merged')\n",
    "        elif id == 'merged_neutral':\n",
    "            dataset_name = 'Merged DynaSent Round 1, Round 2 and SST: Neutral Only'\n",
    "            dataset_source = 'Local'\n",
    "            dataset_path = os.path.join('data', 'merged')\n",
    "        elif id == 'merged_positive':\n",
    "            dataset_name = 'Merged DynaSent Round 1, Round 2 and SST: Positive Only'\n",
    "            dataset_source = 'Local'\n",
    "            dataset_path = os.path.join('data', 'merged')\n",
    "        elif id == 'merged_negative':\n",
    "            dataset_name = 'Merged DynaSent Round 1, Round 2 and SST: Negative Only'\n",
    "            dataset_source = 'Local'\n",
    "            dataset_path = os.path.join('data', 'merged')\n",
    "        elif id == 'merged_balanced':\n",
    "            dataset_name = 'Merged DynaSent Round 1, Round 2 and SST: Balanced'\n",
    "            dataset_source = 'Local'\n",
    "            dataset_path = os.path.join('data', 'merged')\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {id}\")\n",
    "        print(f\"{purpose} Data: {dataset_name} from {dataset_source}: '{dataset_path}'\") if rank == 0 else None\n",
    "        #print(f\"Dataset URL: {dataset_url}\") if dataset_url is not None and rank == 0 else None\n",
    "\n",
    "        # Load the dataset, do any pre-processing, and select appropriate split\n",
    "        if id == 'sst_local':\n",
    "            if split == 'train':\n",
    "                src = os.path.join(dataset_path, 'sst3-train.csv')\n",
    "                data_split = sst.sentiment_reader(src, include_subtrees=False, dedup=False)\n",
    "            elif split in ['dev', 'validation']:\n",
    "                src = os.path.join(dataset_path, 'sst3-dev.csv')\n",
    "                data_split = sst.sentiment_reader(src, include_subtrees=False, dedup=False)\n",
    "            elif split in ['test', 'test-labeled']:\n",
    "                src = os.path.join(dataset_path, 'sst3-test-labeled.csv')\n",
    "                data_split = sst.sentiment_reader(src, include_subtrees=False, dedup=False)\n",
    "            elif split == 'test-unlabeled':\n",
    "                src = os.path.join(dataset_path, 'sst3-test-unlabeled.csv')\n",
    "                data_split = sst.sentiment_reader(src, include_subtrees=False, dedup=False)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown split: {split}\")\n",
    "        elif id == 'sst':\n",
    "            data = load_dataset(dataset_path)\n",
    "            data = data.rename_column('label', 'label_orig') \n",
    "            for split_name in ('train', 'validation', 'test'):\n",
    "                dis = [convert_sst_label(s) for s in data[split_name]['label_text']]\n",
    "                data[split_name] = data[split_name].add_column('label', dis)\n",
    "                data[split_name] = data[split_name].add_column('sentence', data[split_name]['text'])\n",
    "            data_split = get_split(data, split)\n",
    "        elif id in ['dynasent', 'dynasent_r1']:\n",
    "            data = load_dataset(dataset_path, dataset_subset)\n",
    "            data = data.rename_column('gold_label', 'label')\n",
    "            data_split = get_split(data, split)\n",
    "        elif id == 'dynasent_r2':\n",
    "            data = load_dataset(dataset_path, dataset_subset)\n",
    "            data = data.rename_column('gold_label', 'label')\n",
    "            data_split = get_split(data, split)\n",
    "        elif id == 'mteb_tweet':\n",
    "            data = load_dataset(dataset_path)\n",
    "            data = data.rename_column('label', 'label_orig') \n",
    "            data = data.rename_column('label_text', 'label')\n",
    "            data = data.rename_column('text', 'sentence')\n",
    "            split = 'test' if split == 'dev' else split\n",
    "            data_split = get_split(data, split)\n",
    "        elif id == 'merged_local':\n",
    "            if split == 'train':\n",
    "                data_split = pd.read_csv(os.path.join(dataset_path, 'train_all.csv'), index_col=None)\n",
    "            elif split in ['dev', 'validation']:\n",
    "                data_split = pd.read_csv(os.path.join(dataset_path, 'val_all.csv'), index_col=None)\n",
    "            elif split == 'test':\n",
    "                data_split = pd.read_csv(os.path.join(dataset_path, 'test_all.csv'), index_col=None)\n",
    "        elif id == 'merged_balanced':\n",
    "            if split == 'train':\n",
    "                data_split = pd.read_csv(os.path.join(dataset_path, 'train_balanced.csv'), index_col=None)\n",
    "            elif split in ['dev', 'validation']:\n",
    "                data_split = pd.read_csv(os.path.join(dataset_path, 'val_all.csv'), index_col=None)\n",
    "            elif split == 'test':\n",
    "                data_split = pd.read_csv(os.path.join(dataset_path, 'test_all.csv'), index_col=None)\n",
    "        elif id == 'merged_neutral':\n",
    "            if split == 'train':\n",
    "                data_split = pd.read_csv(os.path.join(dataset_path, 'train_all_binary.csv'), index_col=None)\n",
    "                data_split = data_split.rename(columns={'label': 'label_orig'})\n",
    "                data_split = data_split.rename(columns={'neutral_label': 'label'})\n",
    "            elif split in ['dev', 'validation']:\n",
    "                data_split = pd.read_csv(os.path.join(dataset_path, 'val_all_binary.csv'), index_col=None)\n",
    "                data_split = data_split.rename(columns={'label': 'label_orig'})\n",
    "                data_split = data_split.rename(columns={'neutral_label': 'label'})\n",
    "            elif split == 'test':\n",
    "                data_split = pd.read_csv(os.path.join(dataset_path, 'test_all_binary.csv'), index_col=None)\n",
    "                data_split = data_split.rename(columns={'label': 'label_orig'})\n",
    "                data_split = data_split.rename(columns={'neutral_label': 'label'})\n",
    "        elif id == 'merged_positive':\n",
    "            if split == 'train':\n",
    "                data_split = pd.read_csv(os.path.join(dataset_path, 'train_all_binary.csv'), index_col=None)\n",
    "                data_split = data_split.rename(columns={'label': 'label_orig'})\n",
    "                data_split = data_split.rename(columns={'positive_label': 'label'})\n",
    "            elif split in ['dev', 'validation']:\n",
    "                data_split = pd.read_csv(os.path.join(dataset_path, 'val_all_binary.csv'), index_col=None)\n",
    "                data_split = data_split.rename(columns={'label': 'label_orig'})\n",
    "                data_split = data_split.rename(columns={'positive_label': 'label'})\n",
    "            elif split == 'test':\n",
    "                data_split = pd.read_csv(os.path.join(dataset_path, 'test_all_binary.csv'), index_col=None)\n",
    "                data_split = data_split.rename(columns={'label': 'label_orig'})\n",
    "                data_split = data_split.rename(columns={'positive_label': 'label'})\n",
    "        elif id == 'merged_negative':\n",
    "            if split == 'train':\n",
    "                data_split = pd.read_csv(os.path.join(dataset_path, 'train_all_binary.csv'), index_col=None)\n",
    "                data_split = data_split.rename(columns={'label': 'label_orig'})\n",
    "                data_split = data_split.rename(columns={'negative_label': 'label'})\n",
    "            elif split in ['dev', 'validation']:\n",
    "                data_split = pd.read_csv(os.path.join(dataset_path, 'val_all_binary.csv'), index_col=None)\n",
    "                data_split = data_split.rename(columns={'label': 'label_orig'})\n",
    "                data_split = data_split.rename(columns={'negative_label': 'label'})\n",
    "            elif split == 'test':\n",
    "                data_split = pd.read_csv(os.path.join(dataset_path, 'test_all_binary.csv'), index_col=None)\n",
    "                data_split = data_split.rename(columns={'label': 'label_orig'})\n",
    "                data_split = data_split.rename(columns={'negative_label': 'label'})\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {id}\")\n",
    "\n",
    "        return data_split\n",
    "\n",
    "    if rank == 0:\n",
    "        print(f\"\\n{sky_blue}Loading data...{reset}\")\n",
    "        if eval_dataset is not None:\n",
    "            print(f\"Using different datasets for training and evaluation\")\n",
    "        else:\n",
    "            eval_dataset = dataset\n",
    "            print(f\"Using the same dataset for training and evaluation\")\n",
    "        print(f\"Splits:\")\n",
    "        print(f\"- Train: Using {dataset} 'train' split\")\n",
    "        if use_val_split:\n",
    "            print(f\"- Validation: Using {dataset} 'validation' split\")\n",
    "        else:\n",
    "            print(f\"- Validation: Using {val_percent} of {dataset} 'train' split\")\n",
    "        print(f\"- Evaluation: Using {eval_dataset} '{eval_split}' split\")\n",
    "\n",
    "        train = get_data(dataset, 'train', 'Train', rank, debug)\n",
    "        if use_val_split:\n",
    "            validation = get_data(dataset, 'validation', 'Validation', rank, debug)\n",
    "        else:\n",
    "            validation = None\n",
    "        test = get_data(eval_dataset, eval_split, 'Evaluation', rank, debug)\n",
    "        \n",
    "        print(f\"Train size: {len(train)}\")\n",
    "        print(f\"Validation size: {len(validation)}\") if validation is not None else None\n",
    "        print(f\"Evaluation size: {len(test)}\")\n",
    "\n",
    "        if sample_percent is not None:\n",
    "            print(f\"Sampling {sample_percent:.0%} of data...\")\n",
    "            train = train.sample(frac=sample_percent)\n",
    "            if validation is not None:\n",
    "                validation = validation.sample(frac=sample_percent)\n",
    "            test = test.sample(frac=sample_percent)\n",
    "            print(f\"Sampled Train size: {len(train)}\")\n",
    "            print(f\"Sampled Validation size: {len(validation)}\") if validation is not None else None\n",
    "            print(f\"Sampled Evaluation size: {len(test)}\")\n",
    "\n",
    "    else:\n",
    "        train = None\n",
    "        validation = None\n",
    "        test = None\n",
    "\n",
    "    # Broadcast the data to all ranks\n",
    "    if world_size > 1:\n",
    "        object_list = [train, validation, test]\n",
    "        dist.broadcast_object_list(object_list, src=0)\n",
    "        train, validation, test = object_list\n",
    "\n",
    "        dist.barrier()\n",
    "        print(f\"Data broadcasted to all ranks\") if rank == 0 and debug else None\n",
    "        print(f\"Rank {rank}: Train size: {len(train)}, Validation size: {len(validation) if validation is not None else None}, Evaluation size: {len(test)}\") if debug else None\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\"Train label distribution:\")\n",
    "        print_label_dist(train)\n",
    "        if validation is not None:\n",
    "            print(\"Validation label distribution:\")\n",
    "            print_label_dist(validation)\n",
    "        print(\"Evaluation label distribution:\")\n",
    "        print_label_dist(test)\n",
    "        print(f\"Data loaded ({format_time(time.time() - data_load_start)})\")\n",
    "    dist.barrier()\n",
    "        \n",
    "    return train, validation, test\n",
    "\n",
    "def process_data(bert_tokenizer, bert_model, pooling, world_size, train, validation, test, device, batch_size, rank, debug, save_archive,\n",
    "                 save_dir, num_workers, prefetch, empty_cache, finetune_bert, freeze_bert, chunk_size=None):\n",
    "    data_process_start = time.time()\n",
    "\n",
    "    print(f\"\\n{sky_blue}Processing data...{reset}\") if rank == 0 else None\n",
    "    print(f\"(Batch size: {batch_size}, Pooling: {pooling.upper() if pooling == 'cls' else pooling.capitalize()}, Fine Tune BERT: {finetune_bert}, Chunk size: {chunk_size})...\") if rank == 0 else None\n",
    "    print(f\"Extracting sentences and labels...\") if rank == 0 else None\n",
    "    \n",
    "    # Extract y labels\n",
    "    y_train = train.label.values\n",
    "    y_val = validation.label.values if validation is not None else None\n",
    "    y_test = test.label.values\n",
    "    \n",
    "    # Extract X sentences\n",
    "    X_train_sent = train.sentence.values\n",
    "    X_val_sent = validation.sentence.values if validation is not None else None\n",
    "    X_test_sent = test.sentence.values\n",
    "\n",
    "    if rank == 0:\n",
    "        # Generate random indices\n",
    "        train_indices = np.random.choice(len(X_train_sent), 3, replace=False)\n",
    "        val_indices = np.random.choice(len(X_val_sent), 3, replace=False) if validation is not None else None\n",
    "        test_indices = np.random.choice(len(X_test_sent), 3, replace=False)\n",
    "        \n",
    "        # Collect sample sentences\n",
    "        train_samples = []\n",
    "        val_samples = []\n",
    "        test_samples = []\n",
    "        for i in train_indices:\n",
    "            train_samples.append((f'Train[{i}]: ', X_train_sent[i], f' - {y_train[i].upper()}'))\n",
    "        if validation is not None:\n",
    "            for i in val_indices:\n",
    "                val_samples.append((f'Validation[{i}]: ', X_val_sent[i], f' - {y_val[i].upper()}'))\n",
    "        for i in test_indices:\n",
    "            test_samples.append((f'Evaluation[{i}]: ', X_test_sent[i], f' - {y_test[i].upper()}'))\n",
    "    else:\n",
    "        train_samples = None\n",
    "        val_samples = None\n",
    "        test_samples = None\n",
    "    \n",
    "    # Process X sentences (tokenize and encode with BERT) if we're not fine-tuning BERT\n",
    "    if finetune_bert:\n",
    "        # For fine-tuning, we just return the sentences\n",
    "        X_train = X_train_sent\n",
    "        X_val = X_val_sent\n",
    "        X_test = X_test_sent\n",
    "    else:\n",
    "        # Process X sentences (tokenize and encode with BERT) for non-fine-tuning workflow\n",
    "        X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size, \n",
    "                                      train_samples, rank, debug, split='Train', num_workers=num_workers, prefetch=prefetch,\n",
    "                                      empty_cache=empty_cache, chunk_size=chunk_size)\n",
    "        X_val = process_data_chunks(X_val_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size, \n",
    "                                    val_samples, rank, debug, split='Validation', num_workers=num_workers, prefetch=prefetch,\n",
    "                                    empty_cache=empty_cache, chunk_size=chunk_size) if validation is not None else None\n",
    "        X_test = process_data_chunks(X_test_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size, \n",
    "                                    test_samples, rank, debug, split='Evaluation', num_workers=num_workers, prefetch=prefetch,\n",
    "                                    empty_cache=empty_cache, chunk_size=chunk_size)\n",
    "    \n",
    "    # Data integrity check, make sure the sizes are consistent across ranks\n",
    "    if not finetune_bert and device.type == 'cuda' and world_size > 1:\n",
    "        # Gather sizes from all ranks\n",
    "        train_sizes = [torch.tensor(X_train.shape[0], device=device) for _ in range(world_size)]\n",
    "        val_sizes = [torch.tensor(X_val.shape[0], device=device) for _ in range(world_size)] if validation is not None else None\n",
    "        test_sizes = [torch.tensor(X_test.shape[0], device=device) for _ in range(world_size)]\n",
    "        \n",
    "        dist.all_gather(train_sizes, train_sizes[rank])\n",
    "        dist.all_gather(val_sizes, val_sizes[rank]) if validation is not None else None\n",
    "        dist.all_gather(test_sizes, test_sizes[rank])\n",
    "\n",
    "        if rank == 0:\n",
    "            # Convert to CPU for easier handling\n",
    "            train_sizes = [size.cpu().item() for size in train_sizes]\n",
    "            val_sizes = [size.cpu().item() for size in val_sizes] if validation is not None else None\n",
    "            test_sizes = [size.cpu().item() for size in test_sizes]\n",
    "\n",
    "            if debug:\n",
    "                print(\"\\nDataset size summary:\")\n",
    "                print(f\"Train sizes across ranks: {train_sizes}\")\n",
    "                print(f\"Validation sizes across ranks: {val_sizes}\") if validation is not None else None\n",
    "                print(f\"Test sizes across ranks: {test_sizes}\")\n",
    "                \n",
    "                if len(set(train_sizes)) > 1 or len(set(test_sizes)) > 1 or (validation is not None and len(set(val_sizes)) > 1):\n",
    "                    print(f\"{red}WARNING: Mismatch in dataset sizes across ranks!{red}\")\n",
    "                    print(f\"Train size mismatch: {max(train_sizes) - min(train_sizes)}\")\n",
    "                    print(f\"Validation size mismatch: {max(val_sizes) - min(val_sizes)}\") if validation is not None else None\n",
    "                    print(f\"Test size mismatch: {max(test_sizes) - min(test_sizes)}\")\n",
    "                else:\n",
    "                    print(\"All ranks have consistent dataset sizes.\")\n",
    "                \n",
    "                print(f\"Total train samples: {sum(train_sizes)}\")\n",
    "                print(f\"Total validation samples: {sum(val_sizes)}\") if validation is not None else None\n",
    "                print(f\"Total test samples: {sum(test_sizes)}\")\n",
    "\n",
    "            # Check for significant mismatch and raise error if necessary\n",
    "            max_mismatch = max(max(train_sizes) - min(train_sizes), max(test_sizes) - min(test_sizes))\n",
    "            if max_mismatch > world_size:  # Allow for small mismatches due to uneven division\n",
    "                raise ValueError(f\"{red}Significant mismatch in dataset sizes across ranks. Max difference: {max_mismatch}{reset}\")\n",
    "\n",
    "    if save_archive and rank == 0:\n",
    "        save_data_archive(X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent, world_size, device.type, save_dir)\n",
    "\n",
    "    dist.barrier()\n",
    "    if rank == 0:\n",
    "        if validation is not None:\n",
    "            print(f\"X Train shape: {list(np.shape(X_train))}, X Validation shape: {list(np.shape(X_val))}, X Test shape: {list(np.shape(X_test))}\")\n",
    "            print(f\"y Train shape: {list(np.shape(y_train))}, y Validation shape: {list(np.shape(y_val))}, y Test shape: {list(np.shape(y_test))}\")\n",
    "        else:\n",
    "            print(f\"X Train shape: {list(np.shape(X_train))}, X Test shape: {list(np.shape(X_test))}\")\n",
    "            print(f\"y Train shape: {list(np.shape(y_train))}, y Test shape: {list(np.shape(y_test))}\")\n",
    "        print(f\"Data processed ({format_time(time.time() - data_process_start)})\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent\n",
    "\n",
    "def process_data_chunks(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,\n",
    "                        num_workers, prefetch, empty_cache, chunk_size=None):\n",
    "    if chunk_size is None or chunk_size >= len(texts):\n",
    "        return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,\n",
    "                        num_workers, prefetch, empty_cache)\n",
    "    \n",
    "    print(f\"\\n{sky_blue}Processing {split} data in chunks of size {chunk_size}...{reset}\") if rank == 0 else None\n",
    "    \n",
    "    all_embeddings = []\n",
    "    num_chunks = math.ceil(len(texts) / chunk_size)\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        chunk_start = i * chunk_size\n",
    "        chunk_end = min((i + 1) * chunk_size, len(texts))\n",
    "        chunk_texts = texts[chunk_start:chunk_end]\n",
    "        \n",
    "        print(f\"\\n{sky_blue}Processing chunk {i+1}/{num_chunks} (samples {chunk_start} to {chunk_end-1})...{reset}\") if rank == 0 else None\n",
    "        \n",
    "        # Only pass sample_texts for the first chunk\n",
    "        current_sample_texts = sample_texts if i == 0 else None\n",
    "        \n",
    "        chunk_embeddings = bert_phi(chunk_texts, tokenizer, model, pooling, world_size, device, batch_size, \n",
    "                                    current_sample_texts, rank, debug, f\"{split}_chunk_{i+1}\", \n",
    "                                    num_workers, prefetch, empty_cache, i+1, num_chunks)\n",
    "        \n",
    "        # Move embeddings to CPU and convert to numpy to save GPU memory\n",
    "        chunk_embeddings = chunk_embeddings.cpu().numpy()\n",
    "        all_embeddings.append(chunk_embeddings)\n",
    "        \n",
    "        # Clear CUDA cache\n",
    "        if empty_cache and device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        dist.barrier()\n",
    "    \n",
    "    # Concatenate all chunk embeddings\n",
    "    final_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    \n",
    "    print(f\"Finished processing all chunks for {split} data.\") if rank == 0 else None\n",
    "    \n",
    "    return final_embeddings\n",
    "\n",
    "\n",
    "def bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split, num_workers, prefetch, empty_cache,\n",
    "             chunk_id=None, num_chunks=None):\n",
    "    encoding_start = time.time()\n",
    "    total_texts = len(texts)\n",
    "    embeddings = []\n",
    "\n",
    "    # Ensure texts is a list\n",
    "    if isinstance(texts, np.ndarray):\n",
    "        texts = texts.tolist()\n",
    "    elif not isinstance(texts, (list, tuple)):\n",
    "        raise TypeError(f\"{red}texts must be a list, tuple, or numpy array. Got {type(texts)}{reset}\")\n",
    "    \n",
    "    def tokenize(texts, tokenizer, device):\n",
    "        # Convert NumPy array to list if necessary\n",
    "        if isinstance(texts, np.ndarray):\n",
    "            texts = texts.tolist()\n",
    "\n",
    "        encoded = tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoded['input_ids'].to(device)\n",
    "        attention_mask = encoded['attention_mask'].to(device)\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "    def pool(last_hidden_state, attention_mask, pooling):\n",
    "        if pooling == 'cls':\n",
    "            return last_hidden_state[:, 0, :]\n",
    "        elif pooling == 'mean':\n",
    "            return (last_hidden_state * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1)\n",
    "        elif pooling == 'max':\n",
    "            return torch.max(last_hidden_state * attention_mask.unsqueeze(-1), dim=1)[0]\n",
    "        else:\n",
    "            raise ValueError(f\"{red}Unknown pooling method: {pooling}{reset}\")\n",
    "\n",
    "    # Process and display sample texts first\n",
    "    def display_sample_texts(sample_texts):\n",
    "        if sample_texts is None:\n",
    "            return\n",
    "        print(f\"\\n{sky_blue}Displaying samples from {split.capitalize()} data:{reset}\")\n",
    "        for text in sample_texts:\n",
    "            # Tokenize the text and get the tokens\n",
    "            tokens = tokenizer.tokenize(text[1])\n",
    "            print(f\"{text[0]}{text[1]}{text[2]}\")\n",
    "            print(f\"Tokens: {tokens}\")\n",
    "            \n",
    "            # Encode the text (including special tokens) and get embeddings\n",
    "            input_ids, attention_mask = tokenize([text[1]], tokenizer, device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            embedding = pool(outputs.last_hidden_state, attention_mask, pooling)\n",
    "            \n",
    "            print(f\"Embedding: {embedding[0, :6].cpu().numpy()} ...\")\n",
    "            print()\n",
    "\n",
    "            if device.type == 'cuda':\n",
    "                del input_ids, attention_mask, outputs, embedding\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Use DDP to distribute the encoding process across multiple GPUs\n",
    "    if device.type == 'cuda' and world_size > 1: \n",
    "        if rank == 0:\n",
    "\n",
    "            # Display sample texts\n",
    "            display_sample_texts(sample_texts)\n",
    "\n",
    "            print(f\"\\n{sky_blue}Encoding {split.capitalize()} data of {total_texts} texts distributed across {world_size} GPUs...{reset}\")\n",
    "            print(f\"Batch Size: {batch_size}, Pooling: {pooling.upper() if pooling == 'cls' else pooling.capitalize()}, Empty Cache: {empty_cache}\")\n",
    "\n",
    "        dist.barrier()\n",
    "        # Calculate the number of texts that make the dataset evenly divisible by world_size\n",
    "        texts_per_rank = math.ceil(total_texts / world_size)\n",
    "        padded_total = texts_per_rank * world_size\n",
    "        \n",
    "        if padded_total > total_texts:\n",
    "            print(f\"Padding {split.capitalize()} data to {padded_total} texts for even distribution across {world_size} ranks...\") if rank == 0 else None\n",
    "        \n",
    "        # Calculate number of padding texts needed\n",
    "        padding_texts = padded_total - total_texts\n",
    "\n",
    "        # Create padding texts using [PAD] token\n",
    "        pad_text = tokenizer.pad_token * 10  # Arbitrary length, will be truncated if too long\n",
    "        texts_with_padding = list(texts) + [pad_text] * padding_texts  # Convert texts to list and then concatenate\n",
    "\n",
    "        # Distribute texts evenly across ranks\n",
    "        start_idx = rank * texts_per_rank\n",
    "        end_idx = start_idx + texts_per_rank\n",
    "        local_texts = texts_with_padding[start_idx:end_idx]\n",
    "        local_batch_count = len(local_texts) // batch_size + 1\n",
    "\n",
    "        batch_count = len(texts) // batch_size + 1\n",
    "        total_batches = batch_count\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"Texts per rank: {texts_per_rank}, Total batches: {total_batches}\")\n",
    "        \n",
    "        dist.barrier()\n",
    "        print(f\"Rank {rank}: Processing {len(local_texts)} texts (indices {start_idx} to {end_idx-1}) in {local_batch_count} batches...\")\n",
    "        \n",
    "        dist.barrier()\n",
    "        model.eval()\n",
    "\n",
    "        for i in range(0, len(local_texts), batch_size):\n",
    "            batch_start = time.time()\n",
    "            batch_texts = local_texts[i:i + batch_size]\n",
    "            input_ids, attention_mask = tokenize(batch_texts, tokenizer, device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            batch_embeddings = pool(outputs.last_hidden_state, attention_mask, pooling)\n",
    "            \n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "            batch_shape = list(batch_embeddings.shape)\n",
    "\n",
    "            if empty_cache:\n",
    "                # Delete the unused objects\n",
    "                del outputs, input_ids, attention_mask, batch_embeddings\n",
    "                # Empty CUDA cache\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            shape_color = get_shape_color(batch_size, batch_shape)\n",
    "            if chunk_id is not None:\n",
    "                print(f\"Rank {bright_white}{bold}{rank}{reset}: Chunk {purple}{bold}{chunk_id}{reset} / {num_chunks}, Batch {sky_blue}{bold}{(i // batch_size) + 1:2d}{reset} / {local_batch_count}, Shape: {shape_color}{bold}{batch_shape}{reset}, Time: {format_time(time.time() - batch_start)}\")\n",
    "            else:\n",
    "                print(f\"Rank {bright_white}{bold}{rank}{reset}: Batch {sky_blue}{bold}{(i // batch_size) + 1:2d}{reset} / {local_batch_count}, Shape: {shape_color}{bold}{batch_shape}{reset}, Time: {format_time(time.time() - batch_start)}\")\n",
    "\n",
    "            if rank == 0:\n",
    "                if (i // batch_size) % 5 == 0:\n",
    "                    print_rank_memory_summary(world_size, rank, all_local=True, verbose=False)\n",
    "\n",
    "        local_embeddings = torch.cat(embeddings, dim=0)\n",
    "\n",
    "        #dist.barrier()\n",
    "\n",
    "        if world_size > 1:\n",
    "            gathered_embeddings = [torch.zeros_like(local_embeddings) for _ in range(world_size)]\n",
    "            dist.all_gather(gathered_embeddings, local_embeddings)\n",
    "            all_embeddings = torch.cat(gathered_embeddings, dim=0)\n",
    "\n",
    "            if rank == 0:  # Only one process needs to do this check\n",
    "                total_embeddings = all_embeddings.shape[0]\n",
    "                padding_embeddings = total_embeddings - total_texts\n",
    "                \n",
    "                print(f\"Total embeddings: {total_embeddings}\")\n",
    "                print(f\"Original texts: {total_texts}\")\n",
    "                print(f\"Expected padding: {padding_texts}\")\n",
    "                print(f\"Actual padding: {padding_embeddings}\")\n",
    "                \n",
    "                if padding_embeddings != padding_texts:\n",
    "                    print(f\"{red}WARNING: Mismatch in padding count!{reset}\")\n",
    "\n",
    "                if padding_embeddings > 0:\n",
    "                    padding_embeds = all_embeddings[-padding_embeddings:]\n",
    "                    \n",
    "                    if padding_embeds.shape[0] > 1:  # Ensure there are at least 2 embeddings to compare\n",
    "                        max_diff = torch.max(torch.pdist(padding_embeds))\n",
    "                        print(f\"Maximum difference between padding embeddings: {max_diff}\")\n",
    "                        \n",
    "                        if max_diff > 1e-6:\n",
    "                            print(f\"{red}WARNING: Padding embeddings are not similar.{reset}\")\n",
    "                        else:\n",
    "                            print(\"Padding embeddings verified as very similar.\")\n",
    "                    else:\n",
    "                        print(f\"{yellow}Not enough padding embeddings to calculate differences (found {padding_embeds.shape[0]}).{reset}\")\n",
    "\n",
    "            # Now slice off the padding\n",
    "            final_embeddings = all_embeddings[:total_texts]\n",
    "        else:\n",
    "            final_embeddings = local_embeddings\n",
    "\n",
    "        dist.barrier()\n",
    "        if rank == 0:\n",
    "            print(f\"Final embeddings shape: {list(final_embeddings.shape)}\") if debug else None\n",
    "            print(f\"Encoding completed ({format_time(time.time() - encoding_start)})\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Take a more straightforward approach for CPU or single GPU\n",
    "        if rank == 0:\n",
    "            device_string = 'GPU' if device.type == 'cuda' else 'CPU'\n",
    "            print(f\"\\n{sky_blue}Encoding {split.capitalize()} data of {total_texts} texts on a single {device_string}...{reset}\")\n",
    "            print(f\"Batch Size: {batch_size}, Pooling: {pooling.upper() if pooling == 'cls' else pooling.capitalize()}, Empty Cache: {empty_cache}\")\n",
    "\n",
    "            # Display sample texts\n",
    "            display_sample_texts(sample_texts)\n",
    "\n",
    "        total_batches = len(texts) // batch_size + 1\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_start = time.time()\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            input_ids, attention_mask = tokenize(batch_texts, tokenizer, device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Perform only the selected pooling strategy for all batches\n",
    "            batch_embeddings = pool(outputs.last_hidden_state, attention_mask, pooling)\n",
    "            \n",
    "            embeddings.append(batch_embeddings)\n",
    "            final_embeddings = torch.cat(embeddings, dim=0)\n",
    "            print(f\"Batch {bright_white}{bold}{(i // batch_size) + 1:2d}{reset} / {total_batches}, Shape: {list(batch_embeddings.shape)}, Time: {format_time(time.time() - batch_start)}\")\n",
    "    \n",
    "            if empty_cache and device.type == 'cuda':\n",
    "                # Delete the unused objects\n",
    "                del outputs, input_ids, attention_mask, batch_embeddings\n",
    "                # Empty CUDA cache\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    return final_embeddings\n",
    "\n",
    "\n",
    "def initialize_classifier(bert_model, bert_tokenizer, finetune_bert, finetune_layers, num_layers, hidden_dim, batch_size,\n",
    "                          epochs, lr, lr_decay, early_stop, hidden_activation, n_iter_no_change, tol, rank, world_size, device, debug,\n",
    "                          checkpoint_dir, checkpoint_interval, resume_from_checkpoint, filename=None, use_saved_params=True,\n",
    "                          optimizer_name=None, use_zero=True, scheduler_name=None, l2_strength=0.0, pooling='cls',\n",
    "                          target_score=None, interactive=False, response_pipe=None, accumulation_steps=1, max_grad_norm=None,\n",
    "                          freeze_bert=False, dropout_rate=0.0, show_progress=False, advance_epochs=1, wandb_run=None, val_percent=0.1,\n",
    "                          random_seed=42, label_dict=None, optimizer_kwargs={}, scheduler_kwargs={}):\n",
    "    class_init_start = time.time()\n",
    "    print(f\"\\n{sky_blue}Initializing DDP Neural Classifier...{reset}\") if rank == 0 else None\n",
    "    hidden_activation = get_activation(hidden_activation, hidden_dim)\n",
    "    optimizer_class = get_optimizer(optimizer_name, use_zero, device, rank, world_size)\n",
    "    scheduler_class = get_scheduler(scheduler_name, device, rank, world_size)\n",
    "    print(f\"Layers: {num_layers}, Hidden Dim: {hidden_dim}, Hidden Act: {hidden_activation.__class__.__name__}, Dropout: {dropout_rate}, Optimizer: {optimizer_class.__name__}, L2 Strength: {l2_strength}, Pooling: {pooling.upper()}, Accumulation Steps: {accumulation_steps}, Max Grad Norm: {max_grad_norm}\") if rank == 0 else None\n",
    "    print(f\"Batch Size: {batch_size}, Max Epochs: {epochs}, LR: {lr}, Early Stop: {early_stop}, Fine-tune BERT: {finetune_bert}, Fine-tune Layers: {finetune_layers}, Freeze BERT: {freeze_bert}, Target Score: {target_score}, Interactive: {interactive}\") if rank == 0 else None\n",
    "    \n",
    "    classifier = TorchDDPNeuralClassifier(\n",
    "        bert_model=bert_model,\n",
    "        bert_tokenizer=bert_tokenizer,\n",
    "        finetune_bert=finetune_bert,\n",
    "        finetune_layers=finetune_layers,\n",
    "        pooling=pooling,\n",
    "        num_layers=num_layers,\n",
    "        early_stopping=early_stop,\n",
    "        hidden_dim=hidden_dim,\n",
    "        hidden_activation=hidden_activation,\n",
    "        batch_size=batch_size,\n",
    "        max_iter=epochs,\n",
    "        n_iter_no_change=n_iter_no_change,\n",
    "        tol=tol,\n",
    "        eta=lr,\n",
    "        lr_decay=lr_decay,\n",
    "        rank=rank,\n",
    "        world_size=world_size,\n",
    "        debug=debug,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        checkpoint_interval=checkpoint_interval,\n",
    "        resume_from_checkpoint=resume_from_checkpoint,\n",
    "        device=device,\n",
    "        optimizer_class=optimizer_class,\n",
    "        use_zero=use_zero,\n",
    "        scheduler_class=scheduler_class,\n",
    "        target_score=target_score,\n",
    "        interactive=interactive,\n",
    "        response_pipe=response_pipe,\n",
    "        gradient_accumulation_steps=accumulation_steps,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        freeze_bert=freeze_bert,\n",
    "        dropout_rate=dropout_rate,\n",
    "        l2_strength=l2_strength,\n",
    "        show_progress=show_progress,\n",
    "        advance_epochs=advance_epochs,\n",
    "        wandb_run=wandb_run,\n",
    "        validation_fraction=val_percent,\n",
    "        random_seed=random_seed,\n",
    "        label_dict=label_dict,\n",
    "        optimizer_kwargs=optimizer_kwargs,\n",
    "        scheduler_kwargs=scheduler_kwargs\n",
    "    )\n",
    "\n",
    "    if filename is not None:\n",
    "        print(f\"Loading model from: {checkpoint_dir}/{filename}...\") if rank == 0 else None\n",
    "        start_epoch, model_state_dict, optimizer_state_dict = classifier.load_model(directory=checkpoint_dir, filename=filename, pattern=None, use_saved_params=use_saved_params, rank=rank, debug=debug)\n",
    "    elif resume_from_checkpoint:\n",
    "        print(\"Resuming training from the latest checkpoint...\") if rank == 0 else None\n",
    "        start_epoch, model_state_dict, optimizer_state_dict = classifier.load_model(directory=checkpoint_dir, filename=None, pattern='checkpoint_epoch', use_saved_params=use_saved_params, rank=rank, debug=debug)\n",
    "    else:\n",
    "        start_epoch = 1\n",
    "        model_state_dict = None\n",
    "        optimizer_state_dict = None\n",
    "\n",
    "    dist.barrier()\n",
    "    if rank == 0:\n",
    "        print(classifier) if debug else None\n",
    "        print(f\"Classifier initialized ({format_time(time.time() - class_init_start)})\")\n",
    "\n",
    "    return classifier, start_epoch, model_state_dict, optimizer_state_dict\n",
    "\n",
    "def evaluate_model(model, bert_tokenizer, X_test, y_test, label_dict, numeric_dict, world_size, device, rank, debug, save_preds,\n",
    "                   save_dir, X_test_sent, wandb_run=None, decimal=2, pos_label=1, threshold=0.5, save_plots=False,\n",
    "                   model_name=None, weights_name=None):\n",
    "    eval_start = time.time()\n",
    "    print(f\"\\n{sky_blue}Evaluating model...{reset}\") if rank == 0 else None\n",
    "    model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print(\"Making predictions...\") if rank == 0 and debug else None\n",
    "        if model.finetune_bert:\n",
    "            dataset = SentimentDataset(X_test, [0] * len(X_test), bert_tokenizer)  # Dummy labels\n",
    "            dataloader = DataLoader(dataset, batch_size=model.batch_size, shuffle=False)\n",
    "            preds = []\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                outputs = model.model(input_ids, attention_mask=attention_mask)\n",
    "                preds.append(outputs)\n",
    "            preds = torch.cat(preds, dim=0)\n",
    "        else:\n",
    "            if not torch.is_tensor(X_test):\n",
    "                X_test = torch.tensor(X_test, device=device)\n",
    "            preds = model.model(X_test)\n",
    "        all_preds = [torch.zeros_like(preds) for _ in range(world_size)]\n",
    "        dist.all_gather(all_preds, preds)\n",
    "        if rank == 0:\n",
    "            all_preds = torch.cat(all_preds, dim=0)[:len(y_test)]\n",
    "            y_pred = convert_numeric_to_labels(all_preds.argmax(dim=1).cpu().numpy(), numeric_dict)\n",
    "            print(f\"Predictions: {len(y_pred)}, True labels: {len(y_test)}\") if debug else None\n",
    "\n",
    "            # Convert text labels to numeric labels\n",
    "            y_test_numeric = np.array([label_dict[label] for label in y_test])\n",
    "            y_pred_numeric = np.array([label_dict[label] for label in y_pred])\n",
    "\n",
    "            # Set model name based on run name\n",
    "            if model_name is None:\n",
    "                if wandb_run is not None:\n",
    "                    model_name = wandb_run.name\n",
    "                elif weights_name is not None:\n",
    "                    model_name = weights_name\n",
    "                else:\n",
    "                    model_name = 'Neural Classifier'\n",
    "            \n",
    "            # Use the DataWaza eval_model function\n",
    "            metrics = eval_model(\n",
    "                y_test=y_test_numeric,\n",
    "                y_pred=y_pred_numeric,\n",
    "                class_map=numeric_dict,\n",
    "                estimator=model,\n",
    "                x_test=X_test,\n",
    "                class_type='multi' if len(numeric_dict) > 2 else 'binary',\n",
    "                model_name=model_name,\n",
    "                plot=False,\n",
    "                save_plots=save_plots,\n",
    "                save_dir=save_dir,\n",
    "                debug=debug,\n",
    "                pos_label=pos_label,\n",
    "                decimal=decimal,\n",
    "                return_metrics=True,\n",
    "                threshold=threshold,\n",
    "                wandb_run=wandb_run\n",
    "            )\n",
    "\n",
    "            # Save predictions if requested\n",
    "            if save_preds:\n",
    "                df = pd.DataFrame({\n",
    "                    'X_test_sent': X_test_sent,\n",
    "                    'y_test': y_test,\n",
    "                    'y_pred': y_pred\n",
    "                })\n",
    "                # Create a save directory if it doesn't exist\n",
    "                if not os.path.exists(save_dir):\n",
    "                    print(f\"Creating save directory: {save_dir}\")\n",
    "                    os.makedirs(save_dir)\n",
    "                # Create a filename timestamp\n",
    "                timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "                save_path = os.path.join(save_dir, f'predictions_{timestamp}.csv')\n",
    "                df.to_csv(save_path, index=False)\n",
    "                print(f\"Saved predictions: {save_dir}/predictions_{timestamp}.csv\")\n",
    "                if wandb_run is not None:\n",
    "                    wandb_run.log({\n",
    "                        \"eval/predictions\": wandb.Table(\n",
    "                            data=[[sent, true, pred] for sent, true, pred in zip(X_test_sent, y_test, y_pred)],\n",
    "                            columns=[\"X_test_sent\", \"y_test\", \"y_pred\"]\n",
    "                        )\n",
    "                    })\n",
    "            #print(f\"\\n{bright_white}{bold}Classification report:{reset}\")\n",
    "            #print(classification_report(y_dev, preds_labels, digits=3, zero_division=0))\n",
    "            class_report = classification_report(y_test, y_pred, digits=decimal, zero_division=0, output_dict=True)\n",
    "\n",
    "            # Create a confusion matrix\n",
    "            cm = confusion_matrix(y_test, y_pred, labels=list(numeric_dict.values()))\n",
    "\n",
    "            macro_f1_score = model.score(X_test, y_test, device, debug)\n",
    "            print(f\"\\n{bright_white}{bold}Macro F1 Score:{reset} {bright_cyan}{bold}{macro_f1_score:.2f}{reset}\")\n",
    "\n",
    "            # Log evaluation metrics to Weights & Biases\n",
    "            if wandb_run is not None:\n",
    "                wandb.log({\n",
    "                    'eval/macro_f1_score': macro_f1_score,\n",
    "                    'eval/classification_report': class_report,\n",
    "                    #'eval/confusion_matrix': cm,\n",
    "                    'eval/metrics': metrics,\n",
    "                })\n",
    "\n",
    "            print(f\"\\nEvaluation completed ({format_time(time.time() - eval_start)})\")\n",
    "\n",
    "def make_predictions(classifier, tokenizer, transformer_model, predict_file, numeric_dict, rank, debug, save_dir, device, pooling,\n",
    "                     world_size, batch_size, num_workers, prefetch, empty_cache, finetune_transformer, freeze_transformer, chunk_size):\n",
    "    predictions_start = time.time()\n",
    "    print(f\"\\n{sky_blue}Predicting on unlabled test dataset...{reset}\") if rank == 0 else None\n",
    "    # Load the test dataset\n",
    "    test_df = pd.read_csv(predict_file, index_col=None)\n",
    "    test_texts = test_df.sentence.values\n",
    "    print(f\"Loaded test dataset at: {predict_file}\") if rank == 0 else None\n",
    "    print(f\"Test dataset size: {len(test_texts)}\") if rank == 0 else None\n",
    "    print(f\"Test dataset columns: {list(test_df.columns)}\") if rank == 0 else None\n",
    "    print(f\"Test dataset sample:\\n{test_df[['sentence']].sample(3)}\") if rank == 0 else None\n",
    "            \n",
    "    # Tokenize and encode the test dataset\n",
    "    if not finetune_transformer:\n",
    "        X_test = bert_phi(test_texts, tokenizer, transformer_model, pooling, world_size, device, batch_size, \n",
    "                                    None, rank, debug, 'Test', \n",
    "                                    num_workers, prefetch, empty_cache, None, None)\n",
    "    else:\n",
    "        X_test = test_texts\n",
    "\n",
    "    if rank == 0:\n",
    "        dataset = SentimentDataset(X_test, None, tokenizer)\n",
    "        dataloader = DataLoader(dataset, batch_size=classifier.batch_size, shuffle=False)\n",
    "        classifier.model.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            if finetune_transformer:\n",
    "                dataset = SentimentDataset(X_test, [0] * len(X_test), tokenizer)\n",
    "                dataloader = DataLoader(dataset, batch_size=classifier.batch_size, shuffle=False)\n",
    "                preds = []\n",
    "                for batch in dataloader:\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    outputs = classifier.model(input_ids, attention_mask=attention_mask)\n",
    "                    preds.append(outputs)\n",
    "                preds = torch.cat(preds, dim=0)\n",
    "            else:\n",
    "                if not torch.is_tensor(X_test):\n",
    "                    X_test = torch.tensor(X_test, device=device)\n",
    "                preds = classifier.model(X_test)\n",
    "            preds_labels = convert_numeric_to_labels(preds.argmax(dim=1).cpu().numpy(), numeric_dict)\n",
    "            test_df['prediction'] = preds_labels\n",
    "            print(f\"Sample test predictions:\\n{test_df[['sentence', 'prediction']].sample(3)}\")\n",
    "\n",
    "            # Create a save directory if it doesn't exist\n",
    "            if not os.path.exists(save_dir):\n",
    "                print(f\"Creating save directory: {save_dir}\")\n",
    "                os.makedirs(save_dir)\n",
    "            # Create a filename timestamp\n",
    "            timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "            save_path = os.path.join(save_dir, f'test_predictions_{timestamp}.csv')\n",
    "            test_df.to_csv(save_path, index=False)\n",
    "            \n",
    "            print(f\"Saved test predictions: {save_dir}/test_predictions_{timestamp}.csv\")\n",
    "            print(f\"Test prediction completed ({format_time(time.time() - predictions_start)})\")\n",
    "    dist.barrier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = prepare_device(0, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_environment(rank, world_size, backend, device, debug, port='12355'):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = port\n",
    "    dist.init_process_group(backend=backend, rank=rank, world_size=world_size)\n",
    "    print(f\"Rank {rank} - Device: {device}\")\n",
    "    dist.barrier()\n",
    "    if rank == 0:\n",
    "        print(f\"{world_size} process groups initialized with '{backend}' backend on {os.environ['MASTER_ADDR']}:{os.environ['MASTER_PORT']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_environment(0, 1, 'gloo', device, False, '12356')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_random_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;117mLoading data...\u001b[0m\n",
      "Using the same dataset for training and evaluation\n",
      "Splits:\n",
      "- Train: Using dynasent_r1 'train' split\n",
      "- Validation: Using dynasent_r1 'validation' split\n",
      "- Evaluation: Using dynasent_r1 'test' split\n",
      "Train Data: DynaSent Round 1 from Hugging Face: 'dynabench/dynasent'\n",
      "Validation Data: DynaSent Round 1 from Hugging Face: 'dynabench/dynasent'\n",
      "Evaluation Data: DynaSent Round 1 from Hugging Face: 'dynabench/dynasent'\n",
      "Train size: 80488\n",
      "Validation size: 3600\n",
      "Evaluation size: 3600\n",
      "Train label distribution:\n",
      "\t      Negative: 14021\n",
      "\t       Neutral: 45076\n",
      "\t      Positive: 21391\n",
      "Validation label distribution:\n",
      "\t      Negative: 1200\n",
      "\t       Neutral: 1200\n",
      "\t      Positive: 1200\n",
      "Evaluation label distribution:\n",
      "\t      Negative: 1200\n",
      "\t       Neutral: 1200\n",
      "\t      Positive: 1200\n",
      "Data loaded (2s)\n"
     ]
    }
   ],
   "source": [
    "train_dyn1, val_dyn1, test_dyn1 = load_data(dataset='dynasent_r1', eval_dataset=None, eval_split='test',\n",
    "                                            use_val_split=True, val_percent=0, sample_percent=None,\n",
    "                                            world_size=1, rank=0, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;117mLoading data...\u001b[0m\n",
      "Using the same dataset for training and evaluation\n",
      "Splits:\n",
      "- Train: Using sst_local 'train' split\n",
      "- Validation: Using sst_local 'validation' split\n",
      "- Evaluation: Using sst_local 'test' split\n",
      "Train Data: Stanford Sentiment Treebank (SST) from Local: 'data/sentiment'\n",
      "Validation Data: Stanford Sentiment Treebank (SST) from Local: 'data/sentiment'\n",
      "Evaluation Data: Stanford Sentiment Treebank (SST) from Local: 'data/sentiment'\n",
      "Train size: 8544\n",
      "Validation size: 1101\n",
      "Evaluation size: 2210\n",
      "Train label distribution:\n",
      "\t      Negative: 3310\n",
      "\t       Neutral: 1624\n",
      "\t      Positive: 3610\n",
      "Validation label distribution:\n",
      "\t      Negative: 428\n",
      "\t       Neutral: 229\n",
      "\t      Positive: 444\n",
      "Evaluation label distribution:\n",
      "\t      Negative: 912\n",
      "\t       Neutral: 389\n",
      "\t      Positive: 909\n",
      "Data loaded (276ms)\n"
     ]
    }
   ],
   "source": [
    "train_sst, val_sst, test_sst = load_data(dataset='sst_local', eval_dataset=None, eval_split='test',\n",
    "                                            use_val_split=True, val_percent=0, sample_percent=None,\n",
    "                                            world_size=1, rank=0, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;117mLoading data...\u001b[0m\n",
      "Using the same dataset for training and evaluation\n",
      "Splits:\n",
      "- Train: Using dynasent_r2 'train' split\n",
      "- Validation: Using dynasent_r2 'validation' split\n",
      "- Evaluation: Using dynasent_r2 'test' split\n",
      "Train Data: DynaSent Round 2 from Hugging Face: 'dynabench/dynasent'\n",
      "Validation Data: DynaSent Round 2 from Hugging Face: 'dynabench/dynasent'\n",
      "Evaluation Data: DynaSent Round 2 from Hugging Face: 'dynabench/dynasent'\n",
      "Train size: 13065\n",
      "Validation size: 720\n",
      "Evaluation size: 720\n",
      "Train label distribution:\n",
      "\t      Negative: 4579\n",
      "\t       Neutral: 2448\n",
      "\t      Positive: 6038\n",
      "Validation label distribution:\n",
      "\t      Negative: 240\n",
      "\t       Neutral: 240\n",
      "\t      Positive: 240\n",
      "Evaluation label distribution:\n",
      "\t      Negative: 240\n",
      "\t       Neutral: 240\n",
      "\t      Positive: 240\n",
      "Data loaded (2s)\n"
     ]
    }
   ],
   "source": [
    "train_dyn2, val_dyn2, test_dyn2 = load_data(dataset='dynasent_r2', eval_dataset=None, eval_split='test',\n",
    "                                            use_val_split=True, val_percent=0, sample_percent=None,\n",
    "                                            world_size=1, rank=0, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dyn1['source'] = 'dynasent_r1'\n",
    "train_dyn2['source'] = 'dynasent_r2'\n",
    "train_sst['source'] = 'sst_local'\n",
    "val_dyn1['source'] = 'dynasent_r1'\n",
    "val_dyn2['source'] = 'dynasent_r2'\n",
    "val_sst['source'] = 'sst_local'\n",
    "test_dyn1['source'] = 'dynasent_r1'\n",
    "test_dyn2['source'] = 'dynasent_r2'\n",
    "test_sst['source'] = 'sst_local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dyn1['split'] = 'train'\n",
    "train_dyn2['split'] = 'train'\n",
    "train_sst['split'] = 'train'\n",
    "val_dyn1['split'] = 'validation'\n",
    "val_dyn2['split'] = 'validation'\n",
    "val_sst['split'] = 'validation'\n",
    "test_dyn1['split'] = 'test'\n",
    "test_dyn2['split'] = 'test'\n",
    "test_sst['split'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Roto-Rooter is always good when you need someo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's so worth the price of cox service over he...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I placed my order of \"sticky ribs\" as an appet...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There is mandatory valet parking, so make sure...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My wife and I couldn't finish it.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label       source  \\\n",
       "0  Roto-Rooter is always good when you need someo...  positive  dynasent_r1   \n",
       "1  It's so worth the price of cox service over he...  positive  dynasent_r1   \n",
       "2  I placed my order of \"sticky ribs\" as an appet...   neutral  dynasent_r1   \n",
       "3  There is mandatory valet parking, so make sure...   neutral  dynasent_r1   \n",
       "4                  My wife and I couldn't finish it.   neutral  dynasent_r1   \n",
       "\n",
       "   split  \n",
       "0  train  \n",
       "1  train  \n",
       "2  train  \n",
       "3  train  \n",
       "4  train  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dyn1[['sentence', 'label', 'source', 'split']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We enjoyed our first and last meal in Toronto ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I tried a new place. I can't wait to return an...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The buffalo chicken was not good, but very cos...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The hotel offered complimentary breakfast.</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It work very well</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label       source  \\\n",
       "0  We enjoyed our first and last meal in Toronto ...  positive  dynasent_r2   \n",
       "1  I tried a new place. I can't wait to return an...  positive  dynasent_r2   \n",
       "2  The buffalo chicken was not good, but very cos...  negative  dynasent_r2   \n",
       "3         The hotel offered complimentary breakfast.  positive  dynasent_r2   \n",
       "4                                  It work very well  positive  dynasent_r2   \n",
       "\n",
       "   split  \n",
       "0  train  \n",
       "1  train  \n",
       "2  train  \n",
       "3  train  \n",
       "4  train  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dyn2[['sentence', 'label', 'source', 'split']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence     label     source  \\\n",
       "0    The Rock is destined to be the 21st Century 's...  positive  sst_local   \n",
       "71   The gorgeously elaborate continuation of `` Th...  positive  sst_local   \n",
       "144  Singer\\/composer Bryan Adams contributes a sle...  positive  sst_local   \n",
       "221  You 'd think by now America would have had eno...   neutral  sst_local   \n",
       "258               Yet the act is still charming here .  positive  sst_local   \n",
       "\n",
       "     split  \n",
       "0    train  \n",
       "71   train  \n",
       "144  train  \n",
       "221  train  \n",
       "258  train  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sst[['sentence', 'label', 'source', 'split']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.concat([train_dyn1[['sentence', 'label', 'source', 'split']], train_dyn2[['sentence', 'label', 'source', 'split']], train_sst[['sentence', 'label', 'source', 'split']]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84457</th>\n",
       "      <td>Those 2 drinks are part of the HK culture and ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33315</th>\n",
       "      <td>I was told by the repair company that was doin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95755</th>\n",
       "      <td>It is there to give them a good time .</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99353</th>\n",
       "      <td>Like leafing through an album of photos accomp...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23628</th>\n",
       "      <td>Johnny was a talker and liked to have fun.</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81277</th>\n",
       "      <td>It as burnt to a crisp black flavorless</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61427</th>\n",
       "      <td>I called Moveaholics Jason was amazing he even...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13393</th>\n",
       "      <td>Is this place expensive?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11373</th>\n",
       "      <td>It's likely crowded at the busier times so kee...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59563</th>\n",
       "      <td>I was just looking at bottom line price and my...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence     label  \\\n",
       "84457  Those 2 drinks are part of the HK culture and ...  negative   \n",
       "33315  I was told by the repair company that was doin...  negative   \n",
       "95755             It is there to give them a good time .   neutral   \n",
       "99353  Like leafing through an album of photos accomp...  negative   \n",
       "23628         Johnny was a talker and liked to have fun.  positive   \n",
       "81277            It as burnt to a crisp black flavorless  negative   \n",
       "61427  I called Moveaholics Jason was amazing he even...  positive   \n",
       "13393                           Is this place expensive?   neutral   \n",
       "11373  It's likely crowded at the busier times so kee...   neutral   \n",
       "59563  I was just looking at bottom line price and my...   neutral   \n",
       "\n",
       "            source  split  \n",
       "84457  dynasent_r2  train  \n",
       "33315  dynasent_r1  train  \n",
       "95755    sst_local  train  \n",
       "99353    sst_local  train  \n",
       "23628  dynasent_r1  train  \n",
       "81277  dynasent_r2  train  \n",
       "61427  dynasent_r1  train  \n",
       "13393  dynasent_r1  train  \n",
       "11373  dynasent_r1  train  \n",
       "59563  dynasent_r1  train  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_all = pd.concat([val_dyn1[['sentence', 'label', 'source', 'split']], val_dyn2[['sentence', 'label', 'source', 'split']], val_sst[['sentence', 'label', 'source', 'split']]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>I do like Tasty Asian kitchen across the street.</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>Sadly, they get mixed up with the other locati...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5213</th>\n",
       "      <td>Professionally speaking , it 's tempting to ju...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3691</th>\n",
       "      <td>The dining was sublime.</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>Came back to Elara around 4am.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>You dont have to miss church and the game. tru...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>As long as I can get my job done, I'll wait.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2427</th>\n",
       "      <td>They are doing themselves a great disservice b...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5400</th>\n",
       "      <td>The end result is a film that 's neither .</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>I HAVE NEVER BEEN SO APPALLED BY THE MEDICAL C...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence     label  \\\n",
       "1381   I do like Tasty Asian kitchen across the street.  positive   \n",
       "2337  Sadly, they get mixed up with the other locati...  negative   \n",
       "5213  Professionally speaking , it 's tempting to ju...  negative   \n",
       "3691                            The dining was sublime.  positive   \n",
       "1234                     Came back to Elara around 4am.   neutral   \n",
       "4091  You dont have to miss church and the game. tru...   neutral   \n",
       "2786       As long as I can get my job done, I'll wait.   neutral   \n",
       "2427  They are doing themselves a great disservice b...  negative   \n",
       "5400         The end result is a film that 's neither .  negative   \n",
       "1345  I HAVE NEVER BEEN SO APPALLED BY THE MEDICAL C...  negative   \n",
       "\n",
       "           source       split  \n",
       "1381  dynasent_r1  validation  \n",
       "2337  dynasent_r1  validation  \n",
       "5213    sst_local  validation  \n",
       "3691  dynasent_r2  validation  \n",
       "1234  dynasent_r1  validation  \n",
       "4091  dynasent_r2  validation  \n",
       "2786  dynasent_r1  validation  \n",
       "2427  dynasent_r1  validation  \n",
       "5400    sst_local  validation  \n",
       "1345  dynasent_r1  validation  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_all.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all = pd.concat([test_dyn1[['sentence', 'label', 'source', 'split']], test_dyn2[['sentence', 'label', 'source', 'split']], test_sst[['sentence', 'label', 'source', 'split']]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>If I lived on the Strip, this bar would be my ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>A recent favourite at Sundance , this white-tr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6511</th>\n",
       "      <td>Rainy days and movies about the disintegration...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>As our appetizers arrived (hummus for hubby an...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3859</th>\n",
       "      <td>That really sucks, was it your fault?</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3674</th>\n",
       "      <td>Hence, that's why they do not take walk ins.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040</th>\n",
       "      <td>We checked in around 12:30pm and had some frie...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>HOOD RAT, HOOD RAT, HOOCHIE MAMA!</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4380</th>\n",
       "      <td>Disney has always been hit-or-miss when bringi...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>Let me start by saying the food is good.</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence     label  \\\n",
       "934   If I lived on the Strip, this bar would be my ...  positive   \n",
       "5091  A recent favourite at Sundance , this white-tr...  positive   \n",
       "6511  Rainy days and movies about the disintegration...  negative   \n",
       "57    As our appetizers arrived (hummus for hubby an...  positive   \n",
       "3859              That really sucks, was it your fault?  negative   \n",
       "3674       Hence, that's why they do not take walk ins.   neutral   \n",
       "2040  We checked in around 12:30pm and had some frie...  positive   \n",
       "586                   HOOD RAT, HOOD RAT, HOOCHIE MAMA!  negative   \n",
       "4380  Disney has always been hit-or-miss when bringi...   neutral   \n",
       "1900           Let me start by saying the food is good.  positive   \n",
       "\n",
       "           source split  \n",
       "934   dynasent_r1  test  \n",
       "5091    sst_local  test  \n",
       "6511    sst_local  test  \n",
       "57    dynasent_r1  test  \n",
       "3859  dynasent_r2  test  \n",
       "3674  dynasent_r2  test  \n",
       "2040  dynasent_r1  test  \n",
       "586   dynasent_r1  test  \n",
       "4380    sst_local  test  \n",
       "1900  dynasent_r1  test  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102097, 4) (5421, 4) (6530, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_all.shape, val_all.shape, test_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Roto-Rooter is always good when you need someo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's so worth the price of cox service over he...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I placed my order of \"sticky ribs\" as an appet...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There is mandatory valet parking, so make sure...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My wife and I couldn't finish it.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label       source  \\\n",
       "0  Roto-Rooter is always good when you need someo...  positive  dynasent_r1   \n",
       "1  It's so worth the price of cox service over he...  positive  dynasent_r1   \n",
       "2  I placed my order of \"sticky ribs\" as an appet...   neutral  dynasent_r1   \n",
       "3  There is mandatory valet parking, so make sure...   neutral  dynasent_r1   \n",
       "4                  My wife and I couldn't finish it.   neutral  dynasent_r1   \n",
       "\n",
       "   split  \n",
       "0  train  \n",
       "1  train  \n",
       "2  train  \n",
       "3  train  \n",
       "4  train  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102092</th>\n",
       "      <td>A real snooze .</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102093</th>\n",
       "      <td>No surprises .</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102094</th>\n",
       "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102095</th>\n",
       "      <td>Her fans walked out muttering words like `` ho...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102096</th>\n",
       "      <td>In this case zero .</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence     label  \\\n",
       "102092                                    A real snooze .  negative   \n",
       "102093                                     No surprises .  negative   \n",
       "102094  We 've seen the hippie-turned-yuppie plot befo...  positive   \n",
       "102095  Her fans walked out muttering words like `` ho...  negative   \n",
       "102096                                In this case zero .  negative   \n",
       "\n",
       "           source  split  \n",
       "102092  sst_local  train  \n",
       "102093  sst_local  train  \n",
       "102094  sst_local  train  \n",
       "102095  sst_local  train  \n",
       "102096  sst_local  train  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle the training data and dev data\n",
    "train_all = train_all.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_all = val_all.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_all = test_all.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Those 2 drinks are part of the HK culture and ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was told by the repair company that was doin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is there to give them a good time .</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Like leafing through an album of photos accomp...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Johnny was a talker and liked to have fun.</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label       source  \\\n",
       "0  Those 2 drinks are part of the HK culture and ...  negative  dynasent_r2   \n",
       "1  I was told by the repair company that was doin...  negative  dynasent_r1   \n",
       "2             It is there to give them a good time .   neutral    sst_local   \n",
       "3  Like leafing through an album of photos accomp...  negative    sst_local   \n",
       "4         Johnny was a talker and liked to have fun.  positive  dynasent_r1   \n",
       "\n",
       "   split  \n",
       "0  train  \n",
       "1  train  \n",
       "2  train  \n",
       "3  train  \n",
       "4  train  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102092</th>\n",
       "      <td>I thought this place was supposed to be good.</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102093</th>\n",
       "      <td>They claim it's because people didn't like it ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102094</th>\n",
       "      <td>There is also another marbled-out full bathroo...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102095</th>\n",
       "      <td>You put in your cell phone number &amp; select a d...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102096</th>\n",
       "      <td>I came in for a second opinion on a crown I wa...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence     label  \\\n",
       "102092      I thought this place was supposed to be good.  negative   \n",
       "102093  They claim it's because people didn't like it ...  negative   \n",
       "102094  There is also another marbled-out full bathroo...   neutral   \n",
       "102095  You put in your cell phone number & select a d...   neutral   \n",
       "102096  I came in for a second opinion on a crown I wa...   neutral   \n",
       "\n",
       "             source  split  \n",
       "102092  dynasent_r1  train  \n",
       "102093  dynasent_r1  train  \n",
       "102094  dynasent_r1  train  \n",
       "102095  dynasent_r1  train  \n",
       "102096  dynasent_r1  train  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Found Thai Spoon on the Vegan Pittsburgh website.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our bill came out to around $27 and we ate lik...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>State Farm broke down the costs for me of the ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The only con for this resto is the wait to get...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We could hear the people above us stomping aro...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label       source  \\\n",
       "0  Found Thai Spoon on the Vegan Pittsburgh website.   neutral  dynasent_r1   \n",
       "1  Our bill came out to around $27 and we ate lik...  positive  dynasent_r1   \n",
       "2  State Farm broke down the costs for me of the ...   neutral  dynasent_r1   \n",
       "3  The only con for this resto is the wait to get...  negative  dynasent_r1   \n",
       "4  We could hear the people above us stomping aro...  negative  dynasent_r1   \n",
       "\n",
       "        split  \n",
       "0  validation  \n",
       "1  validation  \n",
       "2  validation  \n",
       "3  validation  \n",
       "4  validation  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5416</th>\n",
       "      <td>I think it's really a matter of mastering the ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5417</th>\n",
       "      <td>A bloated gasbag thesis grotesquely impressed ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5418</th>\n",
       "      <td>Its story may be a thousand years old , but wh...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5419</th>\n",
       "      <td>I felt sad for Lise not so much because of wha...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5420</th>\n",
       "      <td>We always eat in the restaurant, so I can't co...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence     label  \\\n",
       "5416  I think it's really a matter of mastering the ...   neutral   \n",
       "5417  A bloated gasbag thesis grotesquely impressed ...  negative   \n",
       "5418  Its story may be a thousand years old , but wh...  negative   \n",
       "5419  I felt sad for Lise not so much because of wha...   neutral   \n",
       "5420  We always eat in the restaurant, so I can't co...   neutral   \n",
       "\n",
       "           source       split  \n",
       "5416  dynasent_r2  validation  \n",
       "5417    sst_local  validation  \n",
       "5418    sst_local  validation  \n",
       "5419    sst_local  validation  \n",
       "5420  dynasent_r1  validation  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I had called in advance to see if they offered...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EVERY SINGLE ITEM WAS INEDIBLE.</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rooms are small.</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Without resorting to hyperbole , I can state t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Used a $12 Groupon deal (to cover $20 meal).</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label       source  \\\n",
       "0  I had called in advance to see if they offered...  positive  dynasent_r1   \n",
       "1                    EVERY SINGLE ITEM WAS INEDIBLE.  negative  dynasent_r1   \n",
       "2                                   Rooms are small.  negative  dynasent_r1   \n",
       "3  Without resorting to hyperbole , I can state t...  positive    sst_local   \n",
       "4       Used a $12 Groupon deal (to cover $20 meal).   neutral  dynasent_r1   \n",
       "\n",
       "  split  \n",
       "0  test  \n",
       "1  test  \n",
       "2  test  \n",
       "3  test  \n",
       "4  test  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6525</th>\n",
       "      <td>I went back in to ask for cilantro dressing th...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6526</th>\n",
       "      <td>Here , Adrian Lyne comes as close to profundit...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6527</th>\n",
       "      <td>The actors are so terrific at conveying their ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6528</th>\n",
       "      <td>It should be mentioned that the set design and...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6529</th>\n",
       "      <td>She greeted customers by holding the scanner t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence     label  \\\n",
       "6525  I went back in to ask for cilantro dressing th...  positive   \n",
       "6526  Here , Adrian Lyne comes as close to profundit...  positive   \n",
       "6527  The actors are so terrific at conveying their ...   neutral   \n",
       "6528  It should be mentioned that the set design and...  positive   \n",
       "6529  She greeted customers by holding the scanner t...  negative   \n",
       "\n",
       "           source split  \n",
       "6525  dynasent_r2  test  \n",
       "6526    sst_local  test  \n",
       "6527    sst_local  test  \n",
       "6528    sst_local  test  \n",
       "6529  dynasent_r1  test  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined training and dev data to CSV files\n",
    "train_all.to_csv('data/merged/train_all.csv', index=False)\n",
    "val_all.to_csv('data/merged/val_all.csv', index=False)\n",
    "test_all.to_csv('data/merged/test_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_df = pd.read_csv('data/merged/train_all.csv')\n",
    "val_all_df = pd.read_csv('data/merged/val_all.csv')\n",
    "test_all_df = pd.read_csv('data/merged/test_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102097, 4) (5421, 4) (6530, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_all_df.shape, val_all_df.shape, test_all_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Those 2 drinks are part of the HK culture and ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was told by the repair company that was doin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is there to give them a good time .</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Like leafing through an album of photos accomp...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Johnny was a talker and liked to have fun.</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label       source  \\\n",
       "0  Those 2 drinks are part of the HK culture and ...  negative  dynasent_r2   \n",
       "1  I was told by the repair company that was doin...  negative  dynasent_r1   \n",
       "2             It is there to give them a good time .   neutral    sst_local   \n",
       "3  Like leafing through an album of photos accomp...  negative    sst_local   \n",
       "4         Johnny was a talker and liked to have fun.  positive  dynasent_r1   \n",
       "\n",
       "   split  \n",
       "0  train  \n",
       "1  train  \n",
       "2  train  \n",
       "3  train  \n",
       "4  train  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary classification dataset for neutral vs. non-neutral\n",
    "train_all_df['neutral_label'] = train_all_df['label'].apply(lambda x: 'neutral' if x == 'neutral' else 'non-neutral')\n",
    "val_all_df['neutral_label'] = val_all_df['label'].apply(lambda x: 'neutral' if x == 'neutral' else 'non-neutral')\n",
    "test_all_df['neutral_label'] = test_all_df['label'].apply(lambda x: 'neutral' if x == 'neutral' else 'non-neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary classification dataset for positive vs. non-positive\n",
    "train_all_df['positive_label'] = train_all_df['label'].apply(lambda x: 'positive' if x == 'positive' else 'non-positive')\n",
    "val_all_df['positive_label'] = val_all_df['label'].apply(lambda x: 'positive' if x == 'positive' else 'non-positive')\n",
    "test_all_df['positive_label'] = test_all_df['label'].apply(lambda x: 'positive' if x == 'positive' else 'non-positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary classification dataset for negative vs. non-negative\n",
    "train_all_df['negative_label'] = train_all_df['label'].apply(lambda x: 'negative' if x == 'negative' else 'non-negative')\n",
    "val_all_df['negative_label'] = val_all_df['label'].apply(lambda x: 'negative' if x == 'negative' else 'non-negative')\n",
    "test_all_df['negative_label'] = test_all_df['label'].apply(lambda x: 'negative' if x == 'negative' else 'non-negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "      <th>neutral_label</th>\n",
       "      <th>positive_label</th>\n",
       "      <th>negative_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Those 2 drinks are part of the HK culture and ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>train</td>\n",
       "      <td>non-neutral</td>\n",
       "      <td>non-positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was told by the repair company that was doin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "      <td>non-neutral</td>\n",
       "      <td>non-positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is there to give them a good time .</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>non-positive</td>\n",
       "      <td>non-negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Like leafing through an album of photos accomp...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "      <td>non-neutral</td>\n",
       "      <td>non-positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Johnny was a talker and liked to have fun.</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "      <td>non-neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>non-negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label       source  \\\n",
       "0  Those 2 drinks are part of the HK culture and ...  negative  dynasent_r2   \n",
       "1  I was told by the repair company that was doin...  negative  dynasent_r1   \n",
       "2             It is there to give them a good time .   neutral    sst_local   \n",
       "3  Like leafing through an album of photos accomp...  negative    sst_local   \n",
       "4         Johnny was a talker and liked to have fun.  positive  dynasent_r1   \n",
       "\n",
       "   split neutral_label positive_label negative_label  \n",
       "0  train   non-neutral   non-positive       negative  \n",
       "1  train   non-neutral   non-positive       negative  \n",
       "2  train       neutral   non-positive   non-negative  \n",
       "3  train   non-neutral   non-positive       negative  \n",
       "4  train   non-neutral       positive   non-negative  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the binary classification training and dev data to CSV files\n",
    "train_all_df.to_csv('data/merged/train_all_binary.csv', index=False)\n",
    "val_all_df.to_csv('data/merged/val_all_binary.csv', index=False)\n",
    "test_all_df.to_csv('data/merged/test_all_binary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the datasets from CSV files\n",
    "train_all_df = pd.read_csv('data/merged/train_all.csv')\n",
    "val_all_df = pd.read_csv('data/merged/val_all.csv')\n",
    "test_all_df = pd.read_csv('data/merged/test_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_minority_datasets(df, source_col='source', multipliers=None, balanced=True):\n",
    "    \"\"\"\n",
    "    Oversample minority datasets by duplicating their rows.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input DataFrame containing the data\n",
    "    source_col : str, optional\n",
    "        Name of the column containing dataset source information\n",
    "    multipliers : dict, optional\n",
    "        Custom multipliers for each dataset, e.g. {'dynasent_r1': 1, 'dynasent_r2': 3, 'sst_local': 3}\n",
    "        If None and balanced=True, will calculate multipliers to roughly balance datasets\n",
    "    balanced : bool, optional\n",
    "        If True and multipliers not provided, automatically calculate multipliers to make datasets balanced\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        New DataFrame with oversampled minority datasets\n",
    "    \"\"\"\n",
    "    # Get dataset sizes\n",
    "    dataset_counts = df[source_col].value_counts()\n",
    "    max_count = dataset_counts.max()\n",
    "    \n",
    "    if multipliers is None:\n",
    "        if balanced:\n",
    "            # Calculate multipliers to roughly balance datasets\n",
    "            multipliers = {\n",
    "                dataset: int(np.ceil(max_count / count))\n",
    "                for dataset, count in dataset_counts.items()\n",
    "            }\n",
    "        else:\n",
    "            # Default to no oversampling\n",
    "            multipliers = {dataset: 1 for dataset in dataset_counts.index}\n",
    "    \n",
    "    # Print statistics before oversampling\n",
    "    print(\"\\nBefore oversampling:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Dataset':<15} {'Count':<10} {'Percent':<10} {'Multiplier':<10} {'New Count':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    total = len(df)\n",
    "    oversampled_dfs = []\n",
    "    \n",
    "    for dataset in dataset_counts.index:\n",
    "        # Get multiplier for this dataset\n",
    "        mult = multipliers.get(dataset, 1)\n",
    "        \n",
    "        # Get rows for this dataset\n",
    "        dataset_df = df[df[source_col] == dataset]\n",
    "        count = len(dataset_df)\n",
    "        \n",
    "        # Calculate new count after oversampling\n",
    "        new_count = count * mult\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"{dataset:<15} {count:<10} {count/total*100:>6.2f}%    {mult:>6.0f}x    {new_count:<10}\")\n",
    "        \n",
    "        # Duplicate rows based on multiplier\n",
    "        if mult > 1:\n",
    "            dataset_df = pd.concat([dataset_df] * mult, ignore_index=True)\n",
    "        \n",
    "        oversampled_dfs.append(dataset_df)\n",
    "    \n",
    "    # Combine all datasets\n",
    "    result_df = pd.concat(oversampled_dfs, ignore_index=True)\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(\"\\nAfter oversampling:\")\n",
    "    print(f\"Total samples: {len(result_df):,} (before: {len(df):,})\")\n",
    "    new_counts = result_df[source_col].value_counts()\n",
    "    for dataset, count in new_counts.items():\n",
    "        print(f\"{dataset:<15} {count:<10} {count/len(result_df)*100:>6.2f}%\")\n",
    "    \n",
    "    # Shuffle the DataFrame\n",
    "    result_df = result_df.sample(frac=1.0).reset_index(drop=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def print_label_distribution(df, label_col='label', source_col='source'):\n",
    "    \"\"\"\n",
    "    Print the distribution of labels within each dataset source.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input DataFrame containing the data\n",
    "    label_col : str, optional\n",
    "        Name of the column containing labels\n",
    "    source_col : str, optional\n",
    "        Name of the column containing dataset source information\n",
    "    \"\"\"\n",
    "    print(\"\\nLabel distribution by dataset:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for dataset in df[source_col].unique():\n",
    "        dataset_df = df[df[source_col] == dataset]\n",
    "        total = len(dataset_df)\n",
    "        \n",
    "        print(f\"\\n{dataset} (Total: {total:,}):\")\n",
    "        label_counts = dataset_df[label_col].value_counts()\n",
    "        for label, count in label_counts.items():\n",
    "            print(f\"{label:<10} {count:<10} {count/total*100:>6.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before oversampling:\n",
      "------------------------------------------------------------\n",
      "Dataset         Count      Percent    Multiplier New Count \n",
      "------------------------------------------------------------\n",
      "dynasent_r1     80488       78.83%         1x    80488     \n",
      "dynasent_r2     13065       12.80%         7x    91455     \n",
      "sst_local       8544         8.37%        10x    85440     \n",
      "\n",
      "After oversampling:\n",
      "Total samples: 257,383 (before: 102,097)\n",
      "dynasent_r2     91455       35.53%\n",
      "sst_local       85440       33.20%\n",
      "dynasent_r1     80488       31.27%\n"
     ]
    }
   ],
   "source": [
    "# Automatically balance datasets\n",
    "train_balanced_df = oversample_minority_datasets(train_all_df, balanced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_df.to_csv('data/merged/train_balanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102097, 4) (5421, 4) (6530, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_all_df.shape, val_all_df.shape, test_all_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I had called in advance to see if they offered...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EVERY SINGLE ITEM WAS INEDIBLE.</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rooms are small.</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Without resorting to hyperbole , I can state t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Used a $12 Groupon deal (to cover $20 meal).</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label       source  \\\n",
       "0  I had called in advance to see if they offered...  positive  dynasent_r1   \n",
       "1                    EVERY SINGLE ITEM WAS INEDIBLE.  negative  dynasent_r1   \n",
       "2                                   Rooms are small.  negative  dynasent_r1   \n",
       "3  Without resorting to hyperbole , I can state t...  positive    sst_local   \n",
       "4       Used a $12 Groupon deal (to cover $20 meal).   neutral  dynasent_r1   \n",
       "\n",
       "  split  \n",
       "0  test  \n",
       "1  test  \n",
       "2  test  \n",
       "3  test  \n",
       "4  test  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6525</th>\n",
       "      <td>I went back in to ask for cilantro dressing th...</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6526</th>\n",
       "      <td>Here , Adrian Lyne comes as close to profundit...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6527</th>\n",
       "      <td>The actors are so terrific at conveying their ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6528</th>\n",
       "      <td>It should be mentioned that the set design and...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6529</th>\n",
       "      <td>She greeted customers by holding the scanner t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence     label  \\\n",
       "6525  I went back in to ask for cilantro dressing th...  positive   \n",
       "6526  Here , Adrian Lyne comes as close to profundit...  positive   \n",
       "6527  The actors are so terrific at conveying their ...   neutral   \n",
       "6528  It should be mentioned that the set design and...  positive   \n",
       "6529  She greeted customers by holding the scanner t...  negative   \n",
       "\n",
       "           source split  \n",
       "6525  dynasent_r2  test  \n",
       "6526    sst_local  test  \n",
       "6527    sst_local  test  \n",
       "6528    sst_local  test  \n",
       "6529  dynasent_r1  test  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
