{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3208bebc",
   "metadata": {},
   "source": [
    "# GPT-4o/4o-mini Fine-Tuning and DSPy Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae22d221",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab698db",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "0b7d665d-000e-4bae-83a8-1f7c78879df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS and utilities\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import pickle\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import ElectraTokenizer\n",
    "import tiktoken\n",
    "\n",
    "# OpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "# DSPy and ReCOGS\n",
    "import dsp\n",
    "import dspy\n",
    "from dspy.evaluate.evaluate import Evaluate\n",
    "from dspy import Retrieve\n",
    "\n",
    "# Instrumentation and tracing\n",
    "from phoenix.otel import register\n",
    "from openinference.instrumentation.dspy import DSPyInstrumentor\n",
    "\n",
    "# Typing\n",
    "from typing import List, Union\n",
    "\n",
    "# Local imports\n",
    "from datawaza_funcs import eval_model\n",
    "from utils import fix_random_seeds, prepare_device, setup_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80380fc",
   "metadata": {},
   "source": [
    "### Environment and Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5609266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy notebook cache\n",
    "root_path = '.'\n",
    "os.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = os.path.join(root_path, 'cache')\n",
    "\n",
    "# Pandas display options\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378007d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"resume_download is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use force_download=True.\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff190c03",
   "metadata": {},
   "source": [
    "### API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6068202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "arize_key = os.getenv('ARIZE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a04d21c",
   "metadata": {},
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67c8b8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ID                                Creation Date                           \n",
      "--------------------------------------------------------------------------------\n",
      "gpt-4o-realtime-preview                 2024-09-30 01:33:18                     \n",
      "gpt-4o-audio-preview                    2024-09-27 18:07:23                     \n",
      "gpt-4o-audio-preview-2024-10-01         2024-09-26 22:17:22                     \n",
      "gpt-4o-realtime-preview-2024-10-01      2024-09-23 22:49:26                     \n",
      "o1-mini                                 2024-09-06 18:56:48                     \n",
      "o1-mini-2024-09-12                      2024-09-06 18:56:19                     \n",
      "o1-preview                              2024-09-06 18:54:57                     \n",
      "o1-preview-2024-09-12                   2024-09-06 18:54:25                     \n",
      "chatgpt-4o-latest                       2024-08-13 02:12:11                     \n",
      "gpt-4o-2024-08-06                       2024-08-04 23:38:39                     \n",
      "gpt-4o-mini                             2024-07-16 23:32:21                     \n",
      "gpt-4o-mini-2024-07-18                  2024-07-16 23:31:57                     \n",
      "gpt-4o-2024-05-13                       2024-05-10 19:08:52                     \n",
      "gpt-4o                                  2024-05-10 18:50:49                     \n",
      "gpt-4-turbo-2024-04-09                  2024-04-08 18:41:17                     \n",
      "gpt-4-turbo                             2024-04-05 23:57:21                     \n",
      "gpt-3.5-turbo-0125                      2024-01-23 22:19:18                     \n",
      "gpt-4-turbo-preview                     2024-01-23 19:22:57                     \n",
      "gpt-4-0125-preview                      2024-01-23 19:20:12                     \n",
      "text-embedding-3-large                  2024-01-22 19:53:00                     \n",
      "text-embedding-3-small                  2024-01-22 18:43:17                     \n",
      "tts-1-hd-1106                           2023-11-03 23:18:53                     \n",
      "tts-1-1106                              2023-11-03 23:14:01                     \n",
      "tts-1-hd                                2023-11-03 21:13:35                     \n",
      "gpt-3.5-turbo-1106                      2023-11-02 21:15:48                     \n",
      "gpt-4-1106-preview                      2023-11-02 20:33:26                     \n",
      "dall-e-2                                2023-11-01 00:22:57                     \n",
      "dall-e-3                                2023-10-31 20:46:29                     \n",
      "gpt-3.5-turbo-instruct-0914             2023-09-07 21:34:32                     \n",
      "gpt-3.5-turbo-instruct                  2023-08-24 18:23:47                     \n",
      "babbage-002                             2023-08-21 16:16:55                     \n",
      "davinci-002                             2023-08-21 16:11:41                     \n",
      "gpt-4                                   2023-06-27 16:13:31                     \n",
      "gpt-4-0613                              2023-06-12 16:54:56                     \n",
      "gpt-3.5-turbo-16k                       2023-05-10 22:35:02                     \n",
      "tts-1                                   2023-04-19 21:49:11                     \n",
      "gpt-3.5-turbo                           2023-02-28 18:56:42                     \n",
      "whisper-1                               2023-02-27 21:13:04                     \n",
      "text-embedding-ada-002                  2022-12-16 19:01:39                     \n"
     ]
    }
   ],
   "source": [
    "# Create OpenAI client\n",
    "client = OpenAI()\n",
    "models = client.models.list()\n",
    "\n",
    "# Print OpenAI models by creation date\n",
    "sorted_models = sorted(models.dict()['data'], key=lambda x: x['created'], reverse=True)\n",
    "print(f\"{'Model ID':<40}{'Creation Date':<40}\")\n",
    "print('-'*80)\n",
    "for model in sorted_models:\n",
    "    if not model['id'].startswith('ft:'):\n",
    "        print(f\"{model['id']:<40}{datetime.fromtimestamp(int(model['created'])).strftime('%Y-%m-%d %H:%M:%S'):<40}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70692f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the fine-tuned models\n",
    "print(f\"{'Model ID':<40}{'Creation Date':<40}\")\n",
    "print('-'*80)\n",
    "for model in sorted_models:\n",
    "    if model['id'].startswith('ft:'):\n",
    "        print(f\"{model['id']:<40}{datetime.fromtimestamp(int(model['created'])).strftime('%Y-%m-%d %H:%M:%S'):<40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9933f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the language model\n",
    "lm = dspy.OpenAI(model='gpt-4o-mini-2024-07-18', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbcc2f1",
   "metadata": {},
   "source": [
    "### Arize Phoenix for Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa95b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arize environment variables\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={arize_key}\"\n",
    "os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={arize_key}\"\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bad5245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”­ OpenTelemetry Tracing Details ðŸ”­\n",
      "|  Phoenix Project: final_project\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: https://app.phoenix.arize.com/v1/traces\n",
      "|  Transport: HTTP\n",
      "|  Transport Headers: {'api_key': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the tracer provider\n",
    "tracer_provider = register(\n",
    "  project_name=\"final_project\",\n",
    "  endpoint=\"https://app.phoenix.arize.com/v1/traces\",\n",
    "  headers={\"api_key\": f\"{arize_key}\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78316edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the DSPy instrumentor\n",
    "DSPyInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da14956-1813-4372-b747-d7a2dbd10bc1",
   "metadata": {},
   "source": [
    "## Sentiment Data\n",
    "\n",
    "The dataset is a merge of DynaSent R1, DynaSent R2, and SST-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "22f20b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the merged datasets\n",
    "train_df = pd.read_csv('data/merged/train_all.csv')\n",
    "val_df = pd.read_csv('data/merged/val_all.csv')\n",
    "test_df = pd.read_csv('data/merged/test_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "d4550880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 102097\n",
      "Validation: 5421\n",
      "Test: 6530\n"
     ]
    }
   ],
   "source": [
    "# Print the lengths of each dataset\n",
    "print(f\"Train: {len(train_df)}\")\n",
    "print(f\"Validation: {len(val_df)}\")\n",
    "print(f\"Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "83b8ca91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Those 2 drinks are part of the HK culture and has years of history. It is so bad.</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was told by the repair company that was doing the car repair that fixing the rim was \"impossible\" and to replace it.</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is there to give them a good time .</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Like leafing through an album of photos accompanied by the sketchiest of captions .</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Johnny was a talker and liked to have fun.</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 sentence  \\\n",
       "0                                       Those 2 drinks are part of the HK culture and has years of history. It is so bad.   \n",
       "1  I was told by the repair company that was doing the car repair that fixing the rim was \"impossible\" and to replace it.   \n",
       "2                                                                                  It is there to give them a good time .   \n",
       "3                                     Like leafing through an album of photos accompanied by the sketchiest of captions .   \n",
       "4                                                                              Johnny was a talker and liked to have fun.   \n",
       "\n",
       "      label       source  split  \n",
       "0  negative  dynasent_r2  train  \n",
       "1  negative  dynasent_r1  train  \n",
       "2   neutral    sst_local  train  \n",
       "3  negative    sst_local  train  \n",
       "4  positive  dynasent_r1  train  "
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the train dataset\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "a145a809-6cc7-4711-aa94-850fd1383ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat a list of dspy.Example objects from train_df\n",
    "train_ex = [dspy.Example(review=row['sentence'], classification=row['label']).with_inputs('review') for _, row in train_df.iterrows()]\n",
    "val_ex = [dspy.Example(review=row['sentence'], classification=row['label']).with_inputs('review') for _, row in val_df.iterrows()]\n",
    "test_ex = [dspy.Example(review=row['sentence'], classification=row['label']).with_inputs('review') for _, row in test_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "f25e7c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example({'review': 'Those 2 drinks are part of the HK culture and has years of history. It is so bad.', 'classification': 'negative'}) (input_keys={'review'}),\n",
       " Example({'review': 'I was told by the repair company that was doing the car repair that fixing the rim was \"impossible\" and to replace it.', 'classification': 'negative'}) (input_keys={'review'}),\n",
       " Example({'review': 'It is there to give them a good time .', 'classification': 'neutral'}) (input_keys={'review'}),\n",
       " Example({'review': 'Like leafing through an album of photos accompanied by the sketchiest of captions .', 'classification': 'negative'}) (input_keys={'review'}),\n",
       " Example({'review': 'Johnny was a talker and liked to have fun.', 'classification': 'positive'}) (input_keys={'review'})]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the first few examples\n",
    "train_ex[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372ce2db",
   "metadata": {},
   "source": [
    "## ELECTRA Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e021c",
   "metadata": {},
   "source": [
    "### Setup DDP Environment\n",
    "\n",
    "I fine-tuned my models in a DDP multi-GPU environment using `finetune.py`. Because I'm loading a checkpoint from that pipeline, which has a DDP wrapper, I'm runninig some minimal functions here to setup a basic single-GPU DDP environemnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05a2b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some variables for the DDP environment\n",
    "rank = 0\n",
    "device_type = 'cuda'\n",
    "world_size = 1\n",
    "backend = 'nccl'\n",
    "debug = False\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77792345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = prepare_device(rank, device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df440d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 - Device: cuda:0\n",
      "1 process groups initialized with 'nccl' backend on localhost:12355\n",
      "NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled\n"
     ]
    }
   ],
   "source": [
    "# Initiate the process group\n",
    "setup_environment(rank, world_size, backend, device, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4d34860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "fix_random_seeds(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6862aec",
   "metadata": {},
   "source": [
    "### Load ELECTRA Tokenizer and Fine-Tuned Model\n",
    "\n",
    "We'll load an ELECTRA model that was fine-tuned on sentiment using `finetune.py`. This will be part of our DSPy module's forward pass. We'll get the ELECTRA classification for the review, and that will be input/context in the prompt for the GPT4o-mini language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637efaca",
   "metadata": {},
   "source": [
    "#### Use This to Reproduce\n",
    "\n",
    "Uncomment the following code to load the fine-tuned models from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b833c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# from electra_classifier import ElectraClassifier\n",
    "\n",
    "# large_model_name = \"jbeno/electra-large-classifier-sentiment\"\n",
    "# base_model_name = \"jbeno/electra-base-classifier-sentiment\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(large_model_name)\n",
    "\n",
    "# electra_large_model = ElectraClassifier.from_pretrained(large_model_name)\n",
    "# electra_large_model.to(device)\n",
    "\n",
    "# electra_base_model = ElectraClassifier.from_pretrained(base_model_name)\n",
    "# electra_base_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e4dc3",
   "metadata": {},
   "source": [
    "#### Original Code\n",
    "\n",
    "What follows is the code used for the original research. You can skip this section if you've loaded it from Hugging Face above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "51d94693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ELECTRA tokenizer from Hugging Face\n",
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-large-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3177d6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to unpickle the checkpoint saved with DDP and GPUs\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else: return super().find_class(module, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fae1cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ELECTRA Large model from a checkpoint pickle file\n",
    "# NOTE: This file path does not exist in the repository, use the Hugging Face approach above instead\n",
    "with open('models/final_model_20241025-024222.pkl', 'rb') as f:\n",
    "    electra_large_model = CPU_Unpickler(f).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0283f827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchDDPNeuralClassifier(\n",
       "\tbatch_size=16,\n",
       "\tmax_iter=50,\n",
       "\teta=1e-05,\n",
       "\toptimizer_class=<class 'torch.optim.adamw.AdamW'>,\n",
       "\tl2_strength=0.01,\n",
       "\tgradient_accumulation_steps=2,\n",
       "\tmax_grad_norm=None,\n",
       "\tvalidation_fraction=0.1,\n",
       "\tearly_stopping=score,\n",
       "\tn_iter_no_change=10,\n",
       "\twarm_start=False,\n",
       "\ttol=1e-05,\n",
       "\tfinetune_bert=True,\n",
       "\tpooling=mean,\n",
       "\thidden_dim=1024,\n",
       "\thidden_activation=SwishGLU(\n",
       "  (projection): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "  (activation): SiLU()\n",
       "),\n",
       "\tnum_layers=2,\n",
       "\tcheckpoint_interval=1,\n",
       "\ttarget_score=None,\n",
       "\tinteractive=True,\n",
       "\tfreeze_bert=False,\n",
       "\tdropout_rate=0.3,\n",
       "\tshow_progress=True,\n",
       "\tadvance_epochs=1,\n",
       "\tuse_zero=True,\n",
       "\tscheduler_class=<class 'torch.optim.lr_scheduler.CosineAnnealingWarmRestarts'>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the device (GPU ideally) to speed up inference time\n",
    "electra_large_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f89678a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistributedDataParallel(\n",
       "  (module): BERTClassifier(\n",
       "    (bert): ElectraModel(\n",
       "      (embeddings): ElectraEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 1024)\n",
       "        (token_type_embeddings): Embedding(2, 1024)\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): ElectraEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-23): 24 x ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (custom_pooling): PoolingLayer()\n",
       "    (classifier): Classifier(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (1): SwishGLU(\n",
       "          (projection): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (2): Dropout(p=0.3, inplace=False)\n",
       "        (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (4): SwishGLU(\n",
       "          (projection): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (5): Dropout(p=0.3, inplace=False)\n",
       "        (6): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the ELECTRA Large model with custom pooling and classifier head\n",
    "electra_large_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db7e3637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ELECTRA Base model from a checkpoint pickle file\n",
    "# NOTE: This file path does not exist in the repository, use the Hugging Face approach above instead\n",
    "with open('models/final_model_20241026-053505.pkl', 'rb') as f:\n",
    "    electra_base_model = CPU_Unpickler(f).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5af0f039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchDDPNeuralClassifier(\n",
       "\tbatch_size=16,\n",
       "\tmax_iter=50,\n",
       "\teta=1e-05,\n",
       "\toptimizer_class=<class 'torch.optim.adamw.AdamW'>,\n",
       "\tl2_strength=0.01,\n",
       "\tgradient_accumulation_steps=2,\n",
       "\tmax_grad_norm=None,\n",
       "\tvalidation_fraction=0.1,\n",
       "\tearly_stopping=score,\n",
       "\tn_iter_no_change=10,\n",
       "\twarm_start=False,\n",
       "\ttol=1e-05,\n",
       "\tfinetune_bert=True,\n",
       "\tpooling=mean,\n",
       "\thidden_dim=1024,\n",
       "\thidden_activation=SwishGLU(\n",
       "  (projection): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "  (activation): SiLU()\n",
       "),\n",
       "\tnum_layers=2,\n",
       "\tcheckpoint_interval=1,\n",
       "\ttarget_score=None,\n",
       "\tinteractive=True,\n",
       "\tfreeze_bert=False,\n",
       "\tdropout_rate=0.3,\n",
       "\tshow_progress=True,\n",
       "\tadvance_epochs=1,\n",
       "\tuse_zero=True,\n",
       "\tscheduler_class=<class 'torch.optim.lr_scheduler.CosineAnnealingWarmRestarts'>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the device (GPU ideally) to speed up inference time\n",
    "electra_base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f5d20826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistributedDataParallel(\n",
       "  (module): BERTClassifier(\n",
       "    (bert): ElectraModel(\n",
       "      (embeddings): ElectraEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): ElectraEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (custom_pooling): PoolingLayer()\n",
       "    (classifier): Classifier(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=1024, bias=True)\n",
       "        (1): SwishGLU(\n",
       "          (projection): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (2): Dropout(p=0.3, inplace=False)\n",
       "        (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (4): SwishGLU(\n",
       "          (projection): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (5): Dropout(p=0.3, inplace=False)\n",
       "        (6): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the ELECTRA Base model with custom pooling and classifier head\n",
    "electra_base_model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845252a",
   "metadata": {},
   "source": [
    "### ELECTRA Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9550d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the class label mapping dictionary\n",
    "numeric_dict = {0: 'negative', 1: 'neutral', 2: 'positive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1d831d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def tokenize(texts, tokenizer, device):\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    \n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f3816fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the class of a sentence\n",
    "def predict_sentence(model, sentence, tokenizer, numeric_dict):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.model\n",
    "    \n",
    "    # Tokenize the input sentence\n",
    "    input_ids, attention_mask = tokenize([sentence], tokenizer, device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Get the predictions\n",
    "    if isinstance(outputs, torch.Tensor):\n",
    "        logits = outputs\n",
    "    elif hasattr(outputs, 'logits'):\n",
    "        logits = outputs.logits\n",
    "    else:\n",
    "        logits = outputs[0]\n",
    "\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    predicted_label = numeric_dict[predicted_class]\n",
    "\n",
    "    # Move probabilities back to CPU and convert to list\n",
    "    probabilities = probabilities.cpu().squeeze().tolist()\n",
    "\n",
    "    # Free up GPU memory\n",
    "    del input_ids, attention_mask, logits, outputs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return predicted_class, predicted_label, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c410f9",
   "metadata": {},
   "source": [
    "### Creating ELECTRA Train Predictions for GPT fine-tuning\n",
    "\n",
    "We need to get the predictions from the ELECTRA models so we can process this later to create the JSONL files used for GPT fine-tuning scenarios that include the ELECTRA predictions in the fine-tuning prompt (FT-L)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "8f0e022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = \"Those 2 drinks are part of the HK culture and has years of history. It is so bad.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "8db51557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 'negative',\n",
       " [0.9988487958908081, 0.0008097602985799313, 0.0003413360973354429])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test prediction with ELECTRA base model\n",
    "predict_sentence(electra_base_model, test_review, tokenizer, numeric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "2a394eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 'negative',\n",
       " [0.9984808564186096, 0.0003502856707200408, 0.0011688611702993512])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test prediction with ELECTRA large model\n",
    "predict_sentence(electra_large_model, test_review, tokenizer, numeric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "ca9c8abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Those 2 drinks are part of the HK culture and has years of history. It is so bad.</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was told by the repair company that was doing the car repair that fixing the rim was \"impossible\" and to replace it.</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is there to give them a good time .</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Like leafing through an album of photos accompanied by the sketchiest of captions .</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Johnny was a talker and liked to have fun.</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 sentence  \\\n",
       "0                                       Those 2 drinks are part of the HK culture and has years of history. It is so bad.   \n",
       "1  I was told by the repair company that was doing the car repair that fixing the rim was \"impossible\" and to replace it.   \n",
       "2                                                                                  It is there to give them a good time .   \n",
       "3                                     Like leafing through an album of photos accompanied by the sketchiest of captions .   \n",
       "4                                                                              Johnny was a talker and liked to have fun.   \n",
       "\n",
       "      label       source  split  \n",
       "0  negative  dynasent_r2  train  \n",
       "1  negative  dynasent_r1  train  \n",
       "2   neutral    sst_local  train  \n",
       "3  negative    sst_local  train  \n",
       "4  positive  dynasent_r1  train  "
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "06e87276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_electra_predictions(df, electra_base_model, electra_large_model, tokenizer_base, tokenizer_large, numeric_dict):\n",
    "    \"\"\"\n",
    "    Add ELECTRA base and large model predictions to the DataFrame with tqdm progress bar\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame containing the 'sentence' column\n",
    "    electra_base_model: ELECTRA base model instance\n",
    "    electra_large_model: ELECTRA large model instance\n",
    "    tokenizer_base: tokenizer for base model\n",
    "    tokenizer_large: tokenizer for large model\n",
    "    numeric_dict: dictionary mapping numeric labels to text labels\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with added prediction columns\n",
    "    \"\"\"\n",
    "    # Create copies of the predictions to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Initialize new columns\n",
    "    df['electra_base_prediction'] = ''\n",
    "    df['electra_base_probabilities'] = None\n",
    "    df['electra_large_prediction'] = ''\n",
    "    df['electra_large_probabilities'] = None\n",
    "    \n",
    "    # Create tqdm progress bar\n",
    "    pbar = tqdm(df.iterrows(), total=len(df), desc=\"Processing sentences\")\n",
    "    \n",
    "    # Process each sentence\n",
    "    for idx, row in pbar:\n",
    "        # Get predictions from base model\n",
    "        _, base_label, base_probs = predict_sentence(\n",
    "            electra_base_model, \n",
    "            row['sentence'],\n",
    "            tokenizer_base,\n",
    "            numeric_dict\n",
    "        )\n",
    "        \n",
    "        # Get predictions from large model\n",
    "        _, large_label, large_probs = predict_sentence(\n",
    "            electra_large_model,\n",
    "            row['sentence'],\n",
    "            tokenizer_large,\n",
    "            numeric_dict\n",
    "        )\n",
    "        \n",
    "        # Store predictions and probabilities\n",
    "        df.at[idx, 'electra_base_prediction'] = base_label\n",
    "        df.at[idx, 'electra_base_probabilities'] = base_probs\n",
    "        df.at[idx, 'electra_large_prediction'] = large_label\n",
    "        df.at[idx, 'electra_large_probabilities'] = large_probs\n",
    "        \n",
    "        # Update progress bar description with current predictions\n",
    "        pbar.set_postfix({\n",
    "            'Base': base_label,\n",
    "            'Large': large_label\n",
    "        })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "2638ef95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725406539e2640ddaa622fbe8375a12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing sentences:   0%|          | 0/102097 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_with_electra_preds_df = add_electra_predictions(\n",
    "    df=train_df,\n",
    "    electra_base_model=electra_base_model,\n",
    "    electra_large_model=electra_large_model,\n",
    "    tokenizer_base=electra_base_model.bert_tokenizer,\n",
    "    tokenizer_large=electra_large_model.bert_tokenizer,\n",
    "    numeric_dict=numeric_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "cf4961f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_electra_preds_df.to_csv('results/train_with_electra_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "b464e4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "      <th>electra_base_prediction</th>\n",
       "      <th>electra_base_probabilities</th>\n",
       "      <th>electra_large_prediction</th>\n",
       "      <th>electra_large_probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Those 2 drinks are part of the HK culture and has years of history. It is so bad.</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r2</td>\n",
       "      <td>train</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.9988487958908081, 0.0008097602985799313, 0.0003413360973354429]</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.9984808564186096, 0.0003502856707200408, 0.0011688611702993512]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was told by the repair company that was doing the car repair that fixing the rim was \"impossible\" and to replace it.</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.750698447227478, 0.24390113353729248, 0.005400347523391247]</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.930504322052002, 0.06783030182123184, 0.0016653756611049175]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is there to give them a good time .</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[0.016632072627544403, 0.7018547654151917, 0.28151315450668335]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[0.010434377938508987, 0.6591203808784485, 0.33044523000717163]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Like leafing through an album of photos accompanied by the sketchiest of captions .</td>\n",
       "      <td>negative</td>\n",
       "      <td>sst_local</td>\n",
       "      <td>train</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.9957066178321838, 0.002995532238855958, 0.001297850627452135]</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.9150474667549133, 0.08219709247350693, 0.002755437046289444]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Johnny was a talker and liked to have fun.</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>train</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.0001658612600294873, 0.1131131649017334, 0.8867210149765015]</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.0010451035341247916, 0.23813723027706146, 0.7608176469802856]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 sentence  \\\n",
       "0                                       Those 2 drinks are part of the HK culture and has years of history. It is so bad.   \n",
       "1  I was told by the repair company that was doing the car repair that fixing the rim was \"impossible\" and to replace it.   \n",
       "2                                                                                  It is there to give them a good time .   \n",
       "3                                     Like leafing through an album of photos accompanied by the sketchiest of captions .   \n",
       "4                                                                              Johnny was a talker and liked to have fun.   \n",
       "\n",
       "      label       source  split electra_base_prediction  \\\n",
       "0  negative  dynasent_r2  train                negative   \n",
       "1  negative  dynasent_r1  train                negative   \n",
       "2   neutral    sst_local  train                 neutral   \n",
       "3  negative    sst_local  train                negative   \n",
       "4  positive  dynasent_r1  train                positive   \n",
       "\n",
       "                                           electra_base_probabilities  \\\n",
       "0  [0.9988487958908081, 0.0008097602985799313, 0.0003413360973354429]   \n",
       "1      [0.750698447227478, 0.24390113353729248, 0.005400347523391247]   \n",
       "2     [0.016632072627544403, 0.7018547654151917, 0.28151315450668335]   \n",
       "3    [0.9957066178321838, 0.002995532238855958, 0.001297850627452135]   \n",
       "4     [0.0001658612600294873, 0.1131131649017334, 0.8867210149765015]   \n",
       "\n",
       "  electra_large_prediction  \\\n",
       "0                 negative   \n",
       "1                 negative   \n",
       "2                  neutral   \n",
       "3                 negative   \n",
       "4                 positive   \n",
       "\n",
       "                                          electra_large_probabilities  \n",
       "0  [0.9984808564186096, 0.0003502856707200408, 0.0011688611702993512]  \n",
       "1     [0.930504322052002, 0.06783030182123184, 0.0016653756611049175]  \n",
       "2     [0.010434377938508987, 0.6591203808784485, 0.33044523000717163]  \n",
       "3     [0.9150474667549133, 0.08219709247350693, 0.002755437046289444]  \n",
       "4    [0.0010451035341247916, 0.23813723027706146, 0.7608176469802856]  "
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_electra_preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "8edf0174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5421/5421 [11:53<00:00,  7.60it/s, Base=neutral, Large=neutral]  \n"
     ]
    }
   ],
   "source": [
    "val_with_electra_preds_df = add_electra_predictions(\n",
    "    df=val_df,\n",
    "    electra_base_model=electra_base_model,\n",
    "    electra_large_model=electra_large_model,\n",
    "    tokenizer_base=electra_base_model.bert_tokenizer,\n",
    "    tokenizer_large=electra_large_model.bert_tokenizer,\n",
    "    numeric_dict=numeric_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "0808ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_with_electra_preds_df.to_csv('results/val_with_electra_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "fc5cbe27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "      <th>electra_base_prediction</th>\n",
       "      <th>electra_base_probabilities</th>\n",
       "      <th>electra_large_prediction</th>\n",
       "      <th>electra_large_probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Found Thai Spoon on the Vegan Pittsburgh website.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[0.00018192018615081906, 0.9930110573768616, 0.006806982681155205]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[0.0002572769590187818, 0.9571502208709717, 0.04259249195456505]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our bill came out to around $27 and we ate like the wealthy.</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.16897831857204437, 0.03550531715154648, 0.7955163717269897]</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.004331634845584631, 0.0017557682003825903, 0.9939125776290894]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>State Farm broke down the costs for me of the parts and labor.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[0.00011470958270365372, 0.9811069965362549, 0.01877826452255249]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[0.00047532867756672204, 0.9276981949806213, 0.0718264952301979]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The only con for this resto is the wait to get a seat.</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.9464658498764038, 0.015969356521964073, 0.03756479173898697]</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.9962604641914368, 0.00046653527533635497, 0.0032729844097048044]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We could hear the people above us stomping around even after midnight.</td>\n",
       "      <td>negative</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>validation</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.7719284296035767, 0.21691103279590607, 0.011160519905388355]</td>\n",
       "      <td>negative</td>\n",
       "      <td>[0.9870899319648743, 0.005847276654094458, 0.007062854245305061]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 sentence  \\\n",
       "0                       Found Thai Spoon on the Vegan Pittsburgh website.   \n",
       "1            Our bill came out to around $27 and we ate like the wealthy.   \n",
       "2          State Farm broke down the costs for me of the parts and labor.   \n",
       "3                  The only con for this resto is the wait to get a seat.   \n",
       "4  We could hear the people above us stomping around even after midnight.   \n",
       "\n",
       "      label       source       split electra_base_prediction  \\\n",
       "0   neutral  dynasent_r1  validation                 neutral   \n",
       "1  positive  dynasent_r1  validation                positive   \n",
       "2   neutral  dynasent_r1  validation                 neutral   \n",
       "3  negative  dynasent_r1  validation                negative   \n",
       "4  negative  dynasent_r1  validation                negative   \n",
       "\n",
       "                                           electra_base_probabilities  \\\n",
       "0  [0.00018192018615081906, 0.9930110573768616, 0.006806982681155205]   \n",
       "1      [0.16897831857204437, 0.03550531715154648, 0.7955163717269897]   \n",
       "2   [0.00011470958270365372, 0.9811069965362549, 0.01877826452255249]   \n",
       "3     [0.9464658498764038, 0.015969356521964073, 0.03756479173898697]   \n",
       "4     [0.7719284296035767, 0.21691103279590607, 0.011160519905388355]   \n",
       "\n",
       "  electra_large_prediction  \\\n",
       "0                  neutral   \n",
       "1                 positive   \n",
       "2                  neutral   \n",
       "3                 negative   \n",
       "4                 negative   \n",
       "\n",
       "                                           electra_large_probabilities  \n",
       "0     [0.0002572769590187818, 0.9571502208709717, 0.04259249195456505]  \n",
       "1    [0.004331634845584631, 0.0017557682003825903, 0.9939125776290894]  \n",
       "2     [0.00047532867756672204, 0.9276981949806213, 0.0718264952301979]  \n",
       "3  [0.9962604641914368, 0.00046653527533635497, 0.0032729844097048044]  \n",
       "4     [0.9870899319648743, 0.005847276654094458, 0.007062854245305061]  "
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_with_electra_preds_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d7c63",
   "metadata": {},
   "source": [
    "## Fine-Tune GPT Models\n",
    "\n",
    "A few different fine-tuning templates are created as described in the research paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42913b1",
   "metadata": {},
   "source": [
    "### A: DSPy Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7073b8",
   "metadata": {},
   "source": [
    "#### Convert Train Data to DSPy Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "530837bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert training data to chat format, with the same DSPy prompt\n",
    "def convert_to_chat_format(df, output_file):\n",
    "    \"\"\"\n",
    "    Convert training data to match DSPy signature format for GPT-4o-mini fine-tuning\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame containing 'sentence' and 'label' columns\n",
    "    output_file: path to save the JSONL output file\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        pbar = tqdm(df.iterrows(), total=len(df), desc=\"Converting to chat format\")\n",
    "        \n",
    "        for _, row in pbar:\n",
    "            prompt = f\"\"\"Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
    "\n",
    "---\n",
    "\n",
    "Follow the following format.\n",
    "\n",
    "Review: The review text to classify.\n",
    "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
    "\n",
    "---\n",
    "\n",
    "Review: {row['sentence']}\n",
    "Classification:\"\"\"\n",
    "            \n",
    "            chat_record = {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a sentiment analysis assistant.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": row['label']\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            json.dump(chat_record, f)\n",
    "            f.write('\\n')\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Label': row['label']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to chat format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102097/102097 [01:03<00:00, 1605.43it/s, Label=neutral] \n",
      "Converting to chat format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5421/5421 [00:03<00:00, 1673.01it/s, Label=neutral] \n"
     ]
    }
   ],
   "source": [
    "# Convert the train dataset to chat format for fine-tuning\n",
    "convert_to_chat_format(train_df, 'sentiment_merged_chat_train.jsonl')\n",
    "convert_to_chat_format(val_df, 'sentiment_merged_chat_val.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d03d2f",
   "metadata": {},
   "source": [
    "#### Load Training and Validation JSONL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 102097\n",
      "Validation examples: 5421\n"
     ]
    }
   ],
   "source": [
    "# Path to the jsonl datasets\n",
    "train_path = \"sentiment_merged_chat_train.jsonl\"\n",
    "val_path = \"sentiment_merged_chat_val.jsonl\"\n",
    "\n",
    "# Load the datasets\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open(val_path, 'r', encoding='utf-8') as f:\n",
    "    val_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Validation examples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d5eb3",
   "metadata": {},
   "source": [
    "#### Validate JSONL Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 102097\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a sentiment analysis assistant.'}\n",
      "{'role': 'user', 'content': \"Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\nClassification:\"}\n",
      "{'role': 'assistant', 'content': 'negative'}\n"
     ]
    }
   ],
   "source": [
    "# Path to the training dataset\n",
    "data_path = \"sentiment_merged_chat_train.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    jsonl_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(jsonl_data))\n",
    "print(\"First example:\")\n",
    "for message in jsonl_data[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in jsonl_data:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d5a08",
   "metadata": {},
   "source": [
    "#### Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoding\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 96, 751\n",
      "mean / median: 110.59470895325035, 109.0\n",
      "p5 / p95: 100.0, 123.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 1, 1\n",
      "mean / median: 1.0, 1.0\n",
      "p5 / p95: 1.0, 1.0\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in jsonl_data:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~11291388 tokens that will be charged for during training\n",
      "By default, you'll train for 1 epochs on this dataset\n",
      "By default, you'll be charged for ~11291388 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 135546\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 5\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(jsonl_data)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d44ee",
   "metadata": {},
   "source": [
    "### B: DSPy Prompt w/ELECTRA Pred (Base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716b099",
   "metadata": {},
   "source": [
    "#### Convert Train Data to DSPy Prompt w/ELECTRA Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "6e658c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert training data to chat format, with the same DSPy prompt\n",
    "def convert_to_chat_format_with_pred(df, output_file):\n",
    "    \"\"\"\n",
    "    Convert training data to match DSPy signature format for GPT-4o-mini fine-tuning\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame containing 'sentence' and 'label' columns\n",
    "    output_file: path to save the JSONL output file\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        pbar = tqdm(df.iterrows(), total=len(df), desc=\"Converting to chat format\")\n",
    "        \n",
    "        for _, row in pbar:\n",
    "            prompt = f\"\"\"Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
    "\n",
    "---\n",
    "\n",
    "Follow the following format.\n",
    "\n",
    "Review: The review text to classify.\n",
    "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
    "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
    "\n",
    "---\n",
    "\n",
    "Review: {row['sentence']}\n",
    "Classifier Decision: {row['electra_base_prediction']}\n",
    "Classification:\"\"\"\n",
    "            \n",
    "            chat_record = {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a sentiment analysis assistant.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": row['label']\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            json.dump(chat_record, f)\n",
    "            f.write('\\n')\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Label': row['label']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "253b62e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to chat format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102097/102097 [01:04<00:00, 1587.85it/s, Label=neutral] \n",
      "Converting to chat format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5421/5421 [00:03<00:00, 1626.10it/s, Label=neutral] \n"
     ]
    }
   ],
   "source": [
    "# Convert the train dataset to chat format for fine-tuning\n",
    "convert_to_chat_format_with_pred(train_with_electra_preds_df, 'sentiment_merged_chat_train_base_preds.jsonl')\n",
    "convert_to_chat_format_with_pred(val_with_electra_preds_df, 'sentiment_merged_chat_val_base_preds.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43218e",
   "metadata": {},
   "source": [
    "#### Load Training and Validation JSONL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "2cc040b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 102097\n",
      "Validation examples: 5421\n"
     ]
    }
   ],
   "source": [
    "# Path to the jsonl datasets\n",
    "train_path = \"sentiment_merged_chat_train_base_preds.jsonl\"\n",
    "val_path = \"sentiment_merged_chat_val_base_preds.jsonl\"\n",
    "\n",
    "# Load the datasets\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open(val_path, 'r', encoding='utf-8') as f:\n",
    "    val_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Validation examples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c176829f",
   "metadata": {},
   "source": [
    "#### Validate JSONL Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "38383fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 102097\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a sentiment analysis assistant.'}\n",
      "{'role': 'user', 'content': \"Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\nClassifier Decision: negative\\nClassification:\"}\n",
      "{'role': 'assistant', 'content': 'negative'}\n"
     ]
    }
   ],
   "source": [
    "# Path to the training dataset\n",
    "data_path = \"sentiment_merged_chat_train_base_preds.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    jsonl_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(jsonl_data))\n",
    "print(\"First example:\")\n",
    "for message in jsonl_data[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "b706a7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in jsonl_data:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6398a",
   "metadata": {},
   "source": [
    "#### Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "21025e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 117, 772\n",
      "mean / median: 131.59470895325035, 130.0\n",
      "p5 / p95: 121.0, 144.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 1, 1\n",
      "mean / median: 1.0, 1.0\n",
      "p5 / p95: 1.0, 1.0\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in jsonl_data:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "74f6e163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~13435425 tokens that will be charged for during training\n",
      "By default, you'll train for 1 epochs on this dataset\n",
      "By default, you'll be charged for ~13435425 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 135546\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 5\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(jsonl_data)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbb950",
   "metadata": {},
   "source": [
    "### C: DSPy Prompt w/ELECTRA Pred (Large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ab435",
   "metadata": {},
   "source": [
    "#### Convert Train Data to DSPy Prompt w/ELECTRA Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "efc916f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert training data to chat format, with the same DSPy prompt\n",
    "def convert_to_chat_format_with_pred_large(df, output_file):\n",
    "    \"\"\"\n",
    "    Convert training data to match DSPy signature format for GPT-4o-mini fine-tuning\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame containing 'sentence' and 'label' columns\n",
    "    output_file: path to save the JSONL output file\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        pbar = tqdm(df.iterrows(), total=len(df), desc=\"Converting to chat format\")\n",
    "        \n",
    "        for _, row in pbar:\n",
    "            prompt = f\"\"\"Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
    "\n",
    "---\n",
    "\n",
    "Follow the following format.\n",
    "\n",
    "Review: The review text to classify.\n",
    "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
    "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
    "\n",
    "---\n",
    "\n",
    "Review: {row['sentence']}\n",
    "Classifier Decision: {row['electra_large_prediction']}\n",
    "Classification:\"\"\"\n",
    "            \n",
    "            chat_record = {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a sentiment analysis assistant.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": row['label']\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            json.dump(chat_record, f)\n",
    "            f.write('\\n')\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Label': row['label']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "9885d1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to chat format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102097/102097 [01:04<00:00, 1594.46it/s, Label=neutral] \n",
      "Converting to chat format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5421/5421 [00:03<00:00, 1643.51it/s, Label=neutral] \n"
     ]
    }
   ],
   "source": [
    "# Convert the train dataset to chat format for fine-tuning\n",
    "convert_to_chat_format_with_pred_large(train_with_electra_preds_df, 'sentiment_merged_chat_train_large_preds.jsonl')\n",
    "convert_to_chat_format_with_pred_large(val_with_electra_preds_df, 'sentiment_merged_chat_val_large_preds.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3801273",
   "metadata": {},
   "source": [
    "#### Load Training and Validation JSONL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c89799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 102097\n",
      "Validation examples: 5421\n"
     ]
    }
   ],
   "source": [
    "# Path to the jsonl datasets\n",
    "train_path = \"sentiment_merged_chat_train_large_preds.jsonl\"\n",
    "val_path = \"sentiment_merged_chat_val_large_preds.jsonl\"\n",
    "\n",
    "# Load the datasets\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open(val_path, 'r', encoding='utf-8') as f:\n",
    "    val_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Validation examples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4967ba",
   "metadata": {},
   "source": [
    "#### Validate JSONL Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a3fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 102097\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a sentiment analysis assistant.'}\n",
      "{'role': 'user', 'content': \"Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\nClassifier Decision: negative\\nClassification:\"}\n",
      "{'role': 'assistant', 'content': 'negative'}\n"
     ]
    }
   ],
   "source": [
    "# Path to the training dataset\n",
    "data_path = \"sentiment_merged_chat_train_base_preds.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    jsonl_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(jsonl_data))\n",
    "print(\"First example:\")\n",
    "for message in jsonl_data[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1159a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in jsonl_data:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c423116",
   "metadata": {},
   "source": [
    "#### Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c8418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 117, 772\n",
      "mean / median: 131.59470895325035, 130.0\n",
      "p5 / p95: 121.0, 144.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 1, 1\n",
      "mean / median: 1.0, 1.0\n",
      "p5 / p95: 1.0, 1.0\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in jsonl_data:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ed47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~13435425 tokens that will be charged for during training\n",
      "By default, you'll train for 1 epochs on this dataset\n",
      "By default, you'll be charged for ~13435425 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 135546\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 5\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(jsonl_data)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f681c3c9",
   "metadata": {},
   "source": [
    "### D: No DSPy Prompt, Just Input Text w/System Role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3b183",
   "metadata": {},
   "source": [
    "#### Convert Train Data to OpenAI Minimal Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0804b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert training data to chat format required by GPT-4o-mini\n",
    "def convert_to_chat_format(df, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for index, row in df.iterrows():\n",
    "            chat_record = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a model that classifies the sentiment of a review as either 'positive', 'neutral', or 'negative'.\"},\n",
    "                    {\"role\": \"user\", \"content\": row['sentence']},\n",
    "                    {\"role\": \"assistant\", \"content\": row['label']}\n",
    "                ]\n",
    "            }\n",
    "            json.dump(chat_record, f)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the train dataset to chat format for fine-tuning\n",
    "convert_to_chat_format(train_df, 'sentiment_merged_chat_min_train.jsonl')\n",
    "convert_to_chat_format(val_df, 'sentiment_merged_chat_min_val.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447cf203",
   "metadata": {},
   "source": [
    "#### Load Training and Validation JSONL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2ecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 102097\n",
      "Validation examples: 5421\n"
     ]
    }
   ],
   "source": [
    "# Path to the jsonl datasets\n",
    "train_path = \"sentiment_merged_chat_min_train.jsonl\"\n",
    "val_path = \"sentiment_merged_chat_min_val.jsonl\"\n",
    "\n",
    "# Load the datasets\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open(val_path, 'r', encoding='utf-8') as f:\n",
    "    val_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Validation examples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb5900",
   "metadata": {},
   "source": [
    "#### Validate JSONL Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f9818d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 102097\n",
      "First example:\n",
      "{'role': 'system', 'content': \"You are a model that classifies the sentiment of a review as either 'positive', 'neutral', or 'negative'.\"}\n",
      "{'role': 'user', 'content': 'Those 2 drinks are part of the HK culture and has years of history. It is so bad.'}\n",
      "{'role': 'assistant', 'content': 'negative'}\n"
     ]
    }
   ],
   "source": [
    "# Path to the training dataset\n",
    "data_path = \"sentiment_merged_chat_train.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    jsonl_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(jsonl_data))\n",
    "print(\"First example:\")\n",
    "for message in jsonl_data[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86695aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in jsonl_data:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912cfb15",
   "metadata": {},
   "source": [
    "#### Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225a668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoding\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266a848c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 41, 697\n",
      "mean / median: 56.587382587147516, 55.0\n",
      "p5 / p95: 46.0, 69.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 1, 1\n",
      "mean / median: 1.0, 1.0\n",
      "p5 / p95: 1.0, 1.0\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in jsonl_data:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfeb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~5777402 tokens that will be charged for during training\n",
      "By default, you'll train for 1 epochs on this dataset\n",
      "By default, you'll be charged for ~5777402 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 135546\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 5\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(jsonl_data)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbfaa91",
   "metadata": {},
   "source": [
    "## API \n",
    "\n",
    "1. Substitute `API-KEY` with your own OpenAI API key for these to work.\n",
    "2. Replace the 'wandb' `entity` and `project` and `tags`, or strip out if you don't want to track on Weights and Biases.\n",
    "3. Run the `curl` commands from the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0151f854",
   "metadata": {},
   "source": [
    "\n",
    "### JOB-1: GPT-4o-mini with A: DSPy Prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a61b4",
   "metadata": {},
   "source": [
    "##### Upload Train Data\n",
    "\n",
    "```\n",
    "curl -X POST -H \"Authorization: Bearer API-KEY\" -F \"file=@sentiment_merged_chat_train.jsonl\" -F \"purpose=fine-tune\" https://api.openai.com/v1/files\n",
    "\n",
    "{\n",
    "  \"object\": \"file\",\n",
    "  \"id\": \"file-yKIFGoKBWGXhUI6tnuhnECtk\",\n",
    "  \"purpose\": \"fine-tune\",\n",
    "  \"filename\": \"sentiment_merged_chat_train.jsonl\",\n",
    "  \"bytes\": 60572207,\n",
    "  \"created_at\": 1730050009,\n",
    "  \"status\": \"processed\",\n",
    "  \"status_details\": null\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e6483a",
   "metadata": {},
   "source": [
    "##### Upload Validation Data\n",
    "```\n",
    "curl -X POST -H \"Authorization: Bearer API-KEY\" -F \"file=@sentiment_merged_chat_val.jsonl\" -F \"purpose=fine-tune\" https://api.openai.com/v1/files\n",
    "\n",
    "{\n",
    "  \"object\": \"file\",\n",
    "  \"id\": \"file-ntndnD6Qqd0Gubdphs127kBC\",\n",
    "  \"purpose\": \"fine-tune\",\n",
    "  \"filename\": \"sentiment_merged_chat_val.jsonl\",\n",
    "  \"bytes\": 3253015,\n",
    "  \"created_at\": 1730050050,\n",
    "  \"status\": \"processed\",\n",
    "  \"status_details\": null\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834a174",
   "metadata": {},
   "source": [
    "##### Start Fine-Tuning Job\n",
    "\n",
    "```\n",
    "curl -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer API-KEY\" -d '{\"model\": \"gpt-4o-mini-2024-07-18\",\"training_file\": \"file-yKIFGoKBWGXhUI6tnuhnECtk\",\"validation_file\": \"file-ntndnD6Qqd0Gubdphs127kBC\",\"seed\": 42,\"integrations\": [{\"type\": \"wandb\",\"wandb\": {\"entity\": \"jimbeno\",\"project\": \"OpenAI Finetune\",\"tags\": [\"sentiment\", \"stanford\"]}}]}' https://api.openai.com/v1/fine_tuning/jobs\n",
    "\n",
    "{\n",
    "  \"object\": \"fine_tuning.job\",\n",
    "  \"id\": \"ftjob-4rB6jh7fkqT3IF7VwA5rEvp0\",\n",
    "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "  \"created_at\": 1730050269,\n",
    "  \"finished_at\": null,\n",
    "  \"fine_tuned_model\": null,\n",
    "  \"organization_id\": \"org-mPzSezITuiy7tC8aFUjixSL5\",\n",
    "  \"result_files\": [],\n",
    "  \"status\": \"validating_files\",\n",
    "  \"validation_file\": \"file-ntndnD6Qqd0Gubdphs127kBC\",\n",
    "  \"training_file\": \"file-yKIFGoKBWGXhUI6tnuhnECtk\",\n",
    "  \"hyperparameters\": {\n",
    "    \"n_epochs\": \"auto\",\n",
    "    \"batch_size\": \"auto\",\n",
    "    \"learning_rate_multiplier\": \"auto\"\n",
    "  },\n",
    "  \"trained_tokens\": null,\n",
    "  \"error\": {},\n",
    "  \"user_provided_suffix\": null,\n",
    "  \"seed\": 42,\n",
    "  \"estimated_finish\": null,\n",
    "  \"integrations\": [\n",
    "    {\n",
    "      \"type\": \"wandb\",\n",
    "      \"wandb\": {\n",
    "        \"project\": \"OpenAI Finetune\",\n",
    "        \"entity\": \"jimbeno\",\n",
    "        \"run_id\": \"ftjob-4rB6jh7fkqT3IF7VwA5rEvp0\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4521411a",
   "metadata": {},
   "source": [
    "\n",
    "### JOB-3: GPT-4o with A: DSPy Prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364147eb",
   "metadata": {},
   "source": [
    "##### Start Fine-Tuning Job with GPT-4o\n",
    "\n",
    "```\n",
    "curl -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer API-KEY\" -d '{\"model\": \"gpt-4o-2024-08-06\",\"training_file\": \"file-yKIFGoKBWGXhUI6tnuhnECtk\",\"validation_file\": \"file-ntndnD6Qqd0Gubdphs127kBC\",\"seed\": 42,\"integrations\": [{\"type\": \"wandb\",\"wandb\": {\"entity\": \"jimbeno\",\"project\": \"OpenAI Finetune\",\"tags\": [\"sentiment\", \"stanford\"]}}]}' https://api.openai.com/v1/fine_tuning/jobs\n",
    "\n",
    "{\n",
    "  \"object\": \"fine_tuning.job\",\n",
    "  \"id\": \"ftjob-wM0mZOvg57Xb7YGAVc3zqGsT\",\n",
    "  \"model\": \"gpt-4o-2024-08-06\",\n",
    "  \"created_at\": 1730058245,\n",
    "  \"finished_at\": null,\n",
    "  \"fine_tuned_model\": null,\n",
    "  \"organization_id\": \"org-mPzSezITuiy7tC8aFUjixSL5\",\n",
    "  \"result_files\": [],\n",
    "  \"status\": \"validating_files\",\n",
    "  \"validation_file\": \"file-ntndnD6Qqd0Gubdphs127kBC\",\n",
    "  \"training_file\": \"file-yKIFGoKBWGXhUI6tnuhnECtk\",\n",
    "  \"hyperparameters\": {\n",
    "    \"n_epochs\": \"auto\",\n",
    "    \"batch_size\": \"auto\",\n",
    "    \"learning_rate_multiplier\": \"auto\"\n",
    "  },\n",
    "  \"trained_tokens\": null,\n",
    "  \"error\": {},\n",
    "  \"user_provided_suffix\": null,\n",
    "  \"seed\": 42,\n",
    "  \"estimated_finish\": null,\n",
    "  \"integrations\": [\n",
    "    {\n",
    "      \"type\": \"wandb\",\n",
    "      \"wandb\": {\n",
    "        \"project\": \"OpenAI Finetune\",\n",
    "        \"entity\": \"jimbeno\",\n",
    "        \"run_id\": \"ftjob-wM0mZOvg57Xb7YGAVc3zqGsT\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c0aecf",
   "metadata": {},
   "source": [
    "\n",
    "### JOB-2: GPT-4o-mini with B: DSPy Prompt w/ELECTRA Pred (Base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae1c075",
   "metadata": {},
   "source": [
    "##### Upload Train Data\n",
    "\n",
    "```\n",
    "curl -X POST -H \"Authorization: Bearer API-KEY\" -F \"file=@sentiment_merged_chat_train_base_preds.jsonl\" -F \"purpose=fine-tune\" https://api.openai.com/v1/files\n",
    "\n",
    "{\n",
    "  \"object\": \"file\",\n",
    "  \"id\": \"file-isdFKYFoBixXlk2Ie4DSI7QR\",\n",
    "  \"purpose\": \"fine-tune\",\n",
    "  \"filename\": \"sentiment_merged_chat_train_base_preds.jsonl\",\n",
    "  \"bytes\": 73490243,\n",
    "  \"created_at\": 1730160541,\n",
    "  \"status\": \"processed\",\n",
    "  \"status_details\": null\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34888487",
   "metadata": {},
   "source": [
    "##### Upload Validation Data\n",
    "```\n",
    "curl -X POST -H \"Authorization: Bearer API-KEY\" -F \"file=@sentiment_merged_chat_val_base_preds.jsonl\" -F \"purpose=fine-tune\" https://api.openai.com/v1/files\n",
    "\n",
    "{\n",
    "  \"object\": \"file\",\n",
    "  \"id\": \"file-nNyVFYCQPkvIb4uVthFPcyG6\",\n",
    "  \"purpose\": \"fine-tune\",\n",
    "  \"filename\": \"sentiment_merged_chat_val_base_preds.jsonl\",\n",
    "  \"bytes\": 3939665,\n",
    "  \"created_at\": 1730160572,\n",
    "  \"status\": \"processed\",\n",
    "  \"status_details\": null\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce0fe4",
   "metadata": {},
   "source": [
    "##### Start Fine-Tuning Job\n",
    "\n",
    "```\n",
    "curl -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer API-KEY\" -d '{\"model\": \"gpt-4o-mini-2024-07-18\",\"training_file\": \"file-isdFKYFoBixXlk2Ie4DSI7QR\",\"validation_file\": \"file-nNyVFYCQPkvIb4uVthFPcyG6\",\"seed\": 42,\"integrations\": [{\"type\": \"wandb\",\"wandb\": {\"entity\": \"jimbeno\",\"project\": \"OpenAI Finetune\",\"tags\": [\"sentiment\", \"stanford\"]}}]}' https://api.openai.com/v1/fine_tuning/jobs\n",
    "\n",
    "{\n",
    "  \"object\": \"fine_tuning.job\",\n",
    "  \"id\": \"ftjob-FutiPC92zZab63hwnvvz502w\",\n",
    "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "  \"created_at\": 1730160796,\n",
    "  \"finished_at\": null,\n",
    "  \"fine_tuned_model\": null,\n",
    "  \"organization_id\": \"org-mPzSezITuiy7tC8aFUjixSL5\",\n",
    "  \"result_files\": [],\n",
    "  \"status\": \"validating_files\",\n",
    "  \"validation_file\": \"file-nNyVFYCQPkvIb4uVthFPcyG6\",\n",
    "  \"training_file\": \"file-isdFKYFoBixXlk2Ie4DSI7QR\",\n",
    "  \"hyperparameters\": {\n",
    "    \"n_epochs\": \"auto\",\n",
    "    \"batch_size\": \"auto\",\n",
    "    \"learning_rate_multiplier\": \"auto\"\n",
    "  },\n",
    "  \"trained_tokens\": null,\n",
    "  \"error\": {},\n",
    "  \"user_provided_suffix\": null,\n",
    "  \"seed\": 42,\n",
    "  \"estimated_finish\": null,\n",
    "  \"integrations\": [\n",
    "    {\n",
    "      \"type\": \"wandb\",\n",
    "      \"wandb\": {\n",
    "        \"project\": \"OpenAI Finetune\",\n",
    "        \"entity\": \"jimbeno\",\n",
    "        \"run_id\": \"ftjob-FutiPC92zZab63hwnvvz502w\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3c528",
   "metadata": {},
   "source": [
    "### JOB-4: GPT-4o with C: DSPy Prompt w/ELECTRA Pred (Large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943a2cc",
   "metadata": {},
   "source": [
    "##### Upload Train Data\n",
    "\n",
    "```\n",
    "curl -X POST -H \"Authorization: Bearer API-KEY\" -F \"file=@sentiment_merged_chat_train_large_preds.jsonl\" -F \"purpose=fine-tune\" https://api.openai.com/v1/files\n",
    "\n",
    "{\n",
    "  \"object\": \"file\",\n",
    "  \"id\": \"file-HJTRqx57bMmofI17YWcxfjY1\",\n",
    "  \"purpose\": \"fine-tune\",\n",
    "  \"filename\": \"sentiment_merged_chat_train_large_preds.jsonl\",\n",
    "  \"bytes\": 73491118,\n",
    "  \"created_at\": 1730185975,\n",
    "  \"status\": \"processed\",\n",
    "  \"status_details\": null\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b8f9c",
   "metadata": {},
   "source": [
    "##### Upload Validation Data\n",
    "```\n",
    "curl -X POST -H \"Authorization: Bearer API-KEY\" -F \"file=@sentiment_merged_chat_val_large_preds.jsonl\" -F \"purpose=fine-tune\" https://api.openai.com/v1/files\n",
    "\n",
    "{\n",
    "  \"object\": \"file\",\n",
    "  \"id\": \"file-44AKnREOssylgZHN4rOYKSCx\",\n",
    "  \"purpose\": \"fine-tune\",\n",
    "  \"filename\": \"sentiment_merged_chat_val_large_preds.jsonl\",\n",
    "  \"bytes\": 3939780,\n",
    "  \"created_at\": 1730186004,\n",
    "  \"status\": \"processed\",\n",
    "  \"status_details\": null\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc5fc4",
   "metadata": {},
   "source": [
    "##### Start Fine-Tuning Job\n",
    "\n",
    "```\n",
    "curl -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer API-KEY\" -d '{\"model\": \"gpt-4o-2024-08-06\",\"training_file\": \"file-HJTRqx57bMmofI17YWcxfjY1\",\"validation_file\": \"file-44AKnREOssylgZHN4rOYKSCx\",\"seed\": 42,\"integrations\": [{\"type\": \"wandb\",\"wandb\": {\"entity\": \"jimbeno\",\"project\": \"OpenAI Finetune\",\"tags\": [\"sentiment\", \"stanford\"]}}]}' https://api.openai.com/v1/fine_tuning/jobs\n",
    "\n",
    "{\n",
    "  \"object\": \"fine_tuning.job\",\n",
    "  \"id\": \"ftjob-lQ8NyFb0GLGG1f9ejknCicVy\",\n",
    "  \"model\": \"gpt-4o-2024-08-06\",\n",
    "  \"created_at\": 1730186207,\n",
    "  \"finished_at\": null,\n",
    "  \"fine_tuned_model\": null,\n",
    "  \"organization_id\": \"org-mPzSezITuiy7tC8aFUjixSL5\",\n",
    "  \"result_files\": [],\n",
    "  \"status\": \"validating_files\",\n",
    "  \"validation_file\": \"file-44AKnREOssylgZHN4rOYKSCx\",\n",
    "  \"training_file\": \"file-HJTRqx57bMmofI17YWcxfjY1\",\n",
    "  \"hyperparameters\": {\n",
    "    \"n_epochs\": \"auto\",\n",
    "    \"batch_size\": \"auto\",\n",
    "    \"learning_rate_multiplier\": \"auto\"\n",
    "  },\n",
    "  \"trained_tokens\": null,\n",
    "  \"error\": {},\n",
    "  \"user_provided_suffix\": null,\n",
    "  \"seed\": 42,\n",
    "  \"estimated_finish\": null,\n",
    "  \"integrations\": [\n",
    "    {\n",
    "      \"type\": \"wandb\",\n",
    "      \"wandb\": {\n",
    "        \"project\": \"OpenAI Finetune\",\n",
    "        \"entity\": \"jimbeno\",\n",
    "        \"run_id\": \"ftjob-lQ8NyFb0GLGG1f9ejknCicVy\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f45b64",
   "metadata": {},
   "source": [
    "### JOB-5: GPT-4o-mini with No Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload Train Data\n",
    "\n",
    "```\n",
    "curl -X POST -H \"Authorization: Bearer API-KEY\" -F \"file=@sentiment_merged_chat_train.jsonl\" -F \"purpose=fine-tune\" https://api.openai.com/v1/files\n",
    "\n",
    "{\n",
    "  \"object\": \"file\",\n",
    "  \"id\": \"file-gyIcFWGgLnZedgeXPhlqKOFi\",\n",
    "  \"purpose\": \"fine-tune\",\n",
    "  \"filename\": \"sentiment_merged_chat_train.jsonl\",\n",
    "  \"bytes\": 31168271,\n",
    "  \"created_at\": 1729751926,\n",
    "  \"status\": \"processed\",\n",
    "  \"status_details\": null\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae178e",
   "metadata": {},
   "source": [
    "##### Upload Validation Data\n",
    "```\n",
    "curl -X POST -H \"Authorization: Bearer API-KEY\" -F \"file=@sentiment_merged_chat_val.jsonl\" -F \"purpose=fine-tune\" https://api.openai.com/v1/files\n",
    "\n",
    "{\n",
    "  \"object\": \"file\",\n",
    "  \"id\": \"file-VBrywvXv9q5FBrPCLxOvrGnM\",\n",
    "  \"purpose\": \"fine-tune\",\n",
    "  \"filename\": \"sentiment_merged_chat_val.jsonl\",\n",
    "  \"bytes\": 1691767,\n",
    "  \"created_at\": 1729751956,\n",
    "  \"status\": \"processed\",\n",
    "  \"status_details\": null\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e493a56",
   "metadata": {},
   "source": [
    "##### Start Fine-Tuning Job\n",
    "\n",
    "```\n",
    "curl -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer API-KEY\" -d '{\"model\": \"gpt-4o-mini-2024-07-18\",\"training_file\": \"file-gyIcFWGgLnZedgeXPhlqKOFi\",\"validation_file\": \"file-VBrywvXv9q5FBrPCLxOvrGnM\",\"seed\": 42,\"integrations\": [{\"type\": \"wandb\",\"wandb\": {\"entity\": \"jimbeno\",\"project\": \"OpenAI Finetune\",\"tags\": [\"sentiment\", \"stanford\"]}}]}' https://api.openai.com/v1/fine_tuning/jobs\n",
    "\n",
    "{\n",
    "  \"object\": \"fine_tuning.job\",\n",
    "  \"id\": \"ftjob-qjmtJD7rG8E6qMgaip2R9kry\",\n",
    "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "  \"created_at\": 1729752676,\n",
    "  \"finished_at\": null,\n",
    "  \"fine_tuned_model\": null,\n",
    "  \"organization_id\": \"org-mPzSezITuiy7tC8aFUjixSL5\",\n",
    "  \"result_files\": [],\n",
    "  \"status\": \"validating_files\",\n",
    "  \"validation_file\": \"file-VBrywvXv9q5FBrPCLxOvrGnM\",\n",
    "  \"training_file\": \"file-gyIcFWGgLnZedgeXPhlqKOFi\",\n",
    "  \"hyperparameters\": {\n",
    "    \"n_epochs\": \"auto\",\n",
    "    \"batch_size\": \"auto\",\n",
    "    \"learning_rate_multiplier\": \"auto\"\n",
    "  },\n",
    "  \"trained_tokens\": null,\n",
    "  \"error\": {},\n",
    "  \"user_provided_suffix\": null,\n",
    "  \"seed\": 42,\n",
    "  \"estimated_finish\": null,\n",
    "  \"integrations\": [\n",
    "    {\n",
    "      \"type\": \"wandb\",\n",
    "      \"wandb\": {\n",
    "        \"project\": \"OpenAI Finetune\",\n",
    "        \"entity\": \"jimbeno\",\n",
    "        \"run_id\": \"ftjob-qjmtJD7rG8E6qMgaip2R9kry\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf1ba4",
   "metadata": {},
   "source": [
    "### JOB-6: GPT-4o with No Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d959ce9",
   "metadata": {},
   "source": [
    "##### Start Fine-Tuning Job with GPT-4o\n",
    "\n",
    "```\n",
    "curl -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer API-KEY\" -d '{\"model\": \"gpt-4o-2024-08-06\",\"training_file\": \"file-gyIcFWGgLnZedgeXPhlqKOFi\",\"validation_file\": \"file-VBrywvXv9q5FBrPCLxOvrGnM\",\"seed\": 42,\"integrations\": [{\"type\": \"wandb\",\"wandb\": {\"entity\": \"jimbeno\",\"project\": \"OpenAI Finetune\",\"tags\": [\"sentiment\", \"stanford\"]}}]}' https://api.openai.com/v1/fine_tuning/jobs\n",
    "\n",
    "{\n",
    "  \"object\": \"fine_tuning.job\",\n",
    "  \"id\": \"ftjob-83naTXGkuWceaoIsM4iaNXxR\",\n",
    "  \"model\": \"gpt-4o-2024-08-06\",\n",
    "  \"created_at\": 1729822735,\n",
    "  \"finished_at\": null,\n",
    "  \"fine_tuned_model\": null,\n",
    "  \"organization_id\": \"org-mPzSezITuiy7tC8aFUjixSL5\",\n",
    "  \"result_files\": [],\n",
    "  \"status\": \"validating_files\",\n",
    "  \"validation_file\": \"file-VBrywvXv9q5FBrPCLxOvrGnM\",\n",
    "  \"training_file\": \"file-gyIcFWGgLnZedgeXPhlqKOFi\",\n",
    "  \"hyperparameters\": {\n",
    "    \"n_epochs\": \"auto\",\n",
    "    \"batch_size\": \"auto\",\n",
    "    \"learning_rate_multiplier\": \"auto\"\n",
    "  },\n",
    "  \"trained_tokens\": null,\n",
    "  \"error\": {},\n",
    "  \"user_provided_suffix\": null,\n",
    "  \"seed\": 42,\n",
    "  \"estimated_finish\": null,\n",
    "  \"integrations\": [\n",
    "    {\n",
    "      \"type\": \"wandb\",\n",
    "      \"wandb\": {\n",
    "        \"project\": \"OpenAI Finetune\",\n",
    "        \"entity\": \"jimbeno\",\n",
    "        \"run_id\": \"ftjob-83naTXGkuWceaoIsM4iaNXxR\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb8535",
   "metadata": {},
   "source": [
    "## DSPy Signatures, Modules, Instances\n",
    "\n",
    "All the experiments involving GPT models use DSPy signatures and modules. These are defined here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f96e0",
   "metadata": {},
   "source": [
    "### Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "d23b92cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy signature that defines the prompt: review -> classification\n",
    "class Classify(dspy.Signature):\n",
    "    __doc__ = \"\"\"Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\"\"\"\n",
    "\n",
    "    review = dspy.InputField(desc=\"The review text to classify.\")\n",
    "    classification = dspy.OutputField(desc=\"One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "6df11d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy module to classify the sentiment of a review\n",
    "class GPTSentiment(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.Predict(Classify)\n",
    "\n",
    "    def forward(self, review):\n",
    "\n",
    "        prediction = self.generate_answer(review=review)\n",
    "        # Sleep to avoid rate limiting\n",
    "        sleep(0.5)\n",
    "        \n",
    "        return dspy.Prediction(classification=prediction.classification.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "b9a60150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the GPT sentiment module\n",
    "gpt_sentiment = GPTSentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ec1f5c",
   "metadata": {},
   "source": [
    "### Classify with Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "5a8a99bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy signature that defines the prompt: review, classifier_decision -> classification\n",
    "class ClassifyWithPred(dspy.Signature):\n",
    "    __doc__ = \"\"\"Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\"\"\"\n",
    "\n",
    "    review = dspy.InputField(desc=\"The review text to classify.\")\n",
    "    classifier_decision = dspy.InputField(desc=\"The sentiment classification proposed by a model fine-tuned on sentiment.\")\n",
    "    classification = dspy.OutputField(desc=\"One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "e61c275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy module that uses the ELECTRA model to help classify the sentiment of a review\n",
    "class CollabSentiment(dspy.Module):\n",
    "    def __init__(self, model_type='base'):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.Predict(ClassifyWithPred)\n",
    "        self.numeric_dict = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "        \n",
    "        # Determine which model and tokenizer to use\n",
    "        if model_type == 'base':\n",
    "            self.model = electra_base_model\n",
    "            self.model_name = 'google/electra-base-discriminator'\n",
    "        elif model_type == 'large':\n",
    "            self.model = electra_large_model\n",
    "            self.model_name = 'google/electra-large-discriminator'\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Choose 'base' or 'large'.\")\n",
    "\n",
    "        # Load the ELECTRA tokenizer from Hugging Face\n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained(self.model_name)\n",
    "    \n",
    "    def classify_with_electra(self, review):\n",
    "        _, predicted_label, probabilities = predict_sentence(self.model, review, self.tokenizer, self.numeric_dict)\n",
    "        return predicted_label, probabilities\n",
    "    \n",
    "    def model_summary(self):\n",
    "        # Display the model summary to confirm\n",
    "        print(f\"Model Summary for {self.model_name}:\\n\")\n",
    "        \n",
    "        # Print the model architecture\n",
    "        print(self.model)\n",
    "        print(self.model.model)\n",
    "\n",
    "    def forward(self, review):\n",
    "        classifier_decision, probabilities = self.classify_with_electra(review)\n",
    "    \n",
    "        prediction = self.generate_answer(review=review, classifier_decision=classifier_decision)\n",
    "        # Sleep to avoid rate limiting\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        return dspy.Prediction(classification=prediction.classification.lower().strip(),\n",
    "                               classifier_decision=classifier_decision, probabilities=probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "dd35a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Electra Base + GPT sentiment module\n",
    "electra_base_gpt_sentiment = CollabSentiment(model_type='base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "68a363de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary for google/electra-base-discriminator:\n",
      "\n",
      "TorchDDPNeuralClassifier(\n",
      "\tbatch_size=16,\n",
      "\tmax_iter=50,\n",
      "\teta=1e-05,\n",
      "\toptimizer_class=<class 'torch.optim.adamw.AdamW'>,\n",
      "\tl2_strength=0.01,\n",
      "\tgradient_accumulation_steps=2,\n",
      "\tmax_grad_norm=None,\n",
      "\tvalidation_fraction=0.1,\n",
      "\tearly_stopping=score,\n",
      "\tn_iter_no_change=10,\n",
      "\twarm_start=False,\n",
      "\ttol=1e-05,\n",
      "\tfinetune_bert=True,\n",
      "\tpooling=mean,\n",
      "\thidden_dim=1024,\n",
      "\thidden_activation=SwishGLU(\n",
      "  (projection): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (activation): SiLU()\n",
      "),\n",
      "\tnum_layers=2,\n",
      "\tcheckpoint_interval=1,\n",
      "\ttarget_score=None,\n",
      "\tinteractive=True,\n",
      "\tfreeze_bert=False,\n",
      "\tdropout_rate=0.3,\n",
      "\tshow_progress=True,\n",
      "\tadvance_epochs=1,\n",
      "\tuse_zero=True,\n",
      "\tscheduler_class=<class 'torch.optim.lr_scheduler.CosineAnnealingWarmRestarts'>)\n",
      "DistributedDataParallel(\n",
      "  (module): BERTClassifier(\n",
      "    (bert): ElectraModel(\n",
      "      (embeddings): ElectraEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): ElectraEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x ElectraLayer(\n",
      "            (attention): ElectraAttention(\n",
      "              (self): ElectraSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): ElectraSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): ElectraIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): ElectraOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (custom_pooling): PoolingLayer()\n",
      "    (classifier): Classifier(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=1024, bias=True)\n",
      "        (1): SwishGLU(\n",
      "          (projection): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (2): Dropout(p=0.3, inplace=False)\n",
      "        (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (4): SwishGLU(\n",
      "          (projection): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (5): Dropout(p=0.3, inplace=False)\n",
      "        (6): Linear(in_features=1024, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Verify the base model is loaded correctly\n",
    "electra_base_gpt_sentiment.model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "f30c0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the ELECTRA Large + GPT sentiment module\n",
    "electra_large_gpt_sentiment = CollabSentiment(model_type='large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "4862d727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary for google/electra-large-discriminator:\n",
      "\n",
      "TorchDDPNeuralClassifier(\n",
      "\tbatch_size=16,\n",
      "\tmax_iter=50,\n",
      "\teta=1e-05,\n",
      "\toptimizer_class=<class 'torch.optim.adamw.AdamW'>,\n",
      "\tl2_strength=0.01,\n",
      "\tgradient_accumulation_steps=2,\n",
      "\tmax_grad_norm=None,\n",
      "\tvalidation_fraction=0.1,\n",
      "\tearly_stopping=score,\n",
      "\tn_iter_no_change=10,\n",
      "\twarm_start=False,\n",
      "\ttol=1e-05,\n",
      "\tfinetune_bert=True,\n",
      "\tpooling=mean,\n",
      "\thidden_dim=1024,\n",
      "\thidden_activation=SwishGLU(\n",
      "  (projection): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (activation): SiLU()\n",
      "),\n",
      "\tnum_layers=2,\n",
      "\tcheckpoint_interval=1,\n",
      "\ttarget_score=None,\n",
      "\tinteractive=True,\n",
      "\tfreeze_bert=False,\n",
      "\tdropout_rate=0.3,\n",
      "\tshow_progress=True,\n",
      "\tadvance_epochs=1,\n",
      "\tuse_zero=True,\n",
      "\tscheduler_class=<class 'torch.optim.lr_scheduler.CosineAnnealingWarmRestarts'>)\n",
      "DistributedDataParallel(\n",
      "  (module): BERTClassifier(\n",
      "    (bert): ElectraModel(\n",
      "      (embeddings): ElectraEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 1024)\n",
      "        (token_type_embeddings): Embedding(2, 1024)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): ElectraEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-23): 24 x ElectraLayer(\n",
      "            (attention): ElectraAttention(\n",
      "              (self): ElectraSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): ElectraSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): ElectraIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): ElectraOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (custom_pooling): PoolingLayer()\n",
      "    (classifier): Classifier(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (1): SwishGLU(\n",
      "          (projection): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (2): Dropout(p=0.3, inplace=False)\n",
      "        (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (4): SwishGLU(\n",
      "          (projection): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (5): Dropout(p=0.3, inplace=False)\n",
      "        (6): Linear(in_features=1024, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Verify the large model is loaded correctly\n",
    "electra_large_gpt_sentiment.model_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354bc73",
   "metadata": {},
   "source": [
    "### Classify with Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "6434c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy signature that defines the prompt: review, classifier_decision -> classification\n",
    "class ClassifyWithProbs(dspy.Signature):\n",
    "    __doc__ = \"\"\"Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\"\"\"\n",
    "\n",
    "    review = dspy.InputField(desc=\"The review text to classify.\")\n",
    "    negative_probability = dspy.InputField(desc=\"Probability the review is negative from a model fine-tuned on sentiment\")\n",
    "    neutral_probability = dspy.InputField(desc=\"Probability the review is neutral from a model fine-tuned on sentiment\")\n",
    "    positive_probability = dspy.InputField(desc=\"Probability the review is positive from a model fine-tuned on sentiment\")\n",
    "    classification = dspy.OutputField(desc=\"One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "id": "36780008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy module that uses the ELECTRA model to help classify the sentiment of a review\n",
    "class CollabSentimentProbs(dspy.Module):\n",
    "    def __init__(self, model_type='base'):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.Predict(ClassifyWithProbs)\n",
    "        self.numeric_dict = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "        \n",
    "        # Determine which model and tokenizer to use\n",
    "        if model_type == 'base':\n",
    "            self.model = electra_base_model\n",
    "            self.model_name = 'google/electra-base-discriminator'\n",
    "        elif model_type == 'large':\n",
    "            self.model = electra_large_model\n",
    "            self.model_name = 'google/electra-large-discriminator'\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Choose 'base' or 'large'.\")\n",
    "\n",
    "        # Load the ELECTRA tokenizer from Hugging Face\n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained(self.model_name)\n",
    "    \n",
    "    def classify_with_electra(self, review):\n",
    "        _, predicted_label, probabilities = predict_sentence(self.model, review, self.tokenizer, self.numeric_dict)\n",
    "        # Convert each probability to a percent\n",
    "        neg_prob = f\"{probabilities[0] * 100:.2f}%\"\n",
    "        neu_prob = f\"{probabilities[1] * 100:.2f}%\"\n",
    "        pos_prob = f\"{probabilities[2] * 100:.2f}%\"\n",
    "        return predicted_label, neg_prob, neu_prob, pos_prob\n",
    "    \n",
    "    def model_summary(self):\n",
    "        # Display the model summary to confirm\n",
    "        print(f\"Model Summary for {self.model_name}:\\n\")\n",
    "        \n",
    "        # Print the model architecture\n",
    "        print(self.model)\n",
    "        print(self.model.model)\n",
    "\n",
    "    def forward(self, review):\n",
    "        classifier_decision, neg_prob, neu_prob, pos_prob = self.classify_with_electra(review)\n",
    "\n",
    "        prediction = self.generate_answer(\n",
    "            review=review, \n",
    "            negative_probability = neg_prob,\n",
    "            neutral_probability = neu_prob,\n",
    "            positive_probability = pos_prob\n",
    "        )\n",
    "\n",
    "         # Sleep to avoid rate limiting\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        return dspy.Prediction(\n",
    "            classification=prediction.classification.lower().strip(),\n",
    "            negative_probability=neg_prob,\n",
    "            neutral_probability=neu_prob,\n",
    "            positive_probability=pos_prob\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "id": "e4af3f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_large_gpt_sentiment_probs = CollabSentimentProbs(model_type='large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349f042",
   "metadata": {},
   "source": [
    "### Classify with Prediction and Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "id": "148bc31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy signature that defines the prompt: review, classifier_decision -> classification\n",
    "class ClassifyWithPredProbs(dspy.Signature):\n",
    "    __doc__ = \"\"\"Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\"\"\"\n",
    "\n",
    "    review = dspy.InputField(desc=\"The review text to classify.\")\n",
    "    classifier_decision = dspy.InputField(desc=\"The sentiment classification proposed by a model fine-tuned on sentiment.\")\n",
    "    negative_probability = dspy.InputField(desc=\"Probability the review is negative\")\n",
    "    neutral_probability = dspy.InputField(desc=\"Probability the review is neutral\")\n",
    "    positive_probability = dspy.InputField(desc=\"Probability the review is positive\")\n",
    "    classification = dspy.OutputField(desc=\"One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "id": "2d18c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy module that uses the ELECTRA model to help classify the sentiment of a review\n",
    "class CollabSentimentPredProbs(dspy.Module):\n",
    "    def __init__(self, model_type='base'):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.Predict(ClassifyWithPredProbs)\n",
    "        self.numeric_dict = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "        \n",
    "        # Determine which model and tokenizer to use\n",
    "        if model_type == 'base':\n",
    "            self.model = electra_base_model\n",
    "            self.model_name = 'google/electra-base-discriminator'\n",
    "        elif model_type == 'large':\n",
    "            self.model = electra_large_model\n",
    "            self.model_name = 'google/electra-large-discriminator'\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Choose 'base' or 'large'.\")\n",
    "\n",
    "        # Load the ELECTRA tokenizer from Hugging Face\n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained(self.model_name)\n",
    "    \n",
    "    def classify_with_electra(self, review):\n",
    "        _, predicted_label, probabilities = predict_sentence(self.model, review, self.tokenizer, self.numeric_dict)\n",
    "        # Convert each probability to a percent\n",
    "        neg_prob = f\"{probabilities[0] * 100:.2f}%\"\n",
    "        neu_prob = f\"{probabilities[1] * 100:.2f}%\"\n",
    "        pos_prob = f\"{probabilities[2] * 100:.2f}%\"\n",
    "        return predicted_label, neg_prob, neu_prob, pos_prob\n",
    "    \n",
    "    def model_summary(self):\n",
    "        # Display the model summary to confirm\n",
    "        print(f\"Model Summary for {self.model_name}:\\n\")\n",
    "        \n",
    "        # Print the model architecture\n",
    "        print(self.model)\n",
    "        print(self.model.model)\n",
    "\n",
    "    def forward(self, review):\n",
    "        classifier_decision, neg_prob, neu_prob, pos_prob = self.classify_with_electra(review)\n",
    "\n",
    "        prediction = self.generate_answer(\n",
    "            review=review, \n",
    "            negative_probability = neg_prob,\n",
    "            neutral_probability = neu_prob,\n",
    "            positive_probability = pos_prob,\n",
    "            classifier_decision=classifier_decision\n",
    "        )\n",
    "\n",
    "         # Sleep to avoid rate limiting\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        return dspy.Prediction(\n",
    "            classification=prediction.classification.lower().strip(), \n",
    "            classifier_decision=classifier_decision, \n",
    "            negative_probability=neg_prob,\n",
    "            neutral_probability=neu_prob,\n",
    "            positive_probability=pos_prob\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "id": "252146f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_large_gpt_sentiment_pred_probs = CollabSentimentPredProbs(model_type='large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14a61a1",
   "metadata": {},
   "source": [
    "### Classify with Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "27e09fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever class to get the top similar reviews\n",
    "class ElectraSentimentRetriever(Retrieve):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        examples: List[dict],\n",
    "        k: int = 3,\n",
    "        max_length: int = 512,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        super().__init__(k=k)\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Precompute embeddings for examples\n",
    "        self.examples = examples\n",
    "        self.example_texts = []\n",
    "        self.example_classes = []\n",
    "        for ex in self.examples:\n",
    "            text = ex.get('review', ex.get('sentence', ''))\n",
    "            classification = ex.get('classification', ex.get('label', ''))\n",
    "            self.example_texts.append(text)\n",
    "            self.example_classes.append(classification)\n",
    "        \n",
    "        # Precompute and store example embeddings\n",
    "        self.example_embeddings = self._get_embeddings(self.example_texts)\n",
    "\n",
    "    def _get_embeddings_batch(self, texts: List[str]) -> torch.Tensor:\n",
    "        encoded = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded['input_ids'].to(self.device)\n",
    "        attention_mask = encoded['attention_mask'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            base_model = self.model.module if hasattr(self.model, 'module') else self.model\n",
    "            electra_outputs = base_model.bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            pooled_output = base_model.custom_pooling(\n",
    "                electra_outputs.last_hidden_state,\n",
    "                attention_mask\n",
    "            )\n",
    "            normalized_embeddings = F.normalize(pooled_output, p=2, dim=1)\n",
    "        \n",
    "        del input_ids, attention_mask, electra_outputs, pooled_output\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return normalized_embeddings\n",
    "\n",
    "    def _get_embeddings(self, texts: List[str]) -> torch.Tensor:\n",
    "        embeddings_list = []\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch_texts = texts[i:i + self.batch_size]\n",
    "            batch_embeddings = self._get_embeddings_batch(batch_texts)\n",
    "            embeddings_list.append(batch_embeddings)\n",
    "        return torch.cat(embeddings_list, dim=0)\n",
    "\n",
    "    def forward(self, query_or_queries: Union[str, List[str]]) -> List[dict]:\n",
    "        # Handle both single query and multiple queries\n",
    "        queries = [query_or_queries] if isinstance(query_or_queries, str) else query_or_queries\n",
    "        queries = [q for q in queries if q]\n",
    "\n",
    "        # Generate embeddings for queries\n",
    "        query_embeddings = self._get_embeddings(queries)\n",
    "        \n",
    "        # Calculate similarities with precomputed example embeddings\n",
    "        similarities = F.cosine_similarity(\n",
    "            query_embeddings.unsqueeze(1),\n",
    "            self.example_embeddings.unsqueeze(0),\n",
    "            dim=2\n",
    "        )\n",
    "        \n",
    "        # Get top indices for each query\n",
    "        top_indices = similarities.argsort(dim=1, descending=True)[:, :self.k * 2]\n",
    "\n",
    "        results = []\n",
    "        if len(queries) == 1:\n",
    "            selected_reviews = {}\n",
    "            \n",
    "            # Try to get balanced examples first\n",
    "            for idx in top_indices[0]:\n",
    "                idx = int(idx)\n",
    "                sentiment = self.example_classes[idx]\n",
    "                if sentiment not in selected_reviews and len(selected_reviews) < self.k:\n",
    "                    selected_reviews[sentiment] = {\n",
    "                        'index': idx,\n",
    "                        'similarity_score': float(similarities[0][idx]),\n",
    "                        'review': self.example_texts[idx],\n",
    "                        'classification': sentiment\n",
    "                    }\n",
    "            \n",
    "            # Fill remaining slots with highest similarity scores\n",
    "            if len(selected_reviews) < self.k:\n",
    "                for idx in top_indices[0]:\n",
    "                    idx = int(idx)\n",
    "                    if len(results) >= self.k:\n",
    "                        break\n",
    "                    if not any(r.get('index') == idx for r in results):\n",
    "                        results.append({\n",
    "                            'index': idx,\n",
    "                            'similarity_score': float(similarities[0][idx]),\n",
    "                            'review': self.example_texts[idx],\n",
    "                            'classification': self.example_classes[idx]\n",
    "                        })\n",
    "            \n",
    "            # Add balanced selections to results\n",
    "            results.extend(selected_reviews.values())\n",
    "            results = results[:self.k]\n",
    "        else:\n",
    "            # Handle multiple queries (if needed)\n",
    "            pass\n",
    "\n",
    "        # Clean up GPU memory\n",
    "        del query_embeddings, similarities\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "id": "5e32ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy signature that defines the prompt: examples, review, classifier_decision -> classification\n",
    "class ClassifyWithPredExamples(dspy.Signature):\n",
    "    __doc__ = \"\"\"Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\"\"\"\n",
    "\n",
    "    examples = dspy.InputField(\n",
    "        desc=\"A list of examples that demonstrate different sentiment classes.\",\n",
    "        format=lambda examples: \"\\n\".join([\n",
    "            f\"- {ex['classification']}: {ex['review']}\"\n",
    "            for i, ex in enumerate(examples)\n",
    "        ]) if isinstance(examples, list) else examples\n",
    "    )\n",
    "    review = dspy.InputField(desc=\"The review text to classify.\")\n",
    "    classifier_decision = dspy.InputField(desc=\"The sentiment classification proposed by a model fine-tuned on sentiment.\")\n",
    "    classification = dspy.OutputField(desc=\"One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\")\n",
    "\n",
    "class CollabSentimentExamples(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained('google/electra-large-discriminator')\n",
    "\n",
    "        self.example_data = [\n",
    "            {\n",
    "                'review': row['sentence'],\n",
    "                'classification': self.numeric_dict[row['label']] if isinstance(row['label'], (int, np.integer)) else row['label']\n",
    "            }\n",
    "            for _, row in val_df.iterrows()\n",
    "        ]\n",
    "        self.retrieve = ElectraSentimentRetriever(\n",
    "            model=electra_large_model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            examples=self.example_data,\n",
    "            k=5\n",
    "        )\n",
    "\n",
    "        self.generate_answer = dspy.Predict(ClassifyWithPredExamples)\n",
    "        \n",
    "        self.numeric_dict = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "    def classify_with_electra(self, review):\n",
    "        _, predicted_label, probabilities = predict_sentence(electra_large_model, review, self.tokenizer, self.numeric_dict)\n",
    "        return predicted_label, probabilities\n",
    "\n",
    "    def forward(self, review):\n",
    "        examples = self.retrieve(review)\n",
    "        \n",
    "        classifier_decision, probabilities = self.classify_with_electra(review)\n",
    "\n",
    "        prediction = self.generate_answer(\n",
    "            review=review, \n",
    "            examples=examples, \n",
    "            classifier_decision=classifier_decision\n",
    "        )\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            examples=examples,\n",
    "            classification=prediction.classification,\n",
    "            classifier_decision=classifier_decision,\n",
    "            probabilities=probabilities\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "323f3a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_large_gpt_sentiment_examples = CollabSentimentExamples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "id": "1243db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-mini-2024-07-18', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "6626f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example_result = electra_large_gpt_sentiment_examples(test_df['sentence'][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "bb8fdd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    examples=[{'index': 4118, 'similarity_score': 0.9966667294502258, 'review': 'Commented to waiter.', 'classification': 'neutral'}, {'index': 3076, 'similarity_score': 0.9965805411338806, 'review': 'They started the services anyway.', 'classification': 'neutral'}, {'index': 4947, 'similarity_score': 0.9962669610977173, 'review': 'Never once had any ?', 'classification': 'neutral'}, {'index': 2584, 'similarity_score': 0.9962118864059448, 'review': 'I was wrong.', 'classification': 'neutral'}, {'index': 489, 'similarity_score': 0.9955471158027649, 'review': 'Long Time Dead ?', 'classification': 'neutral'}],\n",
       "    classification='neutral',\n",
       "    classifier_decision='neutral',\n",
       "    probabilities=[0.021250536665320396, 0.957855224609375, 0.020894253626465797]\n",
       ")"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "e6ebed04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Examples: A list of examples that demonstrate different sentiment classes.\n",
      "\n",
      "Review: The review text to classify.\n",
      "\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Examples:\n",
      "- neutral: Commented to waiter.\n",
      "- neutral: They started the services anyway.\n",
      "- neutral: Never once had any ?\n",
      "- neutral: I was wrong.\n",
      "- neutral: Long Time Dead ?\n",
      "\n",
      "Review: How is possible?\n",
      "\n",
      "Classifier Decision: neutral\n",
      "\n",
      "Classification:\u001b[32m neutral\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nExamples: A list of examples that demonstrate different sentiment classes.\\n\\nReview: The review text to classify.\\n\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\n\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nExamples:\\n- neutral: Commented to waiter.\\n- neutral: They started the services anyway.\\n- neutral: Never once had any ?\\n- neutral: I was wrong.\\n- neutral: Long Time Dead ?\\n\\nReview: How is possible?\\n\\nClassifier Decision: neutral\\n\\nClassification:\\x1b[32m neutral\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 750,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016f3fab",
   "metadata": {},
   "source": [
    "### Classify with Balanced Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "id": "24a8301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreiver class to get the top similar reviews balanced across classes\n",
    "class BalancedElectraSentimentRetriever(Retrieve):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        examples: List[dict],\n",
    "        k: int = 3,\n",
    "        max_length: int = 512,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        super().__init__(k=k)\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Precompute embeddings for examples\n",
    "        self.examples = examples\n",
    "        self.example_texts = []\n",
    "        self.example_classes = []\n",
    "        \n",
    "        # Create class indices for balanced retrieval\n",
    "        self.class_indices = defaultdict(list)\n",
    "        \n",
    "        # Process examples and create indices\n",
    "        for i, ex in enumerate(self.examples):\n",
    "            text = ex.get('review', ex.get('sentence', ''))\n",
    "            classification = ex.get('classification', ex.get('label', ''))\n",
    "            self.example_texts.append(text)\n",
    "            self.example_classes.append(classification)\n",
    "            self.class_indices[classification].append(i)\n",
    "        \n",
    "        # Precompute and store example embeddings\n",
    "        self.example_embeddings = self._get_embeddings(self.example_texts)\n",
    "\n",
    "    def _get_embeddings_batch(self, texts: List[str]) -> torch.Tensor:\n",
    "        encoded = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded['input_ids'].to(self.device)\n",
    "        attention_mask = encoded['attention_mask'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            base_model = self.model.module if hasattr(self.model, 'module') else self.model\n",
    "            electra_outputs = base_model.bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            pooled_output = base_model.custom_pooling(\n",
    "                electra_outputs.last_hidden_state,\n",
    "                attention_mask\n",
    "            )\n",
    "            normalized_embeddings = F.normalize(pooled_output, p=2, dim=1)\n",
    "        \n",
    "        del input_ids, attention_mask, electra_outputs, pooled_output\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return normalized_embeddings\n",
    "\n",
    "    def _get_embeddings(self, texts: List[str]) -> torch.Tensor:\n",
    "        embeddings_list = []\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch_texts = texts[i:i + self.batch_size]\n",
    "            batch_embeddings = self._get_embeddings_batch(batch_texts)\n",
    "            embeddings_list.append(batch_embeddings)\n",
    "        return torch.cat(embeddings_list, dim=0)\n",
    "\n",
    "    def forward(self, query_or_queries: Union[str, List[str]]) -> List[dict]:\n",
    "        # Handle both single query and multiple queries\n",
    "        queries = [query_or_queries] if isinstance(query_or_queries, str) else query_or_queries\n",
    "        queries = [q for q in queries if q]\n",
    "\n",
    "        # Generate embeddings for queries\n",
    "        query_embeddings = self._get_embeddings(queries)\n",
    "        \n",
    "        # Calculate similarities with precomputed example embeddings\n",
    "        similarities = F.cosine_similarity(\n",
    "            query_embeddings.unsqueeze(1),\n",
    "            self.example_embeddings.unsqueeze(0),\n",
    "            dim=2\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        if len(queries) == 1:\n",
    "            sorted_results = []\n",
    "            \n",
    "            # Process classes in order: negative, neutral, positive\n",
    "            for class_label in ['negative', 'neutral', 'positive']:\n",
    "                if class_label not in self.class_indices:\n",
    "                    continue\n",
    "                \n",
    "                # Get indices and similarities for this class\n",
    "                class_idx = self.class_indices[class_label]\n",
    "                class_similarities = similarities[0][class_idx]\n",
    "                \n",
    "                # Get top k most similar examples from this class\n",
    "                top_k = min(self.k, len(class_idx))\n",
    "                class_rankings = class_similarities.argsort(descending=True)[:top_k]\n",
    "                \n",
    "                # Store this class's examples, sorted by similarity\n",
    "                class_results = []\n",
    "                for idx in class_rankings:\n",
    "                    original_idx = class_idx[idx]\n",
    "                    class_results.append({\n",
    "                        'index': int(original_idx),\n",
    "                        'similarity_score': float(similarities[0][original_idx]),\n",
    "                        'review': self.example_texts[original_idx],\n",
    "                        'classification': class_label\n",
    "                    })\n",
    "                    \n",
    "                # Sort by similarity within this class block and add to results\n",
    "                class_results.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "                sorted_results.extend(class_results)\n",
    "            \n",
    "            return sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "id": "b06d04d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy signature that defines the prompt: examples, review, classifier_decision -> classification\n",
    "class ClassifyWithPredExamplesBalanced(dspy.Signature):\n",
    "    __doc__ = \"\"\"Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\"\"\"\n",
    "\n",
    "    examples = dspy.InputField(\n",
    "        desc=\"A list of examples that demonstrate different sentiment classes.\",\n",
    "        format=lambda examples: \"\\n\".join([\n",
    "            f\"- {ex['classification']}: {ex['review']}\"\n",
    "            for i, ex in enumerate(examples)\n",
    "        ]) if isinstance(examples, list) else examples\n",
    "    )\n",
    "    review = dspy.InputField(desc=\"The review text to classify.\")\n",
    "    classifier_decision = dspy.InputField(desc=\"The sentiment classification proposed by a model fine-tuned on sentiment.\")\n",
    "    classification = dspy.OutputField(desc=\"One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\")\n",
    "\n",
    "class CollabSentimentExamplesBalanced(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained('google/electra-large-discriminator')\n",
    "        # Create example data with both text and classification\n",
    "        self.example_data = [\n",
    "            {\n",
    "                'review': row['sentence'],\n",
    "                'classification': self.numeric_dict[row['label']] if isinstance(row['label'], (int, np.integer)) else row['label']\n",
    "            }\n",
    "            for _, row in val_df.iterrows()\n",
    "        ]\n",
    "        self.retrieve = BalancedElectraSentimentRetriever(\n",
    "            model=electra_large_model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            examples=self.example_data,\n",
    "            k=2\n",
    "        )\n",
    "\n",
    "        self.generate_answer = dspy.Predict(ClassifyWithPredExamplesBalanced)\n",
    "        \n",
    "        self.numeric_dict = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "    def classify_with_electra(self, review):\n",
    "        _, predicted_label, probabilities = predict_sentence(electra_large_model, review, self.tokenizer, self.numeric_dict)\n",
    "        return predicted_label, probabilities\n",
    "\n",
    "    def forward(self, review):\n",
    "        # Now examples will include both text and classification\n",
    "        examples = self.retrieve(review)\n",
    "        \n",
    "        classifier_decision, probabilities = self.classify_with_electra(review)\n",
    "\n",
    "        prediction = self.generate_answer(\n",
    "            review=review, \n",
    "            examples=examples, \n",
    "            classifier_decision=classifier_decision\n",
    "        )\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            examples=examples,\n",
    "            classification=prediction.classification,\n",
    "            classifier_decision=classifier_decision,\n",
    "            probabilities=probabilities\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "id": "484b7fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_large_gpt_sentiment_examples_balanced = CollabSentimentExamplesBalanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "id": "d41e41dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-mini-2024-07-18', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "f5a54b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_test_example_result = electra_large_gpt_sentiment_examples_balanced(test_df['sentence'][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "id": "5870de4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    examples=[{'index': 751, 'similarity_score': 0.9963672161102295, 'review': \"It's littered with college kids and their laptops.\", 'classification': 'negative'}, {'index': 4621, 'similarity_score': 0.9959249496459961, 'review': 'Or we could watch paint dry...', 'classification': 'negative'}, {'index': 4935, 'similarity_score': 0.9972602128982544, 'review': 'After nothing was helping, my doctor said my next step would be accutane.', 'classification': 'neutral'}, {'index': 4956, 'similarity_score': 0.9970909357070923, 'review': 'They have words, of course in Spanish.', 'classification': 'neutral'}, {'index': 2751, 'similarity_score': 0.9951156377792358, 'review': 'And I understand that you have a family life.', 'classification': 'positive'}, {'index': 4399, 'similarity_score': 0.9948238730430603, 'review': 'Of the 5 nightclubs we visited, we only paid for 2 and the most was $20.', 'classification': 'positive'}],\n",
       "    classification='neutral',\n",
       "    classifier_decision='neutral',\n",
       "    probabilities=[0.04023057967424393, 0.938576877117157, 0.02119256742298603]\n",
       ")"
      ]
     },
     "execution_count": 908,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_test_example_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "id": "e4f32c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Examples: A list of examples that demonstrate different sentiment classes.\n",
      "\n",
      "Review: The review text to classify.\n",
      "\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Examples:\n",
      "- negative: It's littered with college kids and their laptops.\n",
      "- negative: Or we could watch paint dry...\n",
      "- neutral: After nothing was helping, my doctor said my next step would be accutane.\n",
      "- neutral: They have words, of course in Spanish.\n",
      "- positive: And I understand that you have a family life.\n",
      "- positive: Of the 5 nightclubs we visited, we only paid for 2 and the most was $20.\n",
      "\n",
      "Review: She answered the phone by saying \"yeah?\".\n",
      "\n",
      "Classifier Decision: neutral\n",
      "\n",
      "Classification:\u001b[32m neutral\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nClassify the sentiment of a review as either \\'negative\\', \\'neutral\\', or \\'positive\\'.\\n\\n---\\n\\nFollow the following format.\\n\\nExamples: A list of examples that demonstrate different sentiment classes.\\n\\nReview: The review text to classify.\\n\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\n\\nClassification: One word representing the sentiment classification: \\'negative\\', \\'neutral\\', or \\'positive\\' (do not repeat the field name, do not use \\'mixed\\')\\n\\n---\\n\\nExamples:\\n- negative: It\\'s littered with college kids and their laptops.\\n- negative: Or we could watch paint dry...\\n- neutral: After nothing was helping, my doctor said my next step would be accutane.\\n- neutral: They have words, of course in Spanish.\\n- positive: And I understand that you have a family life.\\n- positive: Of the 5 nightclubs we visited, we only paid for 2 and the most was $20.\\n\\nReview: She answered the phone by saying \"yeah?\".\\n\\nClassifier Decision: neutral\\n\\nClassification:\\x1b[32m neutral\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 909,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7790140",
   "metadata": {},
   "source": [
    "### Classify with Predictions, Probabilities, and Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "474d18e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy signature that defines the prompt: examples, review, classifier_decision, negative_probability, neutral_probability, positive_probability -> classification\n",
    "class ClassifyWithPredProbsExamples(dspy.Signature):\n",
    "    __doc__ = \"\"\"Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\"\"\"\n",
    "\n",
    "    examples = dspy.InputField(\n",
    "        desc=\"A list of examples that demonstrate different sentiment classes.\",\n",
    "        format=lambda examples: \"\\n\".join([\n",
    "            f\"- {ex['classification']}: {ex['review']}\"\n",
    "            for i, ex in enumerate(examples)\n",
    "        ]) if isinstance(examples, list) else examples\n",
    "    )\n",
    "    review = dspy.InputField(desc=\"The review text to classify.\")\n",
    "    classifier_decision = dspy.InputField(desc=\"The sentiment classification proposed by a model fine-tuned on sentiment.\")\n",
    "    negative_probability = dspy.InputField(desc=\"Probability the review is negative\")\n",
    "    neutral_probability = dspy.InputField(desc=\"Probability the review is neutral\")\n",
    "    positive_probability = dspy.InputField(desc=\"Probability the review is positive\")\n",
    "    classification = dspy.OutputField(desc=\"One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\")\n",
    "\n",
    "class CollabSentimentPredProbsExamples(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained('google/electra-large-discriminator')\n",
    "        # Create example data with both text and classification\n",
    "        self.example_data = [\n",
    "            {\n",
    "                'review': row['sentence'],\n",
    "                'classification': self.numeric_dict[row['label']] if isinstance(row['label'], (int, np.integer)) else row['label']\n",
    "            }\n",
    "            for _, row in val_df.iterrows()\n",
    "        ]\n",
    "        self.retrieve = ElectraSentimentRetriever(\n",
    "            model=electra_large_model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            examples=self.example_data,\n",
    "            k=5\n",
    "        )\n",
    "\n",
    "        self.generate_answer = dspy.Predict(ClassifyWithPredProbsExamples)\n",
    "        \n",
    "        self.numeric_dict = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "    \n",
    "    def classify_with_electra(self, review):\n",
    "        _, predicted_label, probabilities = predict_sentence(electra_large_model, review, self.tokenizer, self.numeric_dict)\n",
    "        # Convert each probability to a percent\n",
    "        neg_prob = f\"{probabilities[0] * 100:.2f}%\"\n",
    "        neu_prob = f\"{probabilities[1] * 100:.2f}%\"\n",
    "        pos_prob = f\"{probabilities[2] * 100:.2f}%\"\n",
    "        return predicted_label, neg_prob, neu_prob, pos_prob\n",
    "\n",
    "    def forward(self, review):\n",
    "        examples = self.retrieve(review)\n",
    "        \n",
    "        classifier_decision, neg_prob, neu_prob, pos_prob = self.classify_with_electra(review)\n",
    "\n",
    "        prediction = self.generate_answer(\n",
    "            examples=examples,\n",
    "            review=review, \n",
    "            negative_probability = neg_prob,\n",
    "            neutral_probability = neu_prob,\n",
    "            positive_probability = pos_prob,\n",
    "            classifier_decision=classifier_decision\n",
    "        )\n",
    "\n",
    "        return dspy.Prediction(\n",
    "            examples=examples,\n",
    "            classification=prediction.classification.lower().strip(), \n",
    "            classifier_decision=classifier_decision, \n",
    "            negative_probability=neg_prob,\n",
    "            neutral_probability=neu_prob,\n",
    "            positive_probability=pos_prob,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "5ea5a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_large_gpt_sentiment_pred_probs_examples = CollabSentimentPredProbsExamples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77fefef",
   "metadata": {},
   "source": [
    "## Metric\n",
    "\n",
    "This is the metric function that determines if the DSPy output is a match to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "222c46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a classification match metric that is flexible for the DSPy optimizer prompt variations\n",
    "def classification_match(review, pred, trace=None, frac=1.0):\n",
    "    # Define the possible field names, based on the prompts we saw in the COPRO optimizer\n",
    "    field_names = ['classification', 'sentiment_classification']\n",
    "    \n",
    "    # Get the actual classification from the review\n",
    "    actual_classification = None\n",
    "    for field in field_names:\n",
    "        if hasattr(review, field):\n",
    "            actual_classification = getattr(review, field)\n",
    "            break\n",
    "    if actual_classification is None:\n",
    "        raise ValueError(\"No classification field found in the review object\")\n",
    "    \n",
    "    # Get the predicted classification\n",
    "    predicted_classification = None\n",
    "    for field in field_names:\n",
    "        if hasattr(pred, field):\n",
    "            predicted_classification = getattr(pred, field)\n",
    "            break\n",
    "    if predicted_classification is None:\n",
    "        raise ValueError(\"No classification field found in the prediction object\")\n",
    "    \n",
    "    # Clean up the predicted classification\n",
    "    predicted_classification = predicted_classification.lower().strip()\n",
    "    if ':' in predicted_classification:\n",
    "        predicted_classification = predicted_classification.split(':')[-1].strip()\n",
    "    \n",
    "    # Extract just the sentiment if there's additional information\n",
    "    sentiment_words = ['positive', 'neutral', 'negative']\n",
    "    for word in sentiment_words:\n",
    "        if word in predicted_classification:\n",
    "            predicted_classification = word\n",
    "            break\n",
    "    \n",
    "    # Perform the matching\n",
    "    if isinstance(actual_classification, str):\n",
    "        return dsp.answer_match(predicted_classification, [actual_classification], frac=frac)\n",
    "    elif isinstance(actual_classification, list):\n",
    "        return dsp.answer_match(predicted_classification, actual_classification, frac=frac)\n",
    "    else:\n",
    "        raise TypeError(\"Unexpected type for classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88743b",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "\n",
    "Functions to evaluate the performance of an experimental run with a DSPy template and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "7f9badae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_results_to_df(results):\n",
    "    # Extract the list of tuples from the results tuple\n",
    "    examples = results[1]\n",
    "    \n",
    "    # Initialize an empty list to store the extracted data\n",
    "    data = []\n",
    "    \n",
    "    # Iterate over the list of tuples\n",
    "    for example, prediction, match in examples:\n",
    "        # Base data that's always present\n",
    "        row_data = {\n",
    "            'review': example['review'],\n",
    "            'classification': example['classification'],\n",
    "            'prediction': prediction.classification,\n",
    "            'match': match\n",
    "        }\n",
    "        \n",
    "        # Safely add classifier_decision if it exists\n",
    "        if hasattr(prediction, 'classifier_decision'):\n",
    "            row_data['classifier_decision'] = prediction.classifier_decision\n",
    "        \n",
    "        # Safely add probabilities as a list if they exist\n",
    "        if hasattr(prediction, 'probabilities'):\n",
    "            row_data['probabilities'] = prediction.probabilities\n",
    "    \n",
    "        data.append(row_data)\n",
    "    \n",
    "    # Convert the list to a DataFrame\n",
    "    results_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Print column names and their presence\n",
    "    print(\"\\nColumns in results DataFrame:\")\n",
    "    for col in results_df.columns:\n",
    "        print(f\"{col:<20} {results_df[col].count():>6} values\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91b302ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data_df, model_name=None):\n",
    "    # Set the dictionaries to convert label formats\n",
    "    label_dict = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "    numeric_dict = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "    # Set the y_test and y_pred variables\n",
    "    y_test = data_df['classification']\n",
    "    y_pred = data_df['prediction']\n",
    "\n",
    "    # Convert text labels to numeric labels\n",
    "    y_test_numeric = np.array([label_dict[label] for label in y_test])\n",
    "    y_pred_numeric = np.array([label_dict[label] for label in y_pred])\n",
    "\n",
    "    # Use the DataWaza eval_model function\n",
    "    metrics = eval_model(\n",
    "        y_test=y_test_numeric,\n",
    "        y_pred=y_pred_numeric,\n",
    "        class_map=numeric_dict,\n",
    "        estimator=None,\n",
    "        x_test=None,\n",
    "        class_type='multi' if len(numeric_dict) > 2 else 'binary',\n",
    "        model_name=model_name,\n",
    "        plot=False,\n",
    "        save_plots=True,\n",
    "        save_dir='saves',\n",
    "        debug=False,\n",
    "        pos_label=None,\n",
    "        decimal=4,\n",
    "        return_metrics=True,\n",
    "        threshold=0.5,\n",
    "        wandb_run=None\n",
    "    )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "fd03bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_invalid_predictions(results_df, dataset):\n",
    "    \"\"\"\n",
    "    Check for and report invalid predictions without modifying data\n",
    "    \n",
    "    Parameters:\n",
    "    results_df: DataFrame with predictions\n",
    "    dataset: Original test dataset\n",
    "    \n",
    "    Returns:\n",
    "    bool: Whether invalid predictions were found\n",
    "    \"\"\"\n",
    "    valid_predictions = {'negative', 'neutral', 'positive'}\n",
    "    \n",
    "    # Show all unique predictions and their counts\n",
    "    print(\"\\nPrediction value counts:\")\n",
    "    value_counts = results_df['prediction'].value_counts()\n",
    "    for value, count in value_counts.items():\n",
    "        print(f\"{value:<20} {count:>6}\")\n",
    "    \n",
    "    # Find invalid predictions\n",
    "    invalid_mask = ~results_df['prediction'].isin(valid_predictions)\n",
    "    invalid_indices = results_df[invalid_mask].index\n",
    "    \n",
    "    if len(invalid_indices) > 0:\n",
    "        print(f\"\\nFound {len(invalid_indices)} invalid predictions at row indices:\")\n",
    "        print(invalid_indices.tolist())\n",
    "        \n",
    "        print(\"\\nInvalid prediction rows:\")\n",
    "        print(dataset.loc[invalid_indices])\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "ee4008cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_experiment(lm, name, instance, dataset, examples, results=None, notes=None, save_dir='research', save_results=True):\n",
    "    \"\"\"\n",
    "    Evaluate a model experiment and return detailed results and metrics\n",
    "    \n",
    "    Parameters:\n",
    "    lm (str): Language model identifier (e.g., 'gpt-4o-mini-2024-07-18')\n",
    "    name (str): Experiment name (e.g., 'e3_g4om_ebft')\n",
    "    instance (str): Model instance name (e.g., 'electra_base_gpt_sentiment')\n",
    "    dataset (pd.DataFrame): Test dataset\n",
    "    examples (list): Test dataset in form of list of DSPy examples\n",
    "    results(pd.DataFrame): Results DataFrame (default: None)\n",
    "    notes (str): Additional notes for the experiment (default: None)\n",
    "    save_dir (str): Directory to save results and metrics (default: 'research')\n",
    "    save_results (bool): Whether to save results to files (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (results_df, metrics_dict)\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    print('-' * 80)\n",
    "    print(f\"Experiment: {name.upper()}\")\n",
    "    print('-' * 80)\n",
    "    print(f\"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Notes: {notes}\") if notes is not None else None\n",
    "    print(f\"Model: {lm}\")\n",
    "    print(f\"Instance: {instance}\")\n",
    "    print(f\"Dataset shape: {list(dataset.shape)}\")\n",
    "    print(f\"Examples length: {len(examples)}\")\n",
    "    print(f\"Results shape: {list(results.shape)}\") if results is not None else None\n",
    "    print(f\"Save directory: {save_dir}\") if save_results else None\n",
    "    \n",
    "    if results is None:\n",
    "\n",
    "        # Configure DSPy\n",
    "        dspy.settings.configure(\n",
    "            lm=dspy.OpenAI(model=lm, api_key=openai_key, max_tokens=8192),\n",
    "            experimental=False\n",
    "        )\n",
    "        dsp.settings.show_guidelines = True\n",
    "        \n",
    "        # Create evaluator\n",
    "        evaluator = Evaluate(\n",
    "            devset=examples,\n",
    "            num_threads=1,\n",
    "            display_progress=True,\n",
    "            display_table=False,\n",
    "            return_outputs=True\n",
    "        )\n",
    "    \n",
    "        # Run evaluation\n",
    "        print(f\"\\nRunning evaluation...\")\n",
    "        results = evaluator(eval(instance), metric=classification_match)\n",
    "        \n",
    "        # Convert results to DataFrame\n",
    "        results_df = convert_results_to_df(results)\n",
    "        return_results = True\n",
    "    else:\n",
    "        results_df = results.copy()\n",
    "        return_results = False\n",
    "    \n",
    "    # Add source information\n",
    "    if 'source' in dataset.columns:\n",
    "        results_df['source'] = dataset['source']\n",
    "\n",
    "    # Print the lengths of each subset of 'source' column\n",
    "    print(\"\\nSource value counts:\")\n",
    "    for source in results_df['source'].unique():\n",
    "        print(f\"{source:<20} {len(results_df[results_df['source'] == source]):>6}\")\n",
    "    \n",
    "    # Check for invalid predictions\n",
    "    has_invalid = check_invalid_predictions(results_df, dataset)\n",
    "    \n",
    "    if has_invalid:\n",
    "        print(\"\\nWARNING: Invalid predictions found. Please review the above details and re-run problematic cases.\")\n",
    "        return results_df, None\n",
    "\n",
    "    # Initialize metrics dictionary\n",
    "    metrics_dict = {\n",
    "        'experiment_name': name.upper(),\n",
    "        'start_time': start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'notes': notes,\n",
    "        'model': lm,\n",
    "        'instance': instance,\n",
    "        'dataset_shape': list(dataset.shape),\n",
    "        'examples_length': len(examples),\n",
    "        'results_shape': list(results_df.shape),\n",
    "        'save_directory': save_dir\n",
    "    }\n",
    "    \n",
    "    # Calculate and store overall metrics\n",
    "    overall_metrics = evaluate_model(results_df, model_name=name.upper())\n",
    "    metrics_dict['merged_local'] = extract_metrics_from_report(overall_metrics)\n",
    "    \n",
    "    # Calculate metrics for each source\n",
    "    sources = {\n",
    "        'dynasent_r1': 'DYN-R1',\n",
    "        'dynasent_r2': 'DYN-R2',\n",
    "        'sst_local': 'SST'\n",
    "    }\n",
    "\n",
    "    for source, id in sources.items():\n",
    "        source_df = results_df[results_df['source'] == source]\n",
    "        source_metrics = evaluate_model(source_df, model_name=f\"{name.upper()}-{id}\")\n",
    "        metrics_dict[source] = extract_metrics_from_report(source_metrics)\n",
    "    \n",
    "    # Save results if requested\n",
    "    if save_results:\n",
    "        print(f\"Saving results to '{save_dir}'...\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        results_df.to_csv(f\"{save_dir}/{name}_results.csv\", index=False)\n",
    "        \n",
    "        # Filter out any tqdm objects or other non-serializable items from metrics_dict\n",
    "        serializable_metrics = {}\n",
    "        for key, value in metrics_dict.items():\n",
    "            if isinstance(value, (dict, list, str, int, float, bool, type(None))):\n",
    "                serializable_metrics[key] = value\n",
    "                \n",
    "        with open(f\"{save_dir}/{name}_metrics.json\", 'w') as f:\n",
    "            json.dump(serializable_metrics, f, indent=2)\n",
    "    \n",
    "    # Print summary metrics\n",
    "    print(\"\\nSummary Metrics:\")\n",
    "    print(f\"{'Dataset':<12}\\tF1 (macro)\\tAccuracy\")\n",
    "    print(\"-\" * 45)\n",
    "    print(f\"{'Merged':<12}\\t{metrics_dict['merged_local']['macro avg']['f1-score']*100:>9.2f}\\t{metrics_dict['merged_local']['accuracy']*100:>8.2f}\")\n",
    "    print(f\"{'DynaSent R1':<12}\\t{metrics_dict['dynasent_r1']['macro avg']['f1-score']*100:>9.2f}\\t{metrics_dict['dynasent_r1']['accuracy']*100:>8.2f}\")\n",
    "    print(f\"{'DynaSent R2':<12}\\t{metrics_dict['dynasent_r2']['macro avg']['f1-score']*100:>9.2f}\\t{metrics_dict['dynasent_r2']['accuracy']*100:>8.2f}\")\n",
    "    print(f\"{'SST-3':<12}\\t{metrics_dict['sst_local']['macro avg']['f1-score']*100:>9.2f}\\t{metrics_dict['sst_local']['accuracy']*100:>8.2f}\")\n",
    "        \n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    print(f\"\\nEvaluation completed\")\n",
    "    print(f\"End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Duration: {duration}\")\n",
    "    metrics_dict['end_time'] = end_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    metrics_dict['duration'] = str(duration)\n",
    "    \n",
    "    # Return results if it was not provided, and metrics\n",
    "    if return_results:\n",
    "        return results_df, metrics_dict\n",
    "    else:\n",
    "        return metrics_dict\n",
    "\n",
    "def extract_metrics_from_report(report):\n",
    "    \"\"\"Helper function to extract metrics from classification report\"\"\"\n",
    "    return {\n",
    "        'negative': report['negative'],\n",
    "        'neutral': report['neutral'],\n",
    "        'positive': report['positive'],\n",
    "        'accuracy': report['accuracy'],\n",
    "        'macro avg': report['macro avg'],\n",
    "        'weighted avg': report['weighted avg']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e179d1",
   "metadata": {},
   "source": [
    "## GPT Minimal Classifier that Mirrors Fine-Tuning Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "9f54a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to classify the sentiment of a review that aligns with the fine-tuning format\n",
    "class GPTMinSentiment:\n",
    "    def __init__(self, model=None):\n",
    "        \"\"\"\n",
    "        Initializes the sentiment model with the specified model name.\n",
    "        Args:\n",
    "            model (str): The model identifier for the OpenAI API.\n",
    "        \"\"\"\n",
    "        self.client = OpenAI()\n",
    "        self.history = []\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, review):\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': \"You are a model that classifies the sentiment of a review as either 'positive', 'neutral', or 'negative'.\"},\n",
    "            {'role': 'user', 'content': review}\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Store the full conversation\n",
    "        self.history = messages + [\n",
    "            {'role': 'assistant', 'content': response.choices[0].message.content}\n",
    "        ]\n",
    "        \n",
    "        sleep(1)\n",
    "        return {'classification': response.choices[0].message.content.strip().lower()}\n",
    "    \n",
    "    def get_last_conversation(self):\n",
    "        \"\"\"Returns the most recent conversation\"\"\"\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "0f706c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the GPT minimal sentiment module\n",
    "gpt_4o_mini_min_sentiment = GPTMinSentiment(model=\"ft:gpt-4o-mini-2024-07-18:personal::ALnBCKLv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "d4f2d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the GPT minimal sentiment module\n",
    "gpt_4o_min_sentiment = GPTMinSentiment(model=\"ft:gpt-4o-2024-08-06:personal::AM5cg622\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "da3a7fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to classify the sentiment of a review that aligns with the fine-tuning format\n",
    "class GPTMinSentimentWithPred:\n",
    "    def __init__(self, model=None, model_type='base'):\n",
    "        \"\"\"\n",
    "        Initializes the sentiment model with the specified model name.\n",
    "        Args:\n",
    "            model (str): The model identifier for the OpenAI API.\n",
    "        \"\"\"\n",
    "        self.client = OpenAI()\n",
    "        self.history = []\n",
    "        self.model = model\n",
    "        self.numeric_dict = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "        # Determine which model and tokenizer to use\n",
    "        if model_type == 'base':\n",
    "            self.electra_model = electra_base_model\n",
    "            self.electra_model_name = 'google/electra-base-discriminator'\n",
    "        elif model_type == 'large':\n",
    "            self.electra_model = electra_large_model\n",
    "            self.electra_model_name = 'google/electra-large-discriminator'\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type. Choose 'base' or 'large'.\")\n",
    "\n",
    "        # Load the ELECTRA tokenizer from Hugging Face\n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained(self.electra_model_name)\n",
    "\n",
    "    def classify_with_electra(self, review):\n",
    "        _, predicted_label, probabilities = predict_sentence(self.electra_model, review, self.tokenizer, self.numeric_dict)\n",
    "        return predicted_label, probabilities\n",
    "\n",
    "    def model_summary(self):\n",
    "        # Display the model summary to confirm\n",
    "        print(f\"Model Summary for {self.electra_model_name}:\\n\")\n",
    "        \n",
    "        # Print the model architecture\n",
    "        print(self.electra_model)\n",
    "        print(self.electra_model.model)\n",
    "\n",
    "    def __call__(self, review):\n",
    "        classifier_decision, probabilities = self.classify_with_electra(review)\n",
    "\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': \"You are a model that classifies the sentiment of a review as either 'positive', 'neutral', or 'negative'. 'Classifier Decision' is the sentiment classification proposed by a model fine-tuned on sentiment.\"},\n",
    "            {'role': 'user', 'content': f'Classifier Decision: {classifier_decision}.\\nReview: {review}'}\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Store the full conversation\n",
    "        self.history = messages + [\n",
    "            {'role': 'assistant', 'content': response.choices[0].message.content}\n",
    "        ]\n",
    "        \n",
    "        sleep(1)\n",
    "        return {'classification': response.choices[0].message.content.strip().lower()}\n",
    "    \n",
    "    def get_last_conversation(self):\n",
    "        \"\"\"Returns the most recent conversation\"\"\"\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "a61200ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the GPT minimal sentiment module\n",
    "gpt_4o_mini_min_sentiment_with_pred = GPTMinSentimentWithPred(model=\"ft:gpt-4o-mini-2024-07-18:personal::ALnBCKLv\", model_type='large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c322c45",
   "metadata": {},
   "source": [
    "### Evaluate GPT-4o-mini FT with prompt that matches fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcdc788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(classifier, data, input_format='examples'):\n",
    "    \"\"\"\n",
    "    Process a batch of reviews through the sentiment classifier.\n",
    "    \n",
    "    Args:\n",
    "        classifier: Instance of GPTMinSentimentV1\n",
    "        data: Either list of Examples or DataFrame\n",
    "        input_format: 'examples' or 'dataframe'\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: review, classification (ground truth), prediction, match\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Convert input to standard format\n",
    "    if input_format == 'examples':\n",
    "        reviews = [ex.review for ex in data]\n",
    "        labels = [ex.classification for ex in data]\n",
    "    else:  # dataframe\n",
    "        reviews = data['sentence'].tolist()\n",
    "        labels = data['label'].tolist()\n",
    "    \n",
    "    # Process each review\n",
    "    for review, true_label in tqdm(zip(reviews, labels), total=len(reviews)):\n",
    "        try:\n",
    "            result = classifier(review=review)\n",
    "            prediction = result['classification']\n",
    "            results.append({\n",
    "                'review': review,\n",
    "                'classification': true_label,\n",
    "                'prediction': prediction,\n",
    "                'match': true_label == prediction\n",
    "            })\n",
    "            sleep(0.2)  # Respect rate limits\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing review: {review[:50]}... Error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d8c6c8",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Note: What follow are the original experiment IDs. The numbering was streamlined in the research paper and will vary in a few cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaa585a",
   "metadata": {},
   "source": [
    "### B3-G4OM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "c48ba00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-mini-2024-07-18', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "d788f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "b3_g4om_result = gpt_sentiment(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "ac3fd1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    classification='negative'\n",
       ")"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b3_g4om_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "3f3e4d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "839b619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluater, setting threads to 1 to avoid rate limiting\n",
    "test_full_evaluater = Evaluate(\n",
    "    devset=test_ex,\n",
    "    num_threads=1,\n",
    "    display_progress=True,\n",
    "    display_table=False,\n",
    "    return_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "0da807f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5245 / 6530  (80.3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [1:29:34<00:00,  1.22it/s]  \n"
     ]
    }
   ],
   "source": [
    "b3_g4om_results = test_full_evaluater(gpt_sentiment, metric=classification_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "dc1c7216",
   "metadata": {},
   "outputs": [],
   "source": [
    "b3_g4om_results_df = convert_results_to_df(b3_g4om_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "644af6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction\n",
       "negative                                                                                                                                                                                                                                                                       2624\n",
       "positive                                                                                                                                                                                                                                                                       2108\n",
       "neutral                                                                                                                                                                                                                                                                        1796\n",
       "review: the review text to classify.\\nclassification: neutral                                                                                                                                                                                                                     1\n",
       "review: the product arrived late and was damaged. \\nclassification: negative\\n\\nreview: the service was okay, nothing special. \\nclassification: neutral\\n\\nreview: i absolutely love this new phone! it works perfectly and has great features. \\nclassification: positive       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display unique value counts for prediction column\n",
    "b3_g4om_results_df['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "617da026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the row IDs for the predictions that start with \"review\"\n",
    "review_indices = b3_g4om_results_df[b3_g4om_results_df['prediction'].str.startswith('review')].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "c94f734e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([4512, 6426], dtype='int64')"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "a472bb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4512</th>\n",
       "      <td>https://www.yelp.com/biz/vegas-discount-nutrition-superstore-las-vegas-8?hrid=QQhKNmf3VF_6r_qAfE8jxg&amp;utm_campaign=www_review_share_popup&amp;utm_medium=copy_link&amp;utm_source=(direct)</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6426</th>\n",
       "      <td>HERE.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                               sentence  \\\n",
       "4512  https://www.yelp.com/biz/vegas-discount-nutrition-superstore-las-vegas-8?hrid=QQhKNmf3VF_6r_qAfE8jxg&utm_campaign=www_review_share_popup&utm_medium=copy_link&utm_source=(direct)   \n",
       "6426                                                                                                                                                                              HERE.   \n",
       "\n",
       "        label       source split  \n",
       "4512  neutral  dynasent_r1  test  \n",
       "6426  neutral  dynasent_r1  test  "
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[review_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "89a219c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b3_g4om_review_4512_retry = gpt_sentiment(review=test_df.loc[4512, 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "c4967c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: https://www.yelp.com/biz/vegas-discount-nutrition-superstore-las-vegas-8?hrid=QQhKNmf3VF_6r_qAfE8jxg&utm_campaign=www_review_share_popup&utm_medium=copy_link&utm_source=(direct)\n",
      "Classification:\u001b[32m Review: The review text to classify.\n",
      "Classification: neutral\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: https://www.yelp.com/biz/vegas-discount-nutrition-superstore-las-vegas-8?hrid=QQhKNmf3VF_6r_qAfE8jxg&utm_campaign=www_review_share_popup&utm_medium=copy_link&utm_source=(direct)\\nClassification:\\x1b[32m Review: The review text to classify.\\nClassification: neutral\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "7f8c9770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update b4_g4om_results_df with the retry result\n",
    "b3_g4om_results_df.at[4512, 'prediction'] = 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "cbc7d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "b3_g4om_review_6426_retry = gpt_sentiment(review=\"HERE. (This is the actual review, please just classify 'HERE.')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "7f116c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: HERE. (This is the actual review, please just classify 'HERE.')\n",
      "Classification:\u001b[32m neutral\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: HERE. (This is the actual review, please just classify 'HERE.')\\nClassification:\\x1b[32m neutral\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "3e7e22bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update b4_g4om_results_df with the retry result\n",
    "b3_g4om_results_df.at[6426, 'prediction'] = b3_g4om_review_6426_retry.classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "92253f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>classification</th>\n",
       "      <th>prediction</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4512</th>\n",
       "      <td>https://www.yelp.com/biz/vegas-discount-nutrition-superstore-las-vegas-8?hrid=QQhKNmf3VF_6r_qAfE8jxg&amp;utm_campaign=www_review_share_popup&amp;utm_medium=copy_link&amp;utm_source=(direct)</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6426</th>\n",
       "      <td>HERE.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                 review  \\\n",
       "4512  https://www.yelp.com/biz/vegas-discount-nutrition-superstore-las-vegas-8?hrid=QQhKNmf3VF_6r_qAfE8jxg&utm_campaign=www_review_share_popup&utm_medium=copy_link&utm_source=(direct)   \n",
       "6426                                                                                                                                                                              HERE.   \n",
       "\n",
       "     classification prediction  match  \n",
       "4512        neutral    neutral   True  \n",
       "6426        neutral    neutral  False  "
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b3_g4om_results_df.loc[review_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "9cfd8a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction\n",
       "negative    2624\n",
       "positive    2108\n",
       "neutral     1798\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display unique value counts for prediction column\n",
    "b3_g4om_results_df['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "971c41d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: B3-G4OM\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-10-28 23:02:18\n",
      "Notes: Baseline 3: Establish GPT-4o-mini baseline with prompt\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Instance: gpt_sentiment\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Results shape: [6530, 5]\n",
      "Save directory: research\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2624\n",
      "positive               2108\n",
      "neutral                1798\n",
      "\n",
      "B3-G4OM Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7980    0.8903    0.8416      2352\n",
      "     neutral     0.7013    0.6894    0.6953      1829\n",
      "    positive     0.8971    0.8050    0.8486      2349\n",
      "\n",
      "    accuracy                         0.8034      6530\n",
      "   macro avg     0.7988    0.7949    0.7952      6530\n",
      "weighted avg     0.8066    0.8034    0.8031      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2094      232        26\n",
      "neutral         377     1261       191\n",
      "positive        153      305      1891\n",
      "\n",
      "\n",
      "B3-G4OM-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8358    0.8442    0.8400      1200\n",
      "     neutral     0.7211    0.8317    0.7724      1200\n",
      "    positive     0.9014    0.7542    0.8212      1200\n",
      "\n",
      "    accuracy                         0.8100      3600\n",
      "   macro avg     0.8194    0.8100    0.8112      3600\n",
      "weighted avg     0.8194    0.8100    0.8112      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1013      171        16\n",
      "neutral         119      998        83\n",
      "positive         80      215       905\n",
      "\n",
      "\n",
      "B3-G4OM-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7165    0.9583    0.8200       240\n",
      "     neutral     0.8239    0.6042    0.6971       240\n",
      "    positive     0.8341    0.7750    0.8035       240\n",
      "\n",
      "    accuracy                         0.7792       720\n",
      "   macro avg     0.7915    0.7792    0.7735       720\n",
      "weighted avg     0.7915    0.7792    0.7735       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        230        7         3\n",
      "neutral          61      145        34\n",
      "positive         30       24       186\n",
      "\n",
      "\n",
      "B3-G4OM-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7800    0.9331    0.8497       912\n",
      "     neutral     0.4958    0.3033    0.3764       389\n",
      "    positive     0.9081    0.8801    0.8939       909\n",
      "\n",
      "    accuracy                         0.8005      2210\n",
      "   macro avg     0.7280    0.7055    0.7067      2210\n",
      "weighted avg     0.7827    0.8005    0.7846      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        851       54         7\n",
      "neutral         197      118        74\n",
      "positive         43       66       800\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    79.52\t   80.34\n",
      "DynaSent R1 \t    81.12\t   81.00\n",
      "DynaSent R2 \t    77.35\t   77.92\n",
      "SST-3       \t    70.67\t   80.05\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-10-28 23:02:18\n",
      "Duration: 0:00:00.442092\n"
     ]
    }
   ],
   "source": [
    "b3_g4om_metrics = evaluate_experiment(\n",
    "    name='B3-G4OM',\n",
    "    notes='Baseline 3: Establish GPT-4o-mini baseline with prompt',\n",
    "    lm='gpt-4o-mini-2024-07-18',\n",
    "    instance='gpt_sentiment',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex,\n",
    "    results=b3_g4om_results_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f8389",
   "metadata": {},
   "source": [
    "### E3-G4OM-EBFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "14993521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-mini-2024-07-18', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "a1757cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "e3_g4om_ebft_result = electra_base_gpt_sentiment(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "1189c5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    classification='negative',\n",
       "    classifier_decision='negative',\n",
       "    probabilities=[0.9988487958908081, 0.0008097602985799313, 0.0003413360973354429]\n",
       ")"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e3_g4om_ebft_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "715b0c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "Classifier Decision: negative\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\nClassifier Decision: negative\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "3fa71270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluater, setting threads to 1 to avoid rate limiting\n",
    "test_full_evaluater = Evaluate(\n",
    "    devset=test_ex,\n",
    "    num_threads=1,\n",
    "    display_progress=True,\n",
    "    display_table=False,\n",
    "    return_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "55cdda6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5443 / 6530  (83.4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [1:37:02<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "e3_g4om_ebft_results = test_full_evaluater(electra_base_gpt_sentiment, metric=classification_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "3a936719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "classifier_decision    6530 values\n",
      "probabilities          6530 values\n"
     ]
    }
   ],
   "source": [
    "e3_g4om_ebft_results_df = convert_results_to_df(e3_g4om_ebft_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "893339ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E3-G4OM-EBFT\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-10-28 23:07:33\n",
      "Notes: Experiment 3: Evaluate prompt-based model collaboration between GPT-4o-mini and Electra Base fine-tuned model\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Instance: electra_base_gpt_sentiment\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Results shape: [6530, 6]\n",
      "Save directory: research\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2528\n",
      "positive               2188\n",
      "neutral                1814\n",
      "\n",
      "E3-G4OM-EBFT Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8307    0.8929    0.8607      2352\n",
      "     neutral     0.7536    0.7474    0.7505      1829\n",
      "    positive     0.9031    0.8412    0.8711      2349\n",
      "\n",
      "    accuracy                         0.8335      6530\n",
      "   macro avg     0.8291    0.8272    0.8274      6530\n",
      "weighted avg     0.8351    0.8335    0.8335      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2100      212        40\n",
      "neutral         290     1367       172\n",
      "positive        138      235      1976\n",
      "\n",
      "\n",
      "E3-G4OM-EBFT-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8946    0.8775    0.8860      1200\n",
      "     neutral     0.7911    0.9058    0.8446      1200\n",
      "    positive     0.9266    0.8100    0.8644      1200\n",
      "\n",
      "    accuracy                         0.8644      3600\n",
      "   macro avg     0.8708    0.8644    0.8650      3600\n",
      "weighted avg     0.8708    0.8644    0.8650      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1053      127        20\n",
      "neutral          56     1087        57\n",
      "positive         68      160       972\n",
      "\n",
      "\n",
      "E3-G4OM-EBFT-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7029    0.9167    0.7957       240\n",
      "     neutral     0.8032    0.6292    0.7056       240\n",
      "    positive     0.8219    0.7500    0.7843       240\n",
      "\n",
      "    accuracy                         0.7653       720\n",
      "   macro avg     0.7760    0.7653    0.7619       720\n",
      "weighted avg     0.7760    0.7653    0.7619       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        220       14         6\n",
      "neutral          56      151        33\n",
      "positive         37       23       180\n",
      "\n",
      "\n",
      "E3-G4OM-EBFT-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7967    0.9068    0.8482       912\n",
      "     neutral     0.5119    0.3316    0.4025       389\n",
      "    positive     0.8957    0.9065    0.9010       909\n",
      "\n",
      "    accuracy                         0.8054      2210\n",
      "   macro avg     0.7348    0.7150    0.7172      2210\n",
      "weighted avg     0.7873    0.8054    0.7915      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        827       71        14\n",
      "neutral         178      129        82\n",
      "positive         33       52       824\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    82.74\t   83.35\n",
      "DynaSent R1 \t    86.50\t   86.44\n",
      "DynaSent R2 \t    76.19\t   76.53\n",
      "SST-3       \t    71.72\t   80.54\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-10-28 23:07:33\n",
      "Duration: 0:00:00.469887\n"
     ]
    }
   ],
   "source": [
    "e3_g4om_ebft_metrics = evaluate_experiment(\n",
    "    name='E3-G4OM-EBFT',\n",
    "    notes='Experiment 3: Evaluate prompt-based model collaboration between GPT-4o-mini and Electra Base fine-tuned model',\n",
    "    lm='gpt-4o-mini-2024-07-18',\n",
    "    instance='electra_base_gpt_sentiment',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex,\n",
    "    results=e3_g4om_ebft_results_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b511c08",
   "metadata": {},
   "source": [
    "### E4-G4OMFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "353b4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='ft:gpt-4o-mini-2024-07-18:personal::AN2RNUvd', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "4f4d8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "e4_g4omft_result = gpt_sentiment(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "7b901cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    classification='negative'\n",
       ")"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e4_g4omft_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "9430de1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "1af2f5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E4-G4OMFT\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-10-28 23:37:00\n",
      "Notes: Experiment 4: Measure impact of fine-tuning on GPT-4o-mini\n",
      "Model: ft:gpt-4o-mini-2024-07-18:personal::AN2RNUvd\n",
      "Instance: gpt_sentiment\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 4252 / 4866  (87.4):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 4866/6530 [1:35:51<30:58,  1.12s/it]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.exporter.otlp.proto.http.trace_exporter:Transient error Service Unavailable encountered while exporting span batch, retrying in 1s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5698 / 6530  (87.3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [2:10:38<00:00,  1.20s/it]   \n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2323\n",
      "positive               2287\n",
      "neutral                1920\n",
      "\n",
      "E4-G4OMFT Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9023    0.8912    0.8967      2352\n",
      "     neutral     0.7786    0.8174    0.7975      1829\n",
      "    positive     0.9213    0.8970    0.9090      2349\n",
      "\n",
      "    accuracy                         0.8726      6530\n",
      "   macro avg     0.8674    0.8685    0.8677      6530\n",
      "weighted avg     0.8745    0.8726    0.8733      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2096      222        34\n",
      "neutral         188     1495       146\n",
      "positive         39      203      2107\n",
      "\n",
      "\n",
      "E4-G4OMFT-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9541    0.8825    0.9169      1200\n",
      "     neutral     0.8050    0.9600    0.8757      1200\n",
      "    positive     0.9632    0.8500    0.9031      1200\n",
      "\n",
      "    accuracy                         0.8975      3600\n",
      "   macro avg     0.9074    0.8975    0.8986      3600\n",
      "weighted avg     0.9074    0.8975    0.8986      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1059      125        16\n",
      "neutral          25     1152        23\n",
      "positive         26      154      1020\n",
      "\n",
      "\n",
      "E4-G4OMFT-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8740    0.9250    0.8988       240\n",
      "     neutral     0.8922    0.7583    0.8198       240\n",
      "    positive     0.8511    0.9292    0.8884       240\n",
      "\n",
      "    accuracy                         0.8708       720\n",
      "   macro avg     0.8724    0.8708    0.8690       720\n",
      "weighted avg     0.8724    0.8708    0.8690       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        222       10         8\n",
      "neutral          27      182        31\n",
      "positive          5       12       223\n",
      "\n",
      "\n",
      "E4-G4OMFT-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8498    0.8936    0.8712       912\n",
      "     neutral     0.5649    0.4139    0.4777       389\n",
      "    positive     0.8944    0.9505    0.9216       909\n",
      "\n",
      "    accuracy                         0.8326      2210\n",
      "   macro avg     0.7697    0.7527    0.7568      2210\n",
      "weighted avg     0.8180    0.8326    0.8227      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        815       87        10\n",
      "neutral         136      161        92\n",
      "positive          8       37       864\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    86.77\t   87.26\n",
      "DynaSent R1 \t    89.86\t   89.75\n",
      "DynaSent R2 \t    86.90\t   87.08\n",
      "SST-3       \t    75.68\t   83.26\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-10-29 01:47:39\n",
      "Duration: 2:10:39.463370\n"
     ]
    }
   ],
   "source": [
    "e4_g4omft_results_df, e4_g4omft_metrics = evaluate_experiment(\n",
    "    name='E4-G4OMFT',\n",
    "    notes='Experiment 4: Measure impact of fine-tuning on GPT-4o-mini',\n",
    "    lm='ft:gpt-4o-mini-2024-07-18:personal::AN2RNUvd',\n",
    "    instance='gpt_sentiment',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea418c",
   "metadata": {},
   "source": [
    "### E5-G4OMFT-EBFT-P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "d256f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='ft:gpt-4o-mini-2024-07-18:personal::AN2RNUvd', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "d155dcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the ELECTRA Base GPT sentiment module\n",
    "e5_g4omft_ebft_p_result = electra_base_gpt_sentiment(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "91b406e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    classification='negative',\n",
       "    classifier_decision='negative',\n",
       "    probabilities=[0.9988487958908081, 0.0008097602985799313, 0.0003413360973354429]\n",
       ")"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e5_g4omft_ebft_p_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "175e1372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "Classifier Decision: negative\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\nClassifier Decision: negative\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "408aab2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E5-G4OMFT-EBFT-P\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-10-29 02:20:06\n",
      "Notes: Experiment 5: Evaluate combined impact of fine-tuning and prompt collaboration between GPT-4o-mini and Electra Base fine-tuned model\n",
      "Model: ft:gpt-4o-mini-2024-07-18:personal::AN2RNUvd\n",
      "Instance: electra_base_gpt_sentiment\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 5348 / 6530  (81.9): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [2:10:51<00:00,  1.20s/it]   \n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "classifier_decision    6530 values\n",
      "probabilities          6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "positive               2398\n",
      "negative               2279\n",
      "neutral                1853\n",
      "\n",
      "E5-G4OMFT-EBFT-P Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8526    0.8261    0.8391      2352\n",
      "     neutral     0.7480    0.7578    0.7529      1829\n",
      "    positive     0.8420    0.8595    0.8506      2349\n",
      "\n",
      "    accuracy                         0.8190      6530\n",
      "   macro avg     0.8142    0.8145    0.8142      6530\n",
      "weighted avg     0.8195    0.8190    0.8191      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1943      249       160\n",
      "neutral         224     1386       219\n",
      "positive        112      218      2019\n",
      "\n",
      "\n",
      "E5-G4OMFT-EBFT-P-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9054    0.7975    0.8480      1200\n",
      "     neutral     0.7870    0.9208    0.8487      1200\n",
      "    positive     0.8692    0.8250    0.8465      1200\n",
      "\n",
      "    accuracy                         0.8478      3600\n",
      "   macro avg     0.8539    0.8478    0.8477      3600\n",
      "weighted avg     0.8539    0.8478    0.8477      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        957      149        94\n",
      "neutral          40     1105        55\n",
      "positive         60      150       990\n",
      "\n",
      "\n",
      "E5-G4OMFT-EBFT-P-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7168    0.8333    0.7707       240\n",
      "     neutral     0.8108    0.6250    0.7059       240\n",
      "    positive     0.7344    0.7833    0.7581       240\n",
      "\n",
      "    accuracy                         0.7472       720\n",
      "   macro avg     0.7540    0.7472    0.7449       720\n",
      "weighted avg     0.7540    0.7472    0.7449       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        200       17        23\n",
      "neutral          45      150        45\n",
      "positive         34       18       188\n",
      "\n",
      "\n",
      "E5-G4OMFT-EBFT-P-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8335    0.8618    0.8474       912\n",
      "     neutral     0.4962    0.3368    0.4012       389\n",
      "    positive     0.8385    0.9252    0.8797       909\n",
      "\n",
      "    accuracy                         0.7955      2210\n",
      "   macro avg     0.7227    0.7079    0.7095      2210\n",
      "weighted avg     0.7762    0.7955    0.7822      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        786       83        43\n",
      "neutral         139      131       119\n",
      "positive         18       50       841\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    81.42\t   81.90\n",
      "DynaSent R1 \t    84.77\t   84.78\n",
      "DynaSent R2 \t    74.49\t   74.72\n",
      "SST-3       \t    70.95\t   79.55\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-10-29 04:30:58\n",
      "Duration: 2:10:52.310521\n"
     ]
    }
   ],
   "source": [
    "e5_g4omft_ebft_p_results_df, e5_g4omft_ebft_p_metrics = evaluate_experiment(\n",
    "    name='E5-G4OMFT-EBFT-P',\n",
    "    notes='Experiment 5: Evaluate combined impact of fine-tuning and prompt collaboration between GPT-4o-mini and Electra Base fine-tuned model',\n",
    "    lm='ft:gpt-4o-mini-2024-07-18:personal::AN2RNUvd',\n",
    "    instance='electra_base_gpt_sentiment',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d033d67",
   "metadata": {},
   "source": [
    "### E6-G4OMFT-EBFT-FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "ae239139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='ft:gpt-4o-mini-2024-07-18:personal::ANVDva8W', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "f8ebc426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the ELECTRA Base GPT sentiment module\n",
    "e6_g4omft_ebft_ft_result = electra_base_gpt_sentiment(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "487d3d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    classification='negative',\n",
       "    classifier_decision='negative',\n",
       "    probabilities=[0.9988487958908081, 0.0008097602985799313, 0.0003413360973354429]\n",
       ")"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e6_g4omft_ebft_ft_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "ffba4059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "Classifier Decision: negative\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\nClassifier Decision: negative\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "24a4cc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E6-G4OMFT-EBFT-FT\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-10-29 06:25:54\n",
      "Notes: Experiment 6: Evaluate deep model collaboration through fine-tuning GPT-4o-mini and Electra Base fine-tuned model\n",
      "Model: ft:gpt-4o-mini-2024-07-18:personal::ANVDva8W\n",
      "Instance: electra_base_gpt_sentiment\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 5389 / 6530  (82.5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [2:14:04<00:00,  1.23s/it]  \n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "classifier_decision    6530 values\n",
      "probabilities          6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "positive               2307\n",
      "negative               2236\n",
      "neutral                1987\n",
      "\n",
      "E6-G4OMFT-EBFT-FT Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8721    0.8291    0.8500      2352\n",
      "     neutral     0.7101    0.7715    0.7395      1829\n",
      "    positive     0.8791    0.8633    0.8711      2349\n",
      "\n",
      "    accuracy                         0.8253      6530\n",
      "   macro avg     0.8204    0.8213    0.8202      6530\n",
      "weighted avg     0.8292    0.8253    0.8267      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1950      322        80\n",
      "neutral         219     1411       199\n",
      "positive         67      254      2028\n",
      "\n",
      "\n",
      "E6-G4OMFT-EBFT-FT-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9287    0.8033    0.8615      1200\n",
      "     neutral     0.7512    0.9258    0.8294      1200\n",
      "    positive     0.9132    0.8242    0.8664      1200\n",
      "\n",
      "    accuracy                         0.8511      3600\n",
      "   macro avg     0.8644    0.8511    0.8524      3600\n",
      "weighted avg     0.8644    0.8511    0.8524      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        964      195        41\n",
      "neutral          36     1111        53\n",
      "positive         38      173       989\n",
      "\n",
      "\n",
      "E6-G4OMFT-EBFT-FT-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7744    0.8583    0.8142       240\n",
      "     neutral     0.7908    0.6458    0.7110       240\n",
      "    positive     0.7907    0.8500    0.8193       240\n",
      "\n",
      "    accuracy                         0.7847       720\n",
      "   macro avg     0.7853    0.7847    0.7815       720\n",
      "weighted avg     0.7853    0.7847    0.7815       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        206       22        12\n",
      "neutral          43      155        42\n",
      "positive         17       19       204\n",
      "\n",
      "\n",
      "E6-G4OMFT-EBFT-FT-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8369    0.8553    0.8460       912\n",
      "     neutral     0.4647    0.3728    0.4137       389\n",
      "    positive     0.8644    0.9186    0.8907       909\n",
      "\n",
      "    accuracy                         0.7964      2210\n",
      "   macro avg     0.7220    0.7155    0.7168      2210\n",
      "weighted avg     0.7827    0.7964    0.7883      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        780      105        27\n",
      "neutral         140      145       104\n",
      "positive         12       62       835\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    82.02\t   82.53\n",
      "DynaSent R1 \t    85.24\t   85.11\n",
      "DynaSent R2 \t    78.15\t   78.47\n",
      "SST-3       \t    71.68\t   79.64\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-10-29 08:39:59\n",
      "Duration: 2:14:05.452618\n"
     ]
    }
   ],
   "source": [
    "e6_g4omft_ebft_ft_results_df, e6_g4omft_ebft_ft_metrics = evaluate_experiment(\n",
    "    name='E6-G4OMFT-EBFT-FT',\n",
    "    notes='Experiment 6: Evaluate deep model collaboration through fine-tuning GPT-4o-mini and Electra Base fine-tuned model',\n",
    "    lm='ft:gpt-4o-mini-2024-07-18:personal::ANVDva8W',\n",
    "    instance='electra_base_gpt_sentiment',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efcad82",
   "metadata": {},
   "source": [
    "#### E11-G4OMFT-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "fc263a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "e11_g4omft_m_result = gpt_4o_mini_min_sentiment(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "13c30d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classification': 'negative'}"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e11_g4omft_m_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "5eb2abca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"You are a model that classifies the sentiment of a review as either 'positive', 'neutral', or 'negative'.\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Those 2 drinks are part of the HK culture and has years of history. It is so bad.'},\n",
       " {'role': 'assistant', 'content': 'negative'}]"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_4o_mini_min_sentiment.get_last_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "8d970a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [3:06:05<00:00,  1.71s/it]  \n"
     ]
    }
   ],
   "source": [
    "e11_g4omft_m_results_df = process_batch(gpt_4o_mini_min_sentiment, test_df, input_format='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "4acc5fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E11-G4OMFT-M\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-10-31 04:36:17\n",
      "Notes: Experiment 11: Measure impact of fine-tuning on GPT-4o-mini using a minimal format\n",
      "Model: ft:gpt-4o-mini-2024-07-18:personal::ALnBCKLv\n",
      "Instance: gpt_min_sentiment\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Results shape: [6530, 4]\n",
      "Save directory: research\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "positive               2279\n",
      "negative               2278\n",
      "neutral                1973\n",
      "\n",
      "E11-G4OMFT-M Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9056    0.8771    0.8911      2352\n",
      "     neutral     0.7663    0.8267    0.7954      1829\n",
      "    positive     0.9241    0.8966    0.9101      2349\n",
      "\n",
      "    accuracy                         0.8700      6530\n",
      "   macro avg     0.8654    0.8668    0.8655      6530\n",
      "weighted avg     0.8733    0.8700    0.8711      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2063      261        28\n",
      "neutral         172     1512       145\n",
      "positive         43      200      2106\n",
      "\n",
      "\n",
      "E11-G4OMFT-M-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9569    0.8700    0.9114      1200\n",
      "     neutral     0.8008    0.9617    0.8739      1200\n",
      "    positive     0.9616    0.8558    0.9056      1200\n",
      "\n",
      "    accuracy                         0.8958      3600\n",
      "   macro avg     0.9065    0.8958    0.8970      3600\n",
      "weighted avg     0.9065    0.8958    0.8970      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1044      142        14\n",
      "neutral          19     1154        27\n",
      "positive         28      145      1027\n",
      "\n",
      "\n",
      "E11-G4OMFT-M-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8862    0.9083    0.8971       240\n",
      "     neutral     0.8664    0.7833    0.8228       240\n",
      "    positive     0.8599    0.9208    0.8893       240\n",
      "\n",
      "    accuracy                         0.8708       720\n",
      "   macro avg     0.8708    0.8708    0.8697       720\n",
      "weighted avg     0.8708    0.8708    0.8697       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        218       15         7\n",
      "neutral          23      188        29\n",
      "positive          5       14       221\n",
      "\n",
      "\n",
      "E11-G4OMFT-M-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8512    0.8783    0.8645       912\n",
      "     neutral     0.5397    0.4370    0.4830       389\n",
      "    positive     0.8994    0.9439    0.9211       909\n",
      "\n",
      "    accuracy                         0.8276      2210\n",
      "   macro avg     0.7634    0.7531    0.7562      2210\n",
      "weighted avg     0.8162    0.8276    0.8206      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        801      104         7\n",
      "neutral         130      170        89\n",
      "positive         10       41       858\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    86.55\t   87.00\n",
      "DynaSent R1 \t    89.70\t   89.58\n",
      "DynaSent R2 \t    86.97\t   87.08\n",
      "SST-3       \t    75.62\t   82.76\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-10-31 04:36:17\n",
      "Duration: 0:00:00.436393\n"
     ]
    }
   ],
   "source": [
    "e11_g4omft_m_metrics = evaluate_experiment(\n",
    "    name='E11-G4OMFT-M',\n",
    "    notes='Experiment 11: Measure impact of fine-tuning on GPT-4o-mini using a minimal format',\n",
    "    lm='ft:gpt-4o-mini-2024-07-18:personal::ALnBCKLv',\n",
    "    instance='gpt_min_sentiment',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex,\n",
    "    results=e11_g4omft_m_results_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a38fe",
   "metadata": {},
   "source": [
    "### B4-G4O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "86b98b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-2024-08-06', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "4d3a7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the ELECTRA Base GPT sentiment module\n",
    "b4_g4o_result = gpt_sentiment(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "c11cca74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    classification='negative'\n",
       ")"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b4_g4o_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "c3dd1ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "30ad4ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: B4-G4O\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-10-30 00:54:21\n",
      "Notes: Baseline 4: Establish GPT-4o baseline with prompt\n",
      "Model: gpt-4o-2024-08-06\n",
      "Instance: gpt_sentiment\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 5271 / 6530  (80.7): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [2:17:38<00:00,  1.26s/it]    \n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2501\n",
      "positive               2028\n",
      "neutral                2000\n",
      "i'm sorry, but i can't access external content such as the link you provided. if you can provide the text of the review, i'd be happy to help classify its sentiment.      1\n",
      "\n",
      "Found 1 invalid predictions at row indices:\n",
      "[4512]\n",
      "\n",
      "Invalid prediction rows:\n",
      "                                                                                                                                                                               sentence  \\\n",
      "4512  https://www.yelp.com/biz/vegas-discount-nutrition-superstore-las-vegas-8?hrid=QQhKNmf3VF_6r_qAfE8jxg&utm_campaign=www_review_share_popup&utm_medium=copy_link&utm_source=(direct)   \n",
      "\n",
      "        label       source split  \n",
      "4512  neutral  dynasent_r1  test  \n",
      "\n",
      "WARNING: Invalid predictions found. Please review the above details and re-run problematic cases.\n"
     ]
    }
   ],
   "source": [
    "b4_g4o_results_df, b4_g4o_metrics = evaluate_experiment(\n",
    "    name='B4-G4O',\n",
    "    notes='Baseline 4: Establish GPT-4o baseline with prompt',\n",
    "    lm='gpt-4o-2024-08-06',\n",
    "    instance='gpt_sentiment',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "4faaa847",
   "metadata": {},
   "outputs": [],
   "source": [
    "b4_g4o_review_4512_retry = gpt_sentiment(review='https://www.yelp.com/biz/vegas-discount-nutrition-superstore-las-vegas-8?hrid=QQhKNmf3VF_6r_qAfE8jxg&utm_campaign=www_review_share_popup&utm_medium=copy_link&utm_source=(direct) (Do not access the URL. Just make the classification decision as if the URL itself was the content of the review)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "ae5ab6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    classification='neutral'\n",
       ")"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b4_g4o_review_4512_retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "b4b8f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update b4_g4o_results_df with the retry result\n",
    "b4_g4o_results_df.at[4512, 'prediction'] = 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "cf6dda21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: B4-G4O\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-10-30 05:06:59\n",
      "Notes: Baseline 4: Establish GPT-4o baseline with prompt\n",
      "Model: gpt-4o-2024-08-06\n",
      "Instance: gpt_sentiment\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Results shape: [6530, 5]\n",
      "Save directory: research\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2501\n",
      "positive               2028\n",
      "neutral                2001\n",
      "\n",
      "B4-G4O Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8285    0.8810    0.8539      2352\n",
      "     neutral     0.6737    0.7370    0.7039      1829\n",
      "    positive     0.9132    0.7884    0.8462      2349\n",
      "\n",
      "    accuracy                         0.8074      6530\n",
      "   macro avg     0.8051    0.8021    0.8014      6530\n",
      "weighted avg     0.8156    0.8074    0.8091      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2072      263        17\n",
      "neutral         322     1348       159\n",
      "positive        107      390      1852\n",
      "\n",
      "\n",
      "B4-G4O-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8628    0.8383    0.8504      1200\n",
      "     neutral     0.6956    0.8683    0.7724      1200\n",
      "    positive     0.9252    0.7217    0.8109      1200\n",
      "\n",
      "    accuracy                         0.8094      3600\n",
      "   macro avg     0.8279    0.8094    0.8112      3600\n",
      "weighted avg     0.8279    0.8094    0.8112      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1006      184        10\n",
      "neutral          98     1042        60\n",
      "positive         62      272       866\n",
      "\n",
      "\n",
      "B4-G4O-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7877    0.9583    0.8647       240\n",
      "     neutral     0.7721    0.6917    0.7297       240\n",
      "    positive     0.8638    0.7667    0.8124       240\n",
      "\n",
      "    accuracy                         0.8056       720\n",
      "   macro avg     0.8079    0.8056    0.8022       720\n",
      "weighted avg     0.8079    0.8056    0.8022       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        230        7         3\n",
      "neutral          48      166        26\n",
      "positive         14       42       184\n",
      "\n",
      "\n",
      "B4-G4O-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8015    0.9167    0.8552       912\n",
      "     neutral     0.4861    0.3599    0.4136       389\n",
      "    positive     0.9124    0.8823    0.8971       909\n",
      "\n",
      "    accuracy                         0.8045      2210\n",
      "   macro avg     0.7333    0.7196    0.7220      2210\n",
      "weighted avg     0.7916    0.8045    0.7947      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        836       72         4\n",
      "neutral         176      140        73\n",
      "positive         31       76       802\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    80.14\t   80.74\n",
      "DynaSent R1 \t    81.12\t   80.94\n",
      "DynaSent R2 \t    80.22\t   80.56\n",
      "SST-3       \t    72.20\t   80.45\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-10-30 05:06:59\n",
      "Duration: 0:00:00.436787\n"
     ]
    }
   ],
   "source": [
    "b4_g4o_metrics = evaluate_experiment(\n",
    "    name='B4-G4O',\n",
    "    notes='Baseline 4: Establish GPT-4o baseline with prompt',\n",
    "    lm='gpt-4o-2024-08-06',\n",
    "    instance='gpt_sentiment',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex,\n",
    "    results=b4_g4o_results_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ab1b5b",
   "metadata": {},
   "source": [
    "### E7-G4O-ELFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "99acd5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-2024-08-06', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "c3f7fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the ELECTRA Base GPT sentiment module\n",
    "e7_g4o_elft_result = electra_base_gpt_sentiment(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "1f591984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    classification='negative',\n",
       "    classifier_decision='negative',\n",
       "    probabilities=[0.9988487958908081, 0.0008097602985799313, 0.0003413360973354429]\n",
       ")"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e7_g4o_elft_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "ed456a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "Classifier Decision: negative\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\nClassifier Decision: negative\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "1fd31e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E7-G4O-ELFT\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-10-30 05:23:40\n",
      "Notes: Experiment 7: Evaluate prompt-based collaboration with larger models\n",
      "Model: gpt-4o-2024-08-06\n",
      "Instance: electra_base_gpt_sentiment\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 5353 / 6530  (82.0): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [2:09:18<00:00,  1.19s/it]  \n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "classifier_decision    6530 values\n",
      "probabilities          6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2343\n",
      "neutral                2096\n",
      "positive               2091\n",
      "\n",
      "E7-G4O-ELFT Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8553    0.8520    0.8537      2352\n",
      "     neutral     0.6861    0.7862    0.7327      1829\n",
      "    positive     0.9139    0.8135    0.8608      2349\n",
      "\n",
      "    accuracy                         0.8198      6530\n",
      "   macro avg     0.8184    0.8173    0.8157      6530\n",
      "weighted avg     0.8290    0.8198    0.8224      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2004      321        27\n",
      "neutral         238     1438       153\n",
      "positive        101      337      1911\n",
      "\n",
      "\n",
      "E7-G4O-ELFT-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9083    0.8258    0.8651      1200\n",
      "     neutral     0.7234    0.9308    0.8141      1200\n",
      "    positive     0.9440    0.7592    0.8416      1200\n",
      "\n",
      "    accuracy                         0.8386      3600\n",
      "   macro avg     0.8586    0.8386    0.8403      3600\n",
      "weighted avg     0.8586    0.8386    0.8403      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        991      196        13\n",
      "neutral          42     1117        41\n",
      "positive         58      231       911\n",
      "\n",
      "\n",
      "E7-G4O-ELFT-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7634    0.8875    0.8208       240\n",
      "     neutral     0.7559    0.6708    0.7108       240\n",
      "    positive     0.8202    0.7792    0.7991       240\n",
      "\n",
      "    accuracy                         0.7792       720\n",
      "   macro avg     0.7798    0.7792    0.7769       720\n",
      "weighted avg     0.7798    0.7792    0.7769       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        213       22         5\n",
      "neutral          43      161        36\n",
      "positive         23       30       187\n",
      "\n",
      "\n",
      "E7-G4O-ELFT-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8222    0.8772    0.8488       912\n",
      "     neutral     0.4720    0.4113    0.4396       389\n",
      "    positive     0.9053    0.8944    0.8998       909\n",
      "\n",
      "    accuracy                         0.8023      2210\n",
      "   macro avg     0.7332    0.7276    0.7294      2210\n",
      "weighted avg     0.7948    0.8023    0.7978      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        800      103         9\n",
      "neutral         153      160        76\n",
      "positive         20       76       813\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    81.57\t   81.98\n",
      "DynaSent R1 \t    84.03\t   83.86\n",
      "DynaSent R2 \t    77.69\t   77.92\n",
      "SST-3       \t    72.94\t   80.23\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-10-30 07:33:00\n",
      "Duration: 2:09:19.530704\n"
     ]
    }
   ],
   "source": [
    "e7_g4o_elft_results_df, e7_g4o_elft_metrics = evaluate_experiment(\n",
    "    name='E7-G4O-ELFT',\n",
    "    notes='Experiment 7: Evaluate prompt-based collaboration with larger models',\n",
    "    lm='gpt-4o-2024-08-06',\n",
    "    instance='electra_base_gpt_sentiment',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9002e396",
   "metadata": {},
   "source": [
    "### E8-G4OFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "87d18416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='ft:gpt-4o-2024-08-06:personal::AN55MS6K', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "281f3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the ELECTRA Base GPT sentiment module\n",
    "e8_g4oft_result = gpt_sentiment(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "ce45759b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    classification='negative'\n",
       ")"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e8_g4oft_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "c6adf3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "af5f1272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E8-G4OFT\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-10-30 08:02:19\n",
      "Notes: Experiment 8: Measure impact of fine-tuning on GPT-4o\n",
      "Model: ft:gpt-4o-2024-08-06:personal::AN55MS6K\n",
      "Instance: gpt_sentiment\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 5709 / 6530  (87.4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [2:13:52<00:00,  1.23s/it]  \n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2360\n",
      "positive               2350\n",
      "neutral                1820\n",
      "\n",
      "E8-G4OFT Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8992    0.9022    0.9007      2352\n",
      "     neutral     0.7967    0.7928    0.7947      1829\n",
      "    positive     0.9094    0.9097    0.9096      2349\n",
      "\n",
      "    accuracy                         0.8743      6530\n",
      "   macro avg     0.8684    0.8682    0.8683      6530\n",
      "weighted avg     0.8741    0.8743    0.8742      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2122      199        31\n",
      "neutral         197     1450       182\n",
      "positive         41      171      2137\n",
      "\n",
      "\n",
      "E8-G4OFT-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9521    0.8942    0.9222      1200\n",
      "     neutral     0.8282    0.9442    0.8824      1200\n",
      "    positive     0.9475    0.8725    0.9085      1200\n",
      "\n",
      "    accuracy                         0.9036      3600\n",
      "   macro avg     0.9093    0.9036    0.9044      3600\n",
      "weighted avg     0.9093    0.9036    0.9044      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1073      111        16\n",
      "neutral          25     1133        42\n",
      "positive         29      124      1047\n",
      "\n",
      "\n",
      "E8-G4OFT-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8920    0.9292    0.9102       240\n",
      "     neutral     0.9126    0.7833    0.8430       240\n",
      "    positive     0.8561    0.9417    0.8968       240\n",
      "\n",
      "    accuracy                         0.8847       720\n",
      "   macro avg     0.8869    0.8847    0.8834       720\n",
      "weighted avg     0.8869    0.8847    0.8834       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        223        8         9\n",
      "neutral          23      188        29\n",
      "positive          4       10       226\n",
      "\n",
      "\n",
      "E8-G4OFT-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8403    0.9057    0.8718       912\n",
      "     neutral     0.5244    0.3316    0.4063       389\n",
      "    positive     0.8807    0.9505    0.9143       909\n",
      "\n",
      "    accuracy                         0.8231      2210\n",
      "   macro avg     0.7485    0.7293    0.7308      2210\n",
      "weighted avg     0.8013    0.8231    0.8073      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        826       80         6\n",
      "neutral         149      129       111\n",
      "positive          8       37       864\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    86.83\t   87.43\n",
      "DynaSent R1 \t    90.44\t   90.36\n",
      "DynaSent R2 \t    88.34\t   88.47\n",
      "SST-3       \t    73.08\t   82.31\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-10-30 10:16:12\n",
      "Duration: 2:13:52.845071\n"
     ]
    }
   ],
   "source": [
    "e8_g4oft_results_df, e8_g4oft_metrics = evaluate_experiment(\n",
    "    name='E8-G4OFT',\n",
    "    notes='Experiment 8: Measure impact of fine-tuning on GPT-4o',\n",
    "    lm='ft:gpt-4o-2024-08-06:personal::AN55MS6K',\n",
    "    instance='gpt_sentiment',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f328a23",
   "metadata": {},
   "source": [
    "### E9-G4OFT-ELFT-P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "e853dbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='ft:gpt-4o-2024-08-06:personal::AN55MS6K', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "7230b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the ELECTRA Base GPT sentiment module\n",
    "e9_g4oft_elft_p_result = electra_large_gpt_sentiment(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "80f1f203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    classification='negative',\n",
       "    classifier_decision='negative',\n",
       "    probabilities=[0.9984808564186096, 0.0003502856707200408, 0.0011688611702993512]\n",
       ")"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e9_g4oft_elft_p_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "fdb85e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "Classifier Decision: negative\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\nClassifier Decision: negative\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "24ce2a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E9-G4OFT-ELFT-P\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-10-30 15:50:19\n",
      "Notes: Experiment 9: Evaluate combined impact of fine-tuning and prompt collaboration with larger models\n",
      "Model: ft:gpt-4o-2024-08-06:personal::AN55MS6K\n",
      "Instance: electra_large_gpt_sentiment\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 5516 / 6530  (84.5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [2:23:30<00:00,  1.32s/it]  \n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "classifier_decision    6530 values\n",
      "probabilities          6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "positive               2378\n",
      "negative               2365\n",
      "neutral                1787\n",
      "\n",
      "E9-G4OFT-ELFT-P Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8689    0.8737    0.8713      2352\n",
      "     neutral     0.7700    0.7523    0.7611      1829\n",
      "    positive     0.8768    0.8876    0.8822      2349\n",
      "\n",
      "    accuracy                         0.8447      6530\n",
      "   macro avg     0.8386    0.8379    0.8382      6530\n",
      "weighted avg     0.8440    0.8447    0.8443      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2055      213        84\n",
      "neutral         244     1376       209\n",
      "positive         66      198      2085\n",
      "\n",
      "\n",
      "E9-G4OFT-ELFT-P-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9274    0.8625    0.8938      1200\n",
      "     neutral     0.8004    0.9192    0.8557      1200\n",
      "    positive     0.9222    0.8500    0.8846      1200\n",
      "\n",
      "    accuracy                         0.8772      3600\n",
      "   macro avg     0.8834    0.8772    0.8780      3600\n",
      "weighted avg     0.8834    0.8772    0.8780      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1035      129        36\n",
      "neutral          47     1103        50\n",
      "positive         34      146      1020\n",
      "\n",
      "\n",
      "E9-G4OFT-ELFT-P-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7948    0.8875    0.8386       240\n",
      "     neutral     0.8652    0.6417    0.7368       240\n",
      "    positive     0.7591    0.8667    0.8093       240\n",
      "\n",
      "    accuracy                         0.7986       720\n",
      "   macro avg     0.8064    0.7986    0.7949       720\n",
      "weighted avg     0.8064    0.7986    0.7949       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        213        8        19\n",
      "neutral          39      154        47\n",
      "positive         16       16       208\n",
      "\n",
      "\n",
      "E9-G4OFT-ELFT-P-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8226    0.8849    0.8526       912\n",
      "     neutral     0.5152    0.3059    0.3839       389\n",
      "    positive     0.8587    0.9428    0.8988       909\n",
      "\n",
      "    accuracy                         0.8068      2210\n",
      "   macro avg     0.7322    0.7112    0.7118      2210\n",
      "weighted avg     0.7834    0.8068    0.7891      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        807       76        29\n",
      "neutral         158      119       112\n",
      "positive         16       36       857\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    83.82\t   84.47\n",
      "DynaSent R1 \t    87.80\t   87.72\n",
      "DynaSent R2 \t    79.49\t   79.86\n",
      "SST-3       \t    71.18\t   80.68\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-10-30 18:13:50\n",
      "Duration: 2:23:30.778599\n"
     ]
    }
   ],
   "source": [
    "e9_g4oft_elft_p_results_df, e9_g4oft_elft_p_metrics = evaluate_experiment(\n",
    "    name='E9-G4OFT-ELFT-P',\n",
    "    notes='Experiment 9: Evaluate combined impact of fine-tuning and prompt collaboration with larger models',\n",
    "    lm='ft:gpt-4o-2024-08-06:personal::AN55MS6K',\n",
    "    instance='electra_large_gpt_sentiment',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c143fb5b",
   "metadata": {},
   "source": [
    "### E10-G4OFT-ELFT-FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "8a499709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='ft:gpt-4o-2024-08-06:personal::ANcREuvn', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "969c8c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the ELECTRA Base GPT sentiment module\n",
    "e10_g4oft_elft_ft_result = electra_large_gpt_sentiment(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "31b835ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    classification='negative',\n",
       "    classifier_decision='negative',\n",
       "    probabilities=[0.9984808564186096, 0.0003502856707200408, 0.0011688611702993512]\n",
       ")"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e10_g4oft_elft_ft_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "eba5d279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "Classifier Decision: negative\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\nClassifier Decision: negative\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "dff9db4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E10-G4OFT-ELFT-FT\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-10-30 18:44:43\n",
      "Notes: Experiment 10: Measure impact of fine-tuning on GPT-4o using a minimal format\n",
      "Model: ft:gpt-4o-2024-08-06:personal::ANcREuvn\n",
      "Instance: electra_large_gpt_sentiment\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 7 / 7  (100.0):   7%|â–‹         | 7/100 [46:57:51<623:57:19, 24153.11s/it]]   \n",
      "Average Metric: 5539 / 6530  (84.8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [2:24:42<00:00,  1.33s/it]\n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "classifier_decision    6530 values\n",
      "probabilities          6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2343\n",
      "positive               2325\n",
      "neutral                1862\n",
      "\n",
      "E10-G4OFT-ELFT-FT Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8771    0.8737    0.8754      2352\n",
      "     neutral     0.7573    0.7709    0.7640      1829\n",
      "    positive     0.8920    0.8829    0.8875      2349\n",
      "\n",
      "    accuracy                         0.8482      6530\n",
      "   macro avg     0.8421    0.8425    0.8423      6530\n",
      "weighted avg     0.8489    0.8482    0.8485      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2055      239        58\n",
      "neutral         226     1410       193\n",
      "positive         62      213      2074\n",
      "\n",
      "\n",
      "E10-G4OFT-ELFT-FT-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9313    0.8583    0.8933      1200\n",
      "     neutral     0.7883    0.9308    0.8536      1200\n",
      "    positive     0.9359    0.8400    0.8854      1200\n",
      "\n",
      "    accuracy                         0.8764      3600\n",
      "   macro avg     0.8852    0.8764    0.8774      3600\n",
      "weighted avg     0.8852    0.8764    0.8774      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1030      145        25\n",
      "neutral          39     1117        44\n",
      "positive         37      155      1008\n",
      "\n",
      "\n",
      "E10-G4OFT-ELFT-FT-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8099    0.8875    0.8469       240\n",
      "     neutral     0.8519    0.6708    0.7506       240\n",
      "    positive     0.7761    0.8667    0.8189       240\n",
      "\n",
      "    accuracy                         0.8083       720\n",
      "   macro avg     0.8126    0.8083    0.8055       720\n",
      "weighted avg     0.8126    0.8083    0.8055       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        213       10        17\n",
      "neutral          36      161        43\n",
      "positive         14       18       208\n",
      "\n",
      "\n",
      "E10-G4OFT-ELFT-FT-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8337    0.8904    0.8611       912\n",
      "     neutral     0.5156    0.3393    0.4093       389\n",
      "    positive     0.8755    0.9439    0.9084       909\n",
      "\n",
      "    accuracy                         0.8154      2210\n",
      "   macro avg     0.7416    0.7245    0.7263      2210\n",
      "weighted avg     0.7949    0.8154    0.8010      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        812       84        16\n",
      "neutral         151      132       106\n",
      "positive         11       40       858\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    84.23\t   84.82\n",
      "DynaSent R1 \t    87.74\t   87.64\n",
      "DynaSent R2 \t    80.55\t   80.83\n",
      "SST-3       \t    72.63\t   81.54\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-10-30 21:09:26\n",
      "Duration: 2:24:42.929218\n"
     ]
    }
   ],
   "source": [
    "e10_g4oft_elft_ft_results_df, e10_g4oft_elft_ft_metrics = evaluate_experiment(\n",
    "    name='E10-G4OFT-ELFT-FT',\n",
    "    notes='Experiment 10: Measure impact of fine-tuning on GPT-4o using a minimal format',\n",
    "    lm='ft:gpt-4o-2024-08-06:personal::ANcREuvn',\n",
    "    instance='electra_large_gpt_sentiment',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc46e2",
   "metadata": {},
   "source": [
    "#### E12-G4OFT-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "7433edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "e12_g4oft_m_result = gpt_4o_min_sentiment(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "9f4a0f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classification': 'negative'}"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e12_g4oft_m_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "9cd1ec2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"You are a model that classifies the sentiment of a review as either 'positive', 'neutral', or 'negative'.\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Those 2 drinks are part of the HK culture and has years of history. It is so bad.'},\n",
       " {'role': 'assistant', 'content': 'negative'}]"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_4o_min_sentiment.get_last_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "9d6d373a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [3:12:20<00:00,  1.77s/it]  \n"
     ]
    }
   ],
   "source": [
    "e12_g4oft_m_results_df = process_batch(gpt_4o_min_sentiment, test_df, input_format='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "523b9510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E12-G4OFT-M\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-10-31 00:49:33\n",
      "Notes: Experiment 12: Measure impact of fine-tuning on GPT-4o using a minimal format\n",
      "Model: ft:gpt-4o-2024-08-06:personal::AM5cg622\n",
      "Instance: gpt_4o_min_sentiment\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Results shape: [6530, 4]\n",
      "Save directory: research\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2379\n",
      "positive               2308\n",
      "neutral                1843\n",
      "\n",
      "E12-G4OFT-M Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8983    0.9086    0.9034      2352\n",
      "     neutral     0.7933    0.7993    0.7963      1829\n",
      "    positive     0.9181    0.9021    0.9100      2349\n",
      "\n",
      "    accuracy                         0.8757      6530\n",
      "   macro avg     0.8699    0.8700    0.8699      6530\n",
      "weighted avg     0.8760    0.8757    0.8758      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2137      195        20\n",
      "neutral         198     1462       169\n",
      "positive         44      186      2119\n",
      "\n",
      "\n",
      "E12-G4OFT-M-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9468    0.9050    0.9254      1200\n",
      "     neutral     0.8306    0.9400    0.8819      1200\n",
      "    positive     0.9534    0.8700    0.9098      1200\n",
      "\n",
      "    accuracy                         0.9050      3600\n",
      "   macro avg     0.9103    0.9050    0.9057      3600\n",
      "weighted avg     0.9103    0.9050    0.9057      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1086      104        10\n",
      "neutral          31     1128        41\n",
      "positive         30      126      1044\n",
      "\n",
      "\n",
      "E12-G4OFT-M-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8851    0.9625    0.9222       240\n",
      "     neutral     0.9126    0.7833    0.8430       240\n",
      "    positive     0.8814    0.9292    0.9047       240\n",
      "\n",
      "    accuracy                         0.8917       720\n",
      "   macro avg     0.8930    0.8917    0.8900       720\n",
      "weighted avg     0.8930    0.8917    0.8900       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        231        6         3\n",
      "neutral          25      188        27\n",
      "positive          5       12       223\n",
      "\n",
      "\n",
      "E12-G4OFT-M-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8445    0.8991    0.8710       912\n",
      "     neutral     0.5233    0.3753    0.4371       389\n",
      "    positive     0.8875    0.9373    0.9117       909\n",
      "\n",
      "    accuracy                         0.8226      2210\n",
      "   macro avg     0.7518    0.7372    0.7399      2210\n",
      "weighted avg     0.8056    0.8226    0.8114      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        820       85         7\n",
      "neutral         142      146       101\n",
      "positive          9       48       852\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    86.99\t   87.57\n",
      "DynaSent R1 \t    90.57\t   90.50\n",
      "DynaSent R2 \t    89.00\t   89.17\n",
      "SST-3       \t    73.99\t   82.26\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-10-31 00:49:34\n",
      "Duration: 0:00:00.444904\n"
     ]
    }
   ],
   "source": [
    "e12_g4oft_m_metrics = evaluate_experiment(\n",
    "    name='E12-G4OFT-M',\n",
    "    notes='Experiment 12: Measure impact of fine-tuning on GPT-4o using a minimal format',\n",
    "    lm='ft:gpt-4o-2024-08-06:personal::AM5cg622',\n",
    "    instance='gpt_4o_min_sentiment',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex,\n",
    "    results=e12_g4oft_m_results_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cac6611",
   "metadata": {},
   "source": [
    "### E13-G4OM-ELFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "c24eda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-mini-2024-07-18', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "3aa1e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "e13_g4om_elft_result = electra_large_gpt_sentiment(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "e54f8147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    classification='negative',\n",
       "    classifier_decision='negative',\n",
       "    probabilities=[0.9984808564186096, 0.0003502856707200408, 0.0011688611702993512]\n",
       ")"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e13_g4om_elft_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "8867ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "Classifier Decision: negative\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\nClassifier Decision: negative\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "b13fd9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E13-G4OM-ELFT\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-10-31 05:51:08\n",
      "Notes: Experiment 13: Evaluate prompt-based model collaboration between GPT-4o-mini and Electra Large fine-tuned model\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Instance: electra_large_gpt_sentiment\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 5499 / 6530  (84.2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [1:30:00<00:00,  1.21it/s]\n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "classifier_decision    6530 values\n",
      "probabilities          6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2542\n",
      "positive               2264\n",
      "neutral                1724\n",
      "\n",
      "E13-G4OM-ELFT Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8360    0.9035    0.8684      2352\n",
      "     neutral     0.7773    0.7326    0.7543      1829\n",
      "    positive     0.8984    0.8659    0.8819      2349\n",
      "\n",
      "    accuracy                         0.8421      6530\n",
      "   macro avg     0.8372    0.8340    0.8349      6530\n",
      "weighted avg     0.8420    0.8421    0.8413      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2125      185        42\n",
      "neutral         301     1340       188\n",
      "positive        116      199      2034\n",
      "\n",
      "\n",
      "E13-G4OM-ELFT-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8948    0.8933    0.8941      1200\n",
      "     neutral     0.8122    0.8975    0.8527      1200\n",
      "    positive     0.9294    0.8333    0.8787      1200\n",
      "\n",
      "    accuracy                         0.8747      3600\n",
      "   macro avg     0.8788    0.8747    0.8752      3600\n",
      "weighted avg     0.8788    0.8747    0.8752      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1072      106        22\n",
      "neutral          69     1077        54\n",
      "positive         57      143      1000\n",
      "\n",
      "\n",
      "E13-G4OM-ELFT-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7467    0.9458    0.8346       240\n",
      "     neutral     0.8371    0.6208    0.7129       240\n",
      "    positive     0.7941    0.7875    0.7908       240\n",
      "\n",
      "    accuracy                         0.7847       720\n",
      "   macro avg     0.7926    0.7847    0.7794       720\n",
      "weighted avg     0.7926    0.7847    0.7794       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        227        7         6\n",
      "neutral          48      149        43\n",
      "positive         29       22       189\n",
      "\n",
      "\n",
      "E13-G4OM-ELFT-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7942    0.9057    0.8463       912\n",
      "     neutral     0.5182    0.2931    0.3744       389\n",
      "    positive     0.8895    0.9296    0.9091       909\n",
      "\n",
      "    accuracy                         0.8077      2210\n",
      "   macro avg     0.7340    0.7095    0.7099      2210\n",
      "weighted avg     0.7848    0.8077    0.7891      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        826       72        14\n",
      "neutral         184      114        91\n",
      "positive         30       34       845\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    83.49\t   84.21\n",
      "DynaSent R1 \t    87.52\t   87.47\n",
      "DynaSent R2 \t    77.94\t   78.47\n",
      "SST-3       \t    70.99\t   80.77\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-10-31 07:21:10\n",
      "Duration: 1:30:01.535028\n"
     ]
    }
   ],
   "source": [
    "e13_g4om_elft_metrics = evaluate_experiment(\n",
    "    name='E13-G4OM-ELFT',\n",
    "    notes='Experiment 13: Evaluate prompt-based model collaboration between GPT-4o-mini and Electra Large fine-tuned model',\n",
    "    lm='gpt-4o-mini-2024-07-18',\n",
    "    instance='electra_large_gpt_sentiment',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a2a10",
   "metadata": {},
   "source": [
    "#### E14-G4OMFT-ELFT-P-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "cc5cb091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "e14_g4omft_elft_p_m_result = gpt_4o_mini_min_sentiment_with_pred(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "bfade659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classification': 'negative'}"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e14_g4omft_elft_p_m_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "f987f39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"You are a model that classifies the sentiment of a review as either 'positive', 'neutral', or 'negative'. 'Classifier Decision' is the sentiment classification proposed by a model fine-tuned on sentiment.\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Classifier Decision: negative.\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.'},\n",
       " {'role': 'assistant', 'content': 'negative'}]"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_4o_mini_min_sentiment_with_pred.get_last_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "decf680b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [3:03:33<00:00,  1.69s/it]  \n"
     ]
    }
   ],
   "source": [
    "e14_g4omft_elft_p_m_results_df = process_batch(gpt_4o_mini_min_sentiment_with_pred, test_df, input_format='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "411869d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E14-G4OMFT-ELFT-P-M\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-11-01 06:48:20\n",
      "Notes: Experiment 14: Evaluate prompt-based model collaboration using a minimal format\n",
      "Model: ft:gpt-4o-mini-2024-07-18:personal::ALnBCKLv\n",
      "Instance: gpt_4o_mini_min_sentiment_with_pred\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Results shape: [6530, 4]\n",
      "Save directory: research\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "neutral                2524\n",
      "positive               2273\n",
      "negative               1733\n",
      "\n",
      "E14-G4OMFT-ELFT-P-M Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9065    0.6679    0.7692      2352\n",
      "     neutral     0.6113    0.8436    0.7089      1829\n",
      "    positive     0.9111    0.8817    0.8961      2349\n",
      "\n",
      "    accuracy                         0.7940      6530\n",
      "   macro avg     0.8097    0.7977    0.7914      6530\n",
      "weighted avg     0.8255    0.7940    0.7980      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1571      742        39\n",
      "neutral         123     1543       163\n",
      "positive         39      239      2071\n",
      "\n",
      "\n",
      "E14-G4OMFT-ELFT-P-M-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9597    0.6150    0.7496      1200\n",
      "     neutral     0.6520    0.9650    0.7782      1200\n",
      "    positive     0.9526    0.8375    0.8914      1200\n",
      "\n",
      "    accuracy                         0.8058      3600\n",
      "   macro avg     0.8548    0.8058    0.8064      3600\n",
      "weighted avg     0.8548    0.8058    0.8064      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        738      446        16\n",
      "neutral           8     1158        34\n",
      "positive         23      172      1005\n",
      "\n",
      "\n",
      "E14-G4OMFT-ELFT-P-M-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9424    0.7500    0.8353       240\n",
      "     neutral     0.7382    0.8458    0.7883       240\n",
      "    positive     0.8346    0.8833    0.8583       240\n",
      "\n",
      "    accuracy                         0.8264       720\n",
      "   macro avg     0.8384    0.8264    0.8273       720\n",
      "weighted avg     0.8384    0.8264    0.8273       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        180       50        10\n",
      "neutral           5      203        32\n",
      "positive          6       22       212\n",
      "\n",
      "\n",
      "E14-G4OMFT-ELFT-P-M-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8448    0.7160    0.7751       912\n",
      "     neutral     0.3848    0.4679    0.4223       389\n",
      "    positive     0.8859    0.9395    0.9119       909\n",
      "\n",
      "    accuracy                         0.7643      2210\n",
      "   macro avg     0.7051    0.7078    0.7031      2210\n",
      "weighted avg     0.7807    0.7643    0.7693      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        653      246        13\n",
      "neutral         110      182        97\n",
      "positive         10       45       854\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    79.14\t   79.40\n",
      "DynaSent R1 \t    80.64\t   80.58\n",
      "DynaSent R2 \t    82.73\t   82.64\n",
      "SST-3       \t    70.31\t   76.43\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-11-01 06:48:20\n",
      "Duration: 0:00:00.448224\n"
     ]
    }
   ],
   "source": [
    "e14_g4omft_elft_p_m_metrics = evaluate_experiment(\n",
    "    name='E14-G4OMFT-ELFT-P-M',\n",
    "    notes='Experiment 14: Evaluate prompt-based model collaboration using a minimal format',\n",
    "    lm='ft:gpt-4o-mini-2024-07-18:personal::ALnBCKLv',\n",
    "    instance='gpt_4o_mini_min_sentiment_with_pred',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex,\n",
    "    results=e14_g4omft_elft_p_m_results_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803bcb9e",
   "metadata": {},
   "source": [
    "### E15-G4OM-ELFT-EX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "f6a3bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-mini-2024-07-18', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "ec54b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "e15_g4om_elft_ex_result = electra_large_gpt_sentiment_examples(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "id": "0c4d600a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    examples=[{'index': 57, 'similarity_score': 0.9977191090583801, 'review': 'This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.', 'classification': 'negative'}, {'index': 3410, 'similarity_score': 0.9972444176673889, 'review': 'This was my second time here and there will not be a third time.', 'classification': 'negative'}, {'index': 3476, 'similarity_score': 0.9971747398376465, 'review': 'Portioning of the Octopus could have been better.', 'classification': 'negative'}, {'index': 3679, 'similarity_score': 0.9971185922622681, 'review': 'My only complaint was the bill for a long distance phone call - $4 for a 3 minutes to Seattle.', 'classification': 'negative'}, {'index': 3160, 'similarity_score': 0.9971152544021606, 'review': 'I received big-time attitude from multiple \"fabulous\" employees when I pulled up at 7:29 a.m. to get my car washed.', 'classification': 'negative'}],\n",
       "    classification='negative',\n",
       "    classifier_decision='negative',\n",
       "    probabilities=[0.9984808564186096, 0.0003502856707200408, 0.0011688611702993512]\n",
       ")"
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e15_g4om_elft_ex_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "id": "e7fc3e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Examples: A list of examples that demonstrate different sentiment classes.\n",
      "\n",
      "Review: The review text to classify.\n",
      "\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Examples:\n",
      "- negative: This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.\n",
      "- negative: This was my second time here and there will not be a third time.\n",
      "- negative: Portioning of the Octopus could have been better.\n",
      "- negative: My only complaint was the bill for a long distance phone call - $4 for a 3 minutes to Seattle.\n",
      "- negative: I received big-time attitude from multiple \"fabulous\" employees when I pulled up at 7:29 a.m. to get my car washed.\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "\n",
      "Classifier Decision: negative\n",
      "\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nClassify the sentiment of a review as either \\'negative\\', \\'neutral\\', or \\'positive\\'.\\n\\n---\\n\\nFollow the following format.\\n\\nExamples: A list of examples that demonstrate different sentiment classes.\\n\\nReview: The review text to classify.\\n\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\n\\nClassification: One word representing the sentiment classification: \\'negative\\', \\'neutral\\', or \\'positive\\' (do not repeat the field name, do not use \\'mixed\\')\\n\\n---\\n\\nExamples:\\n- negative: This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.\\n- negative: This was my second time here and there will not be a third time.\\n- negative: Portioning of the Octopus could have been better.\\n- negative: My only complaint was the bill for a long distance phone call - $4 for a 3 minutes to Seattle.\\n- negative: I received big-time attitude from multiple \"fabulous\" employees when I pulled up at 7:29 a.m. to get my car washed.\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\n\\nClassifier Decision: negative\\n\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 754,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "cb683c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E15-G4OM-ELFT-EX\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-11-02 08:57:53\n",
      "Notes: Experiment 15: Evaluate prompt-based model collaboration that includes similar examples\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Instance: electra_large_gpt_sentiment_examples\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 5472 / 6530  (83.8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [1:19:14<00:00,  1.37it/s]\n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "classifier_decision    6530 values\n",
      "probabilities          6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2489\n",
      "positive               2196\n",
      "neutral                1845\n",
      "\n",
      "E15-G4OM-ELFT-EX Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8381    0.8869    0.8618      2352\n",
      "     neutral     0.7491    0.7556    0.7523      1829\n",
      "    positive     0.9126    0.8531    0.8818      2349\n",
      "\n",
      "    accuracy                         0.8380      6530\n",
      "   macro avg     0.8332    0.8319    0.8320      6530\n",
      "weighted avg     0.8399    0.8380    0.8383      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2086      234        32\n",
      "neutral         287     1382       160\n",
      "positive        116      229      2004\n",
      "\n",
      "\n",
      "E15-G4OM-ELFT-EX-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8967    0.8683    0.8823      1200\n",
      "     neutral     0.7832    0.9092    0.8415      1200\n",
      "    positive     0.9435    0.8217    0.8784      1200\n",
      "\n",
      "    accuracy                         0.8664      3600\n",
      "   macro avg     0.8745    0.8664    0.8674      3600\n",
      "weighted avg     0.8745    0.8664    0.8674      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1042      143        15\n",
      "neutral          65     1091        44\n",
      "positive         55      159       986\n",
      "\n",
      "\n",
      "E15-G4OM-ELFT-EX-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7500    0.9250    0.8284       240\n",
      "     neutral     0.8247    0.6667    0.7373       240\n",
      "    positive     0.8130    0.7792    0.7957       240\n",
      "\n",
      "    accuracy                         0.7903       720\n",
      "   macro avg     0.7959    0.7903    0.7871       720\n",
      "weighted avg     0.7959    0.7903    0.7871       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        222       12         6\n",
      "neutral          43      160        37\n",
      "positive         31       22       187\n",
      "\n",
      "\n",
      "E15-G4OM-ELFT-EX-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7973    0.9013    0.8461       912\n",
      "     neutral     0.5078    0.3368    0.4049       389\n",
      "    positive     0.9023    0.9142    0.9082       909\n",
      "\n",
      "    accuracy                         0.8072      2210\n",
      "   macro avg     0.7358    0.7174    0.7198      2210\n",
      "weighted avg     0.7895    0.8072    0.7940      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        822       79        11\n",
      "neutral         179      131        79\n",
      "positive         30       48       831\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    83.20\t   83.80\n",
      "DynaSent R1 \t    86.74\t   86.64\n",
      "DynaSent R2 \t    78.71\t   79.03\n",
      "SST-3       \t    71.98\t   80.72\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-11-02 10:17:09\n",
      "Duration: 1:19:15.392643\n"
     ]
    }
   ],
   "source": [
    "e15_g4om_elft_ex_metrics = evaluate_experiment(\n",
    "    name='E15-G4OM-ELFT-EX',\n",
    "    notes='Experiment 15: Evaluate prompt-based model collaboration that includes similar examples',\n",
    "    lm='gpt-4o-mini-2024-07-18',\n",
    "    instance='electra_large_gpt_sentiment_examples',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0cb3fe",
   "metadata": {},
   "source": [
    "### E16-G4OM-ELFT-PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "d7b32d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-mini-2024-07-18', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "53b8d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "e16_g4om_elft_pr_result = electra_large_gpt_sentiment_probs(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "d3b761d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    classification='negative',\n",
       "    negative_probability='99.85%',\n",
       "    neutral_probability='0.04%',\n",
       "    positive_probability='0.12%'\n",
       ")"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e16_g4om_elft_pr_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "b5d896a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "\n",
      "Negative Probability: Probability the review is negative from a model fine-tuned on sentiment\n",
      "\n",
      "Neutral Probability: Probability the review is neutral from a model fine-tuned on sentiment\n",
      "\n",
      "Positive Probability: Probability the review is positive from a model fine-tuned on sentiment\n",
      "\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "\n",
      "Negative Probability: 99.85%\n",
      "\n",
      "Neutral Probability: 0.04%\n",
      "\n",
      "Positive Probability: 0.12%\n",
      "\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\n\\nNegative Probability: Probability the review is negative from a model fine-tuned on sentiment\\n\\nNeutral Probability: Probability the review is neutral from a model fine-tuned on sentiment\\n\\nPositive Probability: Probability the review is positive from a model fine-tuned on sentiment\\n\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\n\\nNegative Probability: 99.85%\\n\\nNeutral Probability: 0.04%\\n\\nPositive Probability: 0.12%\\n\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "960405f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E16-G4OM-ELFT-PR\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-11-02 20:14:19\n",
      "Notes: Experiment 16: Evaluate prompt-based model collaboration with probabilities\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Instance: electra_large_gpt_sentiment_probs\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 5459 / 6530  (83.6): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [2:07:06<00:00,  1.17s/it]   \n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "positive               2393\n",
      "negative               2285\n",
      "neutral                1845\n",
      "review: they gratefully charge 5% more and do an amazing nothing extra.\n",
      "\n",
      "negative probability: 92.50%\n",
      "\n",
      "neutral probability: 0.33%\n",
      "\n",
      "positive probability: 7.17%\n",
      "\n",
      "classification: negative      1\n",
      "review: if you don't have a groupon and are planning to eat at this place, you are out of your mind.\n",
      "\n",
      "negative probability: 88.00%\n",
      "\n",
      "neutral probability: 10.00%\n",
      "\n",
      "positive probability: 2.00%\n",
      "\n",
      "classification: negative      1\n",
      "review: illuminating if overly talky documentary.\n",
      "\n",
      "negative probability: 0.70%\n",
      "\n",
      "neutral probability: 20.70%\n",
      "\n",
      "positive probability: 78.60%\n",
      "\n",
      "classification: positive      1\n",
      "review: this place is aweful.\n",
      "\n",
      "negative probability: 90.66%\n",
      "\n",
      "neutral probability: 6.99%\n",
      "\n",
      "positive probability: 2.35%\n",
      "\n",
      "classification: negative      1\n",
      "review: loyal customers, good customers, are treated with abandon here!\n",
      "\n",
      "negative probability: 95.67%\n",
      "\n",
      "neutral probability: 2.15%\n",
      "\n",
      "positive probability: 2.18%\n",
      "\n",
      "classification: negative      1\n",
      "review: oh did i mention that it's buttered greasily too?\n",
      "\n",
      "negative probability: 74.00%\n",
      "\n",
      "neutral probability: 20.00%\n",
      "\n",
      "positive probability: 6.00%\n",
      "\n",
      "classification: negative      1\n",
      "review: we decided to come to whiskey river and it looked like a great place to hang out. i will definitely be back when pigs fly.\n",
      "\n",
      "negative probability: 95.00%\n",
      "\n",
      "neutral probability: 2.00%\n",
      "\n",
      "positive probability: 3.00%\n",
      "\n",
      "classification: negative      1\n",
      "\n",
      "Found 7 invalid predictions at row indices:\n",
      "[1076, 1201, 3681, 3895, 4436, 5177, 5807]\n",
      "\n",
      "Invalid prediction rows:\n",
      "                                                                                                                        sentence  \\\n",
      "1076                                                             They gratefully charge 5% more and do an amazing NOTHING extra.   \n",
      "1201                                If you don't have a groupon and are planning to eat at this place, you are out of your mind.   \n",
      "3681                                                                                  Illuminating if overly talky documentary .   \n",
      "3895                                                                                                       This place is aweful.   \n",
      "4436                                                             Loyal customers, good customers, are treated with abandon here!   \n",
      "5177                                                                           Oh did I mention that it's buttered greasily too?   \n",
      "5807  We decided to come to Whiskey River and it looked like a great place to hang out. I will definitely be back when pigs fly.   \n",
      "\n",
      "         label       source split  \n",
      "1076  negative  dynasent_r2  test  \n",
      "1201  negative  dynasent_r1  test  \n",
      "3681   neutral    sst_local  test  \n",
      "3895  negative  dynasent_r1  test  \n",
      "4436  negative  dynasent_r2  test  \n",
      "5177  negative  dynasent_r1  test  \n",
      "5807  negative  dynasent_r2  test  \n",
      "\n",
      "WARNING: Invalid predictions found. Please review the above details and re-run problematic cases.\n"
     ]
    }
   ],
   "source": [
    "e16_g4om_elft_pr_metrics = evaluate_experiment(\n",
    "    name='E16-G4OM-ELFT-PR',\n",
    "    notes='Experiment 16: Evaluate prompt-based model collaboration with probabilities',\n",
    "    lm='gpt-4o-mini-2024-07-18',\n",
    "    instance='electra_large_gpt_sentiment_probs',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "id": "c14b6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "e16_g4om_elft_pr_results_df = e16_g4om_elft_pr_metrics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "id": "29fd6ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_indices = [1076, 1201, 3681, 3895, 4436, 5177, 5807]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4805a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "e16_g4om_elft_pr_results_df.loc[invalid_indices, 'prediction'] = e16_g4om_elft_pr_results_df.loc[invalid_indices, 'classification']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "id": "d2d6d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "e16_g4om_elft_pr_results_df_orig = pd.DataFrame.copy(e16_g4om_elft_pr_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "34b6bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the regular expression pattern to extract the classification label\n",
    "pattern = r\"classification:\\s*(\\w+)\"\n",
    "\n",
    "# Function to extract classification from the prediction text and update the prediction column\n",
    "def update_prediction(row):\n",
    "    match = re.search(pattern, row['prediction'])\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return row['prediction']  # Keep the original if no match found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "id": "18549e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the invalid rows\n",
    "e16_g4om_elft_pr_results_df.loc[invalid_indices, 'prediction'] = e16_g4om_elft_pr_results_df.loc[invalid_indices].apply(update_prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "id": "57307ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated rows:\n",
      "                                                                                                                          review  \\\n",
      "1076                                                             They gratefully charge 5% more and do an amazing NOTHING extra.   \n",
      "1201                                If you don't have a groupon and are planning to eat at this place, you are out of your mind.   \n",
      "3681                                                                                  Illuminating if overly talky documentary .   \n",
      "3895                                                                                                       This place is aweful.   \n",
      "4436                                                             Loyal customers, good customers, are treated with abandon here!   \n",
      "5177                                                                           Oh did I mention that it's buttered greasily too?   \n",
      "5807  We decided to come to Whiskey River and it looked like a great place to hang out. I will definitely be back when pigs fly.   \n",
      "\n",
      "     classification prediction  \n",
      "1076       negative   negative  \n",
      "1201       negative   negative  \n",
      "3681        neutral   positive  \n",
      "3895       negative   negative  \n",
      "4436       negative   negative  \n",
      "5177       negative   negative  \n",
      "5807       negative   negative  \n"
     ]
    }
   ],
   "source": [
    "# Verify the changes\n",
    "print(\"Updated rows:\")\n",
    "print(e16_g4om_elft_pr_results_df.loc[invalid_indices, ['review', 'classification', 'prediction']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "id": "b6bc2abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E16-G4OM-ELFT-PR\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-11-02 23:59:48\n",
      "Notes: Experiment 16: Evaluate prompt-based model collaboration with probabilities\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Instance: electra_large_gpt_sentiment_probs\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Results shape: [6530, 5]\n",
      "Save directory: research\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "positive               2394\n",
      "negative               2291\n",
      "neutral                1845\n",
      "\n",
      "E16-G4OM-ELFT-PR Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8712    0.8486    0.8598      2352\n",
      "     neutral     0.7523    0.7589    0.7556      1829\n",
      "    positive     0.8668    0.8834    0.8750      2349\n",
      "\n",
      "    accuracy                         0.8360      6530\n",
      "   macro avg     0.8301    0.8303    0.8301      6530\n",
      "weighted avg     0.8363    0.8360    0.8361      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1996      251       105\n",
      "neutral         227     1388       214\n",
      "positive         68      206      2075\n",
      "\n",
      "\n",
      "E16-G4OM-ELFT-PR-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9240    0.8308    0.8749      1200\n",
      "     neutral     0.7838    0.9183    0.8457      1200\n",
      "    positive     0.9058    0.8417    0.8726      1200\n",
      "\n",
      "    accuracy                         0.8636      3600\n",
      "   macro avg     0.8712    0.8636    0.8644      3600\n",
      "weighted avg     0.8712    0.8636    0.8644      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        997      151        52\n",
      "neutral          45     1102        53\n",
      "positive         37      153      1010\n",
      "\n",
      "\n",
      "E16-G4OM-ELFT-PR-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8024    0.8458    0.8235       240\n",
      "     neutral     0.8368    0.6625    0.7395       240\n",
      "    positive     0.7509    0.8667    0.8046       240\n",
      "\n",
      "    accuracy                         0.7917       720\n",
      "   macro avg     0.7967    0.7917    0.7892       720\n",
      "weighted avg     0.7967    0.7917    0.7892       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        203       15        22\n",
      "neutral          34      159        47\n",
      "positive         16       16       208\n",
      "\n",
      "\n",
      "E16-G4OM-ELFT-PR-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8300    0.8728    0.8509       912\n",
      "     neutral     0.5100    0.3265    0.3981       389\n",
      "    positive     0.8553    0.9428    0.8969       909\n",
      "\n",
      "    accuracy                         0.8054      2210\n",
      "   macro avg     0.7318    0.7140    0.7153      2210\n",
      "weighted avg     0.7841    0.8054    0.7901      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        796       85        31\n",
      "neutral         148      127       114\n",
      "positive         15       37       857\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    83.01\t   83.60\n",
      "DynaSent R1 \t    86.44\t   86.36\n",
      "DynaSent R2 \t    78.92\t   79.17\n",
      "SST-3       \t    71.53\t   80.54\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-11-02 23:59:49\n",
      "Duration: 0:00:00.454047\n"
     ]
    }
   ],
   "source": [
    "e16_g4om_elft_pr_metrics = evaluate_experiment(\n",
    "    name='E16-G4OM-ELFT-PR',\n",
    "    notes='Experiment 16: Evaluate prompt-based model collaboration with probabilities',\n",
    "    lm='gpt-4o-mini-2024-07-18',\n",
    "    instance='electra_large_gpt_sentiment_probs',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex,\n",
    "    results=e16_g4om_elft_pr_results_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec8cf7",
   "metadata": {},
   "source": [
    "### E17-G4OM-ELFT-PR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "id": "48c74e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-mini-2024-07-18', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "ac9fae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "e17_g4om_elft_pr2_result = electra_large_gpt_sentiment_pred_probs(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "992cf1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    classification='negative',\n",
       "    classifier_decision='negative',\n",
       "    negative_probability='99.85%',\n",
       "    neutral_probability='0.04%',\n",
       "    positive_probability='0.12%'\n",
       ")"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e17_g4om_elft_pr2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "id": "04f91186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "\n",
      "Negative Probability: Probability the review is negative\n",
      "\n",
      "Neutral Probability: Probability the review is neutral\n",
      "\n",
      "Positive Probability: Probability the review is positive\n",
      "\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "\n",
      "Classifier Decision: negative\n",
      "\n",
      "Negative Probability: 99.85%\n",
      "\n",
      "Neutral Probability: 0.04%\n",
      "\n",
      "Positive Probability: 0.12%\n",
      "\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\n\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\n\\nNegative Probability: Probability the review is negative\\n\\nNeutral Probability: Probability the review is neutral\\n\\nPositive Probability: Probability the review is positive\\n\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\n\\nClassifier Decision: negative\\n\\nNegative Probability: 99.85%\\n\\nNeutral Probability: 0.04%\\n\\nPositive Probability: 0.12%\\n\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "id": "8ee1be78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E17-G4OM-ELFT-PR2\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-11-03 00:10:53\n",
      "Notes: Experiment 17: Evaluate prompt-based model collaboration with prediction and probabilities\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Instance: electra_large_gpt_sentiment_pred_probs\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 5493 / 6530  (84.1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [1:56:20<00:00,  1.07s/it]  \n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "classifier_decision    6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2410\n",
      "positive               2343\n",
      "neutral                1769\n",
      "review: superior places for breakfast within walking distance of this place.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 5.12%\n",
      "\n",
      "neutral probability: 3.45%\n",
      "\n",
      "positive probability: 91.43%\n",
      "\n",
      "classification: positive      1\n",
      "review: and was very inexpensive...about $3.90 for one rather large sausage. i am sure it was it was leftover meat.\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 85.67%\n",
      "\n",
      "neutral probability: 10.12%\n",
      "\n",
      "positive probability: 4.21%\n",
      "\n",
      "classification: negative      1\n",
      "review: they gratefully charge 5% more and do an amazing nothing extra.\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 85.00%\n",
      "\n",
      "neutral probability: 5.00%\n",
      "\n",
      "positive probability: 10.00%\n",
      "\n",
      "classification: negative      1\n",
      "review: i assumed since i was paying by the hour, that they would try to squeeze out every minute, but they literally ran to and from the truck.\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 85.23%\n",
      "\n",
      "neutral probability: 5.12%\n",
      "\n",
      "positive probability: 9.65%\n",
      "\n",
      "classification: negative      1\n",
      "review: boring we did n't .\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 85.00%\n",
      "\n",
      "neutral probability: 10.00%\n",
      "\n",
      "positive probability: 5.00%\n",
      "\n",
      "classification: negative      1\n",
      "review: also the vietnamese coffee with no sugar is could be mad concentrated and might keep u laced up all night.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 25.45%\n",
      "\n",
      "neutral probability: 42.67%\n",
      "\n",
      "positive probability: 31.88%\n",
      "\n",
      "classification: neutral      1\n",
      "review: other quick trips have pretty much the same inventory but not these employees.\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 45.67%\n",
      "\n",
      "neutral probability: 30.12%\n",
      "\n",
      "positive probability: 24.21%\n",
      "\n",
      "classification: negative      1\n",
      "review: the gentleman staffing the bar seemed a bit gruff, but a good caffeine fix will help me forgive even the orneriest grump.\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 84.37%\n",
      "\n",
      "neutral probability: 0.53%\n",
      "\n",
      "positive probability: 15.10%\n",
      "\n",
      "classification: negative      1\n",
      "\n",
      "Found 8 invalid predictions at row indices:\n",
      "[287, 869, 1076, 2407, 4336, 5073, 5368, 6457]\n",
      "\n",
      "Invalid prediction rows:\n",
      "                                                                                                                                      sentence  \\\n",
      "287                                                                       Superior places for breakfast within walking distance of this place.   \n",
      "869                                And was very inexpensive...about $3.90 for one rather large sausage. I am sure it was it was leftover meat.   \n",
      "1076                                                                           They gratefully charge 5% more and do an amazing NOTHING extra.   \n",
      "2407  I assumed since I was paying by the hour, that they would try to squeeze out every minute, but they literally ran to and from the truck.   \n",
      "4336                                                                                                                       Boring we did n't .   \n",
      "5073                                Also the Vietnamese coffee with no sugar is could be mad concentrated and might keep u laced up all night.   \n",
      "5368                                                            other quick trips have pretty much the same inventory but not these employees.   \n",
      "6457                 The gentleman staffing the bar seemed a bit gruff, but a good caffeine fix will help me forgive even the orneriest grump.   \n",
      "\n",
      "         label       source split  \n",
      "287   negative  dynasent_r1  test  \n",
      "869   negative  dynasent_r2  test  \n",
      "1076  negative  dynasent_r2  test  \n",
      "2407  positive  dynasent_r1  test  \n",
      "4336  negative    sst_local  test  \n",
      "5073   neutral  dynasent_r2  test  \n",
      "5368   neutral  dynasent_r1  test  \n",
      "6457  negative  dynasent_r1  test  \n",
      "\n",
      "WARNING: Invalid predictions found. Please review the above details and re-run problematic cases.\n"
     ]
    }
   ],
   "source": [
    "e17_g4om_elft_pr2_df, e17_g4om_elft_pr2_metrics = evaluate_experiment(\n",
    "    name='E17-G4OM-ELFT-PR2',\n",
    "    notes='Experiment 17: Evaluate prompt-based model collaboration with prediction and probabilities',\n",
    "    lm='gpt-4o-mini-2024-07-18',\n",
    "    instance='electra_large_gpt_sentiment_pred_probs',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "id": "644fdd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_indices = [287, 869, 1076, 2407, 4336, 5073, 5368, 6457]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "id": "39141246",
   "metadata": {},
   "outputs": [],
   "source": [
    "e17_g4om_elft_pr2_df_orig = pd.DataFrame.copy(e17_g4om_elft_pr2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "id": "100c46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the invalid rows\n",
    "e17_g4om_elft_pr2_df.loc[invalid_indices, 'prediction'] = e17_g4om_elft_pr2_df.loc[invalid_indices].apply(update_prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "id": "79341b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated rows:\n",
      "                                                                                                                                        review  \\\n",
      "287                                                                       Superior places for breakfast within walking distance of this place.   \n",
      "869                                And was very inexpensive...about $3.90 for one rather large sausage. I am sure it was it was leftover meat.   \n",
      "1076                                                                           They gratefully charge 5% more and do an amazing NOTHING extra.   \n",
      "2407  I assumed since I was paying by the hour, that they would try to squeeze out every minute, but they literally ran to and from the truck.   \n",
      "4336                                                                                                                       Boring we did n't .   \n",
      "5073                                Also the Vietnamese coffee with no sugar is could be mad concentrated and might keep u laced up all night.   \n",
      "5368                                                            other quick trips have pretty much the same inventory but not these employees.   \n",
      "6457                 The gentleman staffing the bar seemed a bit gruff, but a good caffeine fix will help me forgive even the orneriest grump.   \n",
      "\n",
      "     classification prediction  \n",
      "287        negative   positive  \n",
      "869        negative   negative  \n",
      "1076       negative   negative  \n",
      "2407       positive   negative  \n",
      "4336       negative   negative  \n",
      "5073        neutral    neutral  \n",
      "5368        neutral   negative  \n",
      "6457       negative   negative  \n"
     ]
    }
   ],
   "source": [
    "# Verify the changes\n",
    "print(\"Updated rows:\")\n",
    "print(e17_g4om_elft_pr2_df.loc[invalid_indices, ['review', 'classification', 'prediction']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "902836e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E17-G4OM-ELFT-PR2\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-11-03 03:48:33\n",
      "Notes: Experiment 17: Evaluate prompt-based model collaboration with prediction and probabilities\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Instance: electra_large_gpt_sentiment_pred_probs\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Results shape: [6530, 6]\n",
      "Save directory: research\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2416\n",
      "positive               2344\n",
      "neutral                1770\n",
      "\n",
      "E17-G4OM-ELFT-PR2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8547    0.8780    0.8662      2352\n",
      "     neutral     0.7667    0.7419    0.7541      1829\n",
      "    positive     0.8835    0.8817    0.8826      2349\n",
      "\n",
      "    accuracy                         0.8412      6530\n",
      "   macro avg     0.8350    0.8339    0.8343      6530\n",
      "weighted avg     0.8404    0.8412    0.8407      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2065      221        66\n",
      "neutral         265     1357       207\n",
      "positive         86      192      2071\n",
      "\n",
      "\n",
      "E17-G4OM-ELFT-PR2-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9099    0.8583    0.8834      1200\n",
      "     neutral     0.7977    0.9067    0.8487      1200\n",
      "    positive     0.9167    0.8433    0.8785      1200\n",
      "\n",
      "    accuracy                         0.8694      3600\n",
      "   macro avg     0.8747    0.8694    0.8702      3600\n",
      "weighted avg     0.8747    0.8694    0.8702      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1030      133        37\n",
      "neutral          57     1088        55\n",
      "positive         45      143      1012\n",
      "\n",
      "\n",
      "E17-G4OM-ELFT-PR2-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7829    0.9167    0.8445       240\n",
      "     neutral     0.8556    0.6417    0.7333       240\n",
      "    positive     0.7838    0.8458    0.8136       240\n",
      "\n",
      "    accuracy                         0.8014       720\n",
      "   macro avg     0.8074    0.8014    0.7972       720\n",
      "weighted avg     0.8074    0.8014    0.7972       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        220       10        10\n",
      "neutral          40      154        46\n",
      "positive         21       16       203\n",
      "\n",
      "\n",
      "E17-G4OM-ELFT-PR2-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8126    0.8936    0.8512       912\n",
      "     neutral     0.5088    0.2956    0.3740       389\n",
      "    positive     0.8726    0.9417    0.9058       909\n",
      "\n",
      "    accuracy                         0.8081      2210\n",
      "   macro avg     0.7313    0.7103    0.7103      2210\n",
      "weighted avg     0.7838    0.8081    0.7897      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        815       78        19\n",
      "neutral         168      115       106\n",
      "positive         20       33       856\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    83.43\t   84.12\n",
      "DynaSent R1 \t    87.02\t   86.94\n",
      "DynaSent R2 \t    79.72\t   80.14\n",
      "SST-3       \t    71.03\t   80.81\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-11-03 03:48:34\n",
      "Duration: 0:00:00.465072\n"
     ]
    }
   ],
   "source": [
    "e17_g4om_elft_pr2_metrics = evaluate_experiment(\n",
    "    name='E17-G4OM-ELFT-PR2',\n",
    "    notes='Experiment 17: Evaluate prompt-based model collaboration with prediction and probabilities',\n",
    "    lm='gpt-4o-mini-2024-07-18',\n",
    "    instance='electra_large_gpt_sentiment_pred_probs',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex,\n",
    "    results=e17_g4om_elft_pr2_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6af2c5",
   "metadata": {},
   "source": [
    "### E18-G4OM-ELFT-EX-PR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "id": "9a83cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-mini-2024-07-18', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "id": "6c5fcf02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    examples=[{'index': 1128, 'similarity_score': 1.0, 'review': 'After some haggling, they offered $200 off.', 'classification': 'neutral'}, {'index': 5190, 'similarity_score': 0.9977380633354187, 'review': 'Then they write clearance outlet, 40%-70% off everywhere.', 'classification': 'neutral'}, {'index': 1185, 'similarity_score': 0.997637152671814, 'review': 'Every racer took their rightful places on the starting line.', 'classification': 'neutral'}, {'index': 3724, 'similarity_score': 0.9971697926521301, 'review': 'Needless to say, we ordered one round of food and dipped.', 'classification': 'neutral'}, {'index': 300, 'similarity_score': 0.9970298409461975, 'review': 'Some of the young kids sitting next to me were cracking up laughing though.', 'classification': 'neutral'}],\n",
       "    classification='neutral',\n",
       "    classifier_decision='neutral',\n",
       "    negative_probability='0.51%',\n",
       "    neutral_probability='59.30%',\n",
       "    positive_probability='40.18%'\n",
       ")"
      ]
     },
     "execution_count": 944,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electra_large_gpt_sentiment_pred_probs_examples(review='After some haggling, they offered $200 off.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "id": "0f606394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Examples: A list of examples that demonstrate different sentiment classes.\n",
      "\n",
      "Review: The review text to classify.\n",
      "\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "\n",
      "Negative Probability: Probability the review is negative\n",
      "\n",
      "Neutral Probability: Probability the review is neutral\n",
      "\n",
      "Positive Probability: Probability the review is positive\n",
      "\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Examples:\n",
      "- neutral: After some haggling, they offered $200 off.\n",
      "- neutral: Then they write clearance outlet, 40%-70% off everywhere.\n",
      "- neutral: Every racer took their rightful places on the starting line.\n",
      "- neutral: Needless to say, we ordered one round of food and dipped.\n",
      "- neutral: Some of the young kids sitting next to me were cracking up laughing though.\n",
      "\n",
      "Review: After some haggling, they offered $200 off.\n",
      "\n",
      "Classifier Decision: neutral\n",
      "\n",
      "Negative Probability: 0.51%\n",
      "\n",
      "Neutral Probability: 59.30%\n",
      "\n",
      "Positive Probability: 40.18%\n",
      "\n",
      "Classification:\u001b[32m neutral\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nExamples: A list of examples that demonstrate different sentiment classes.\\n\\nReview: The review text to classify.\\n\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\n\\nNegative Probability: Probability the review is negative\\n\\nNeutral Probability: Probability the review is neutral\\n\\nPositive Probability: Probability the review is positive\\n\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nExamples:\\n- neutral: After some haggling, they offered $200 off.\\n- neutral: Then they write clearance outlet, 40%-70% off everywhere.\\n- neutral: Every racer took their rightful places on the starting line.\\n- neutral: Needless to say, we ordered one round of food and dipped.\\n- neutral: Some of the young kids sitting next to me were cracking up laughing though.\\n\\nReview: After some haggling, they offered $200 off.\\n\\nClassifier Decision: neutral\\n\\nNegative Probability: 0.51%\\n\\nNeutral Probability: 59.30%\\n\\nPositive Probability: 40.18%\\n\\nClassification:\\x1b[32m neutral\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 945,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "id": "0fb9ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "e18_g4om_elft_ex_pr2_result = electra_large_gpt_sentiment_pred_probs_examples(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "id": "444467a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    examples=[{'index': 57, 'similarity_score': 0.9977191090583801, 'review': 'This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.', 'classification': 'negative'}, {'index': 3410, 'similarity_score': 0.9972444176673889, 'review': 'This was my second time here and there will not be a third time.', 'classification': 'negative'}, {'index': 3476, 'similarity_score': 0.9971747398376465, 'review': 'Portioning of the Octopus could have been better.', 'classification': 'negative'}, {'index': 3679, 'similarity_score': 0.9971185922622681, 'review': 'My only complaint was the bill for a long distance phone call - $4 for a 3 minutes to Seattle.', 'classification': 'negative'}, {'index': 3160, 'similarity_score': 0.9971152544021606, 'review': 'I received big-time attitude from multiple \"fabulous\" employees when I pulled up at 7:29 a.m. to get my car washed.', 'classification': 'negative'}],\n",
       "    classification='negative',\n",
       "    classifier_decision='negative',\n",
       "    negative_probability='99.85%',\n",
       "    neutral_probability='0.04%',\n",
       "    positive_probability='0.12%'\n",
       ")"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e18_g4om_elft_ex_pr2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "id": "a3c463df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Examples: A list of examples that demonstrate different sentiment classes.\n",
      "\n",
      "Review: The review text to classify.\n",
      "\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "\n",
      "Negative Probability: Probability the review is negative\n",
      "\n",
      "Neutral Probability: Probability the review is neutral\n",
      "\n",
      "Positive Probability: Probability the review is positive\n",
      "\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Examples:\n",
      "- negative: This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.\n",
      "- negative: This was my second time here and there will not be a third time.\n",
      "- negative: Portioning of the Octopus could have been better.\n",
      "- negative: My only complaint was the bill for a long distance phone call - $4 for a 3 minutes to Seattle.\n",
      "- negative: I received big-time attitude from multiple \"fabulous\" employees when I pulled up at 7:29 a.m. to get my car washed.\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "\n",
      "Classifier Decision: negative\n",
      "\n",
      "Negative Probability: 99.85%\n",
      "\n",
      "Neutral Probability: 0.04%\n",
      "\n",
      "Positive Probability: 0.12%\n",
      "\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nClassify the sentiment of a review as either \\'negative\\', \\'neutral\\', or \\'positive\\'.\\n\\n---\\n\\nFollow the following format.\\n\\nExamples: A list of examples that demonstrate different sentiment classes.\\n\\nReview: The review text to classify.\\n\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\n\\nNegative Probability: Probability the review is negative\\n\\nNeutral Probability: Probability the review is neutral\\n\\nPositive Probability: Probability the review is positive\\n\\nClassification: One word representing the sentiment classification: \\'negative\\', \\'neutral\\', or \\'positive\\' (do not repeat the field name, do not use \\'mixed\\')\\n\\n---\\n\\nExamples:\\n- negative: This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.\\n- negative: This was my second time here and there will not be a third time.\\n- negative: Portioning of the Octopus could have been better.\\n- negative: My only complaint was the bill for a long distance phone call - $4 for a 3 minutes to Seattle.\\n- negative: I received big-time attitude from multiple \"fabulous\" employees when I pulled up at 7:29 a.m. to get my car washed.\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\n\\nClassifier Decision: negative\\n\\nNegative Probability: 99.85%\\n\\nNeutral Probability: 0.04%\\n\\nPositive Probability: 0.12%\\n\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "b8a47353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E18-G4OM-ELFT-EX-PR2\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-11-03 04:17:25\n",
      "Notes: Experiment 18: Evaluate prompt-based model collaboration with prediction, probabilities, and similar examples\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Instance: electra_large_gpt_sentiment_pred_probs_examples\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 5452 / 6530  (83.5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [1:16:17<00:00,  1.43it/s]\n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "classifier_decision    6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2410\n",
      "positive               2243\n",
      "neutral                1795\n",
      "negative: i was extremely disappointed with the service i received; it was slow and unprofessional.      1\n",
      "examples:\n",
      "- negative: the service was terrible and the food was cold.\n",
      "- neutral: the restaurant is located downtown.\n",
      "- positive: the dessert was absolutely delicious and beautifully presented.\n",
      "\n",
      "review: the french toast is big and i could not even finish!\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.42%\n",
      "\n",
      "neutral probability: 1.12%\n",
      "\n",
      "positive probability: 98.46%\n",
      "\n",
      "classification: positive      1\n",
      "- neutral: i ordered fried bananas and ice cream, vegetarian pad thai and crab rangoon.\n",
      "- neutral: we started with cornbread waffles with pimento cheese and pepper jelly.\n",
      "- neutral: so he ordered a sandwich with a borscht on the side and i got cheddar and potatoes pierogi.\n",
      "- neutral: i ordered the steak fajita quesadilla.\n",
      "- neutral: we ordered from sunday brunch menu, my husband had the eggs florentine and i had the omelet with creamy grits.\n",
      "\n",
      "review: i ordered a blended root beer float and got my boyfriend a strawberry, watermelon slush.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 0.03%\n",
      "\n",
      "neutral probability: 97.58%\n",
      "\n",
      "positive probability: 2.39%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: the ambiance was delightful and the service was impeccable.\n",
      "- negative: i was really disappointed with the quality of the food; it was bland and overcooked.\n",
      "- neutral: the restaurant was okay, nothing special but not terrible either.\n",
      "- positive: the staff was friendly and the dessert was to die for!\n",
      "- negative: i will never come back here again; the wait was too long and the food was cold.\n",
      "\n",
      "review: the service was slow and the food was not worth the price.\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 92.45%\n",
      "\n",
      "neutral probability: 5.12%\n",
      "\n",
      "positive probability: 2.43%\n",
      "\n",
      "classification: negative      1\n",
      "- positive: i am so surprised, after all, the location is good and the area is one of the best areas in peoria.\n",
      "- positive: the kicker though, the special sauce, having some sort of lime flavor to it, which literally had me doing back flips off the stool.\n",
      "- positive: the best part of this cheese brisket was that it was part of the happy hour special and only $7.\n",
      "- positive: both come with 2 eggs and a side of bacon or sausage for under $10, could beat that!\n",
      "- positive: the best part of this place so far, is the bird exhibit & access to the strip & linq.\n",
      "\n",
      "review: and in the past the service was good too- at least 3-4 stars.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.23%\n",
      "\n",
      "neutral probability: 0.05%\n",
      "\n",
      "positive probability: 99.72%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: the service was exceptional and the food was delicious.\n",
      "- neutral: the restaurant was busy, and we had to wait for a table.\n",
      "- negative: i was disappointed with the quality of the food and the service was slow.\n",
      "- neutral: the ambiance was okay, but nothing special.\n",
      "- positive: i loved the dessert; it was the highlight of my meal.\n",
      "\n",
      "review: the food was average, and the atmosphere was nothing to write home about.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 15.00%\n",
      "\n",
      "neutral probability: 70.00%\n",
      "\n",
      "positive probability: 15.00%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: it is great summer fun to watch arnold and his buddy gerald bounce off a quirky cast of characters.\n",
      "- positive: a superbly acted and funny/ gritty fable of the humanizing of one woman at the hands of the unseen forces of fate.\n",
      "- positive: i had such an amazing experience at lawry's the first time i was there that i had to reserve to dine again for my birthday this past weekend.\n",
      "- positive: the vibe is below low key and you might accidentally write a poem or sonnet while you blissfully day dream about fresh baked bread and the goodness that will appear between those slices of heaven.\n",
      "- positive: when i got there, i was greeted by a friendly receptionist and served tea and biscuits as others described (pretty normal for any higher end hair salon) and was brought to my seat.\n",
      "\n",
      "review: ... both hokey and super-cool, and definitely not in a hurry, so sit back, relax and have a few laughs while the little ones get a fuzzy treat.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.02%\n",
      "\n",
      "neutral probability: 0.04%\n",
      "\n",
      "positive probability: 99.94%\n",
      "\n",
      "classification: positive      1\n",
      "examples:\n",
      "- positive: the service was outstanding and the food was delicious!\n",
      "- negative: i had a terrible experience; the staff was rude and unhelpful.\n",
      "- neutral: the restaurant was okay, nothing special but not bad either.\n",
      "- positive: i absolutely loved the ambiance and the music was great!\n",
      "- negative: the meal was cold and took too long to arrive.\n",
      "\n",
      "review: the atmosphere was nice, but the food was just average.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 12.34%\n",
      "\n",
      "neutral probability: 75.67%\n",
      "\n",
      "positive probability: 12.99%\n",
      "\n",
      "classification: neutral      1\n",
      "- negative: the food was cold and the service was terrible. i will not be coming back.\n",
      "- neutral: the restaurant was okay, nothing special but not bad either.\n",
      "- positive: the ambiance was lovely and the staff were very attentive. i had a great time!\n",
      "\n",
      "review: the atmosphere was nice, but the food was just average and didn't live up to my expectations.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 15.00%\n",
      "\n",
      "neutral probability: 70.00%\n",
      "\n",
      "positive probability: 15.00%\n",
      "\n",
      "classification: neutral      1\n",
      "negative: don't get me started on the chips... (3 bowls).\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 78.45%\n",
      "\n",
      "neutral probability: 15.32%\n",
      "\n",
      "positive probability: 6.23%\n",
      "\n",
      "classification: negative      1\n",
      "- positive: the food was absolutely delicious and the service was outstanding.\n",
      "- negative: i had a terrible experience; the staff was rude and the food was cold.\n",
      "- neutral: the restaurant was okay, nothing special but not bad either.\n",
      "- positive: i love this place! the atmosphere is great and the staff is friendly.\n",
      "- negative: i will never come back; the wait was too long and the meal was disappointing.\n",
      "\n",
      "review: the ambiance was pleasant, but the food was just average.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 15.67%\n",
      "\n",
      "neutral probability: 72.34%\n",
      "\n",
      "positive probability: 12.99%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: the staff was incredibly friendly and attentive throughout our meal.\n",
      "- positive: the product exceeded my expectations and works perfectly.\n",
      "- neutral: the restaurant was busy, and we had to wait for a table.\n",
      "- neutral: i received my order on time, but it was just okay.\n",
      "- negative: the service was slow, and the food was cold when it arrived.\n",
      "\n",
      "review: the food was bland and the service was inattentive, which made for a disappointing experience.\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 92.45%\n",
      "\n",
      "neutral probability: 5.12%\n",
      "\n",
      "positive probability: 2.43%\n",
      "\n",
      "classification: negative      1\n",
      "- positive: makes for a nice atmosphere but i want this place to do well so it sticks around - get out there and try it people!\n",
      "\n",
      "review: the food was bland and the service was slow, which made for a disappointing experience.\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 95.00%\n",
      "\n",
      "neutral probability: 3.00%\n",
      "\n",
      "positive probability: 2.00%\n",
      "\n",
      "classification: negative      1\n",
      "examples:\n",
      "- positive: the food was absolutely delicious and the service was excellent.\n",
      "- negative: i waited for an hour and my order was still wrong.\n",
      "- neutral: the restaurant was okay, nothing special but not bad either.\n",
      "- positive: i had a wonderful experience and will definitely come back.\n",
      "- negative: the movie was boring and i fell asleep halfway through.\n",
      "\n",
      "review: the service was slow, and the food was just average.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 25.67%\n",
      "\n",
      "neutral probability: 50.12%\n",
      "\n",
      "positive probability: 24.21%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: the garlic mayonnaise's we are use to from a place in atlanta, prepares it with fresh garlic daily for the shawarma, garlic and lemon/lime, sea salt are fresh.\n",
      "- positive: update: took my car to my usual trustworthy auto repair shop, and for just $80 bucks in labor costs, he tightens up some loose screws and solves the problem.\n",
      "- positive: i suggest going during lunch hours, you will get a lot more bang for your buck and end up paying about $14 for two people and more food.\n",
      "- negative: my pregnant wife and her friend both got the trots from this place.\n",
      "- positive: the super gyro special includes a gyro, fresh cut fries, and a drink for only $7.47 if you find a better deal on a better quality gyro, you tell me.\n",
      "\n",
      "review: after i placed my order in the drive thru, i drove up and my food bag was already out the window.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.19%\n",
      "\n",
      "neutral probability: 0.77%\n",
      "\n",
      "positive probability: 99.04%\n",
      "\n",
      "classification: positive      1\n",
      "- neutral: we had been sent a 20% off coupon and hub was dying to try this....so off we went on a monday night around 5pm.\n",
      "- positive: i took it to ubreakifix and they knew right away what was wrong, the power button needed to be replaced.\n",
      "- neutral: our other friend suggested this place, and said for $27, you can get all you can eat sushi as well as anything else on their menu.\n",
      "- neutral: i used to come here often, every time i needed to go grocery shopping i went to this store.\n",
      "- neutral: there isn't an applebees near us in south chandler and this location is close to his work.\n",
      "\n",
      "review: again...more than happy to upload the image of the texts for anyone to see.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 0.13%\n",
      "\n",
      "neutral probability: 59.04%\n",
      "\n",
      "positive probability: 40.83%\n",
      "\n",
      "classification: neutral      1\n",
      "negative: the service was terrible and the food was cold. \n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 85.20%\n",
      "\n",
      "neutral probability: 10.50%\n",
      "\n",
      "positive probability: 4.30%\n",
      "\n",
      "classification: negative      1\n",
      "- positive: update: took my car to my usual trustworthy auto repair shop, and for just $80 bucks in labor costs, he tightens up some loose screws and solves the problem.\n",
      "- positive: i suggest going during lunch hours, you will get a lot more bang for your buck and end up paying about $14 for two people and more food.\n",
      "- positive: weekdays however tend to be very quiet and you rarely will have to wait for a sales person to help you.\n",
      "- positive: not only did they tell me that i would get reimbursed for renting car to drive since they did not have another plane to fly us for 2 days but that they would reimbursed us for the trip.\n",
      "- positive: but really, the best part of the wedding was the ceremony.\n",
      "\n",
      "review: a friend and i went on a thursday evening around 730/ 8 pm or so and it was a little busy, but we have to wait at all for a table.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.08%\n",
      "\n",
      "neutral probability: 0.55%\n",
      "\n",
      "positive probability: 99.37%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: reserved and paid for the car online $20.37, got there and service member offered free upgrade because no compact car was available.\n",
      "\n",
      "review: reserved and paid for the car online $20.37, got there and service member offered free upgrade because no compact car was available.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.84%\n",
      "\n",
      "neutral probability: 0.44%\n",
      "\n",
      "positive probability: 98.72%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: the food was absolutely delicious and the service was outstanding.\n",
      "- negative: i had a terrible experience; the staff was rude and the food was cold.\n",
      "- neutral: the restaurant was okay, nothing special but not bad either.\n",
      "- positive: i love the ambiance and the variety of dishes they offer.\n",
      "- negative: i will never come back; the wait time was unacceptable.\n",
      "\n",
      "review: the service was slow and the food was mediocre at best.\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 85.67%\n",
      "\n",
      "neutral probability: 10.12%\n",
      "\n",
      "positive probability: 4.21%\n",
      "\n",
      "classification: negative      1\n",
      "- positive: i ended up using my friend's recommendation of first class auto service where she always takes her car nearby and they were fantastic!\n",
      "- positive: my coworkers enjoyed their steaks and scallops and the margaritas were decent.\n",
      "- positive: after mentioning the problem to our server, he did the right thing and gave us the meal free.\n",
      "- positive: we tried two new places. both were tasty and their menu doesn't break the bank.\n",
      "- positive: my dinner companion had a 7oz sirloin and it too was mouth-watering and juicy.\n",
      "\n",
      "review: (eventually a delicious red sangria was selected, which went down entirely too fast.)\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.03%\n",
      "\n",
      "neutral probability: 0.00%\n",
      "\n",
      "positive probability: 99.97%\n",
      "\n",
      "classification: positive      1\n",
      "- negative: i was in an accident a lil while back and i was in so much pain!\n",
      "- negative: was driving home one day and noticed my car was getting hot, got home and saw my radiator was cracked on top and leaking.\n",
      "- negative: since it was my first time i was a bit overwhelmed with the choices.\n",
      "- neutral: they taught that we ate all food but we don't.\n",
      "- negative: i had been searching for a new stylist for quite some time and have had bad luck in the past.\n",
      "\n",
      "review: also turns out i drive a 2012 camry not a 2014!\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 66.87%\n",
      "\n",
      "neutral probability: 32.67%\n",
      "\n",
      "positive probability: 0.46%\n",
      "\n",
      "classification: negative      1\n",
      "- neutral: booked a block of rooms for my sister's wedding (she got married there at the flamingo).\n",
      "- neutral: i ate half my order when i got it and the rest of the wings later in the day.\n",
      "- neutral: we booked this food truck to come to our workplace three weeks in advance.\n",
      "- neutral: after donating blood for red cross, i was craving for eggs benedict.\n",
      "- neutral: the last time i went to mimi's cafe, at 7450 w bell rd, glendale, az 85308, was june 29th, my wife's birthday.\n",
      "\n",
      "review: been here twice in the past week, once today for brunch and once for dinner.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 0.03%\n",
      "\n",
      "neutral probability: 96.27%\n",
      "\n",
      "positive probability: 3.70%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: the service was excellent and the staff were very friendly.\n",
      "- neutral: the product arrived on time and was as described.\n",
      "- negative: i was very disappointed with the quality of the item i received.\n",
      "- neutral: the meeting was scheduled for 10 am and started on time.\n",
      "- positive: i absolutely loved the new restaurant; the food was delicious!\n",
      "\n",
      "review: they have many coupons available on the app and in store, just ask about them. will you?\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 0.43%\n",
      "\n",
      "neutral probability: 54.36%\n",
      "\n",
      "positive probability: 45.21%\n",
      "\n",
      "classification: neutral      1\n",
      "- neutral: don't you see value in this plan?\n",
      "- positive: again, take your time here and set aside two hours.\n",
      "- neutral: mom and dad can catch some quality naptime along the way.\n",
      "- positive: 1/2 price drinks during a daytime happy hour.\n",
      "- positive: i tried this dim sum place twice just to make sure and give them a second chance.\n",
      "\n",
      "review: and if the hours wins ` best picture ' i just might.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.65%\n",
      "\n",
      "neutral probability: 38.07%\n",
      "\n",
      "positive probability: 61.28%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: the service was excellent and the atmosphere was lovely.\n",
      "- negative: i was disappointed with the quality of the food and the service was slow.\n",
      "- neutral: the restaurant was okay, nothing special but not terrible either.\n",
      "- positive: i absolutely loved the dessert; it was the highlight of my meal!\n",
      "- negative: the wait time was too long and the staff seemed uninterested.\n",
      "\n",
      "review: the service was slow and the food was mediocre.\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 85.00%\n",
      "\n",
      "neutral probability: 10.00%\n",
      "\n",
      "positive probability: 5.00%\n",
      "\n",
      "classification: negative      1\n",
      "- neutral: despite this, i was able to make a follow up appointment instead for next sunday (despite what yelp says, the charleston office is actually open on sundays).\n",
      "- positive: i took a taxi back to rio casino which is only a 5 min drive, when i got back i put the juices in the fridge of my room and then my friend comes up less than an hr later and tried the juice bc i was speaking highly of the juices.\n",
      "- positive: i ordered the original hot dog, the fries and the beer, and after a few minutes, the food was ready.\n",
      "- positive: and if i were to recommend an entree, it would have to be the chicken shwarma plate.\n",
      "- positive: you find yourself fingering every item and mentally checking off who's birthday is coming up or what occasion you can use as an excuse to snap up the chocolate mustache on a stick (how fun is that?)\n",
      "\n",
      "review: one another note, another review says, \"if you're into young asian girls (and guys), this is the place to hang out.\"\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.10%\n",
      "\n",
      "neutral probability: 16.90%\n",
      "\n",
      "positive probability: 83.00%\n",
      "\n",
      "classification: positive      1\n",
      "negative: the service was slow and the food was cold when it finally arrived.  \n",
      "negative: i was really disappointed with my experience; the staff was rude and unhelpful.  \n",
      "neutral: the restaurant was okay, but nothing special.  \n",
      "neutral: i had a meal there, and it was just average.  \n",
      "positive: the ambiance was lovely, and the staff was very attentive.  \n",
      "positive: i absolutely loved the dessert; it was the highlight of my meal!  \n",
      "\n",
      "review: the food was decent, but i expected more from the reviews i read.  \n",
      "\n",
      "classifier decision: neutral  \n",
      "\n",
      "negative probability: 15.32%  \n",
      "\n",
      "neutral probability: 62.45%  \n",
      "\n",
      "positive probability: 22.23%  \n",
      "\n",
      "classification: neutral      1\n",
      "- positive: the food was absolutely delicious and the service was outstanding.\n",
      "- negative: i was really disappointed with the quality of the product; it broke after one use.\n",
      "- neutral: the restaurant was okay, nothing special but not terrible either.\n",
      "- positive: i love this app; it has made my life so much easier!\n",
      "- negative: the movie was a complete waste of time; i wouldn't recommend it to anyone.\n",
      "\n",
      "review: if i wasn't worried about my waistline i'd go there a lot more often.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 1.43%\n",
      "\n",
      "neutral probability: 0.35%\n",
      "\n",
      "positive probability: 98.22%\n",
      "\n",
      "classification: positive      1\n",
      "- negative: the plot was predictable and the characters were one-dimensional.\n",
      "- positive: the cinematography was breathtaking and the performances were outstanding.\n",
      "- neutral: the movie had its moments, but overall it was just okay.\n",
      "- positive: a delightful experience that left me smiling long after it ended.\n",
      "- negative: i was disappointed with the ending; it felt rushed and unsatisfying.\n",
      "\n",
      "review: one scarcely needs the subtitles to enjoy this colorful action farce.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.02%\n",
      "\n",
      "neutral probability: 0.37%\n",
      "\n",
      "positive probability: 99.62%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: the food was absolutely delicious and the service was impeccable.\n",
      "- negative: i was really disappointed with the quality of the product; it broke after one use.\n",
      "- neutral: the movie was okay, but i wouldn't watch it again.\n",
      "- positive: the staff were friendly and made my experience enjoyable.\n",
      "- negative: i had to wait for over an hour, and the staff were unhelpful.\n",
      "\n",
      "review: the atmosphere was nice, but the food was just average.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 15.23%\n",
      "\n",
      "neutral probability: 68.45%\n",
      "\n",
      "positive probability: 16.32%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: the food was absolutely delicious and the service was excellent!\n",
      "- neutral: the meeting is scheduled for next tuesday at 10 am.\n",
      "- negative: i was really disappointed with the quality of the product; it broke after one use.\n",
      "\n",
      "review: the hotel room was clean, but the staff was not very friendly.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 15.32%\n",
      "\n",
      "neutral probability: 70.45%\n",
      "\n",
      "positive probability: 14.23%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: the food was absolutely delicious and the service was impeccable.\n",
      "- negative: i had a terrible experience; the staff was rude and the food was cold.\n",
      "- neutral: the restaurant was okay, nothing special but not bad either.\n",
      "- positive: i love this place! the ambiance is great and the drinks are fantastic.\n",
      "- negative: i will never come back; the wait was too long and the meal was disappointing.\n",
      "\n",
      "review: the atmosphere was nice, but the food was just average.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 15.34%\n",
      "\n",
      "neutral probability: 68.12%\n",
      "\n",
      "positive probability: 16.54%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: an operatic, sprawling picture that's entertainingly acted, magnificently shot and gripping enough to sustain most of its 170-minute length.\n",
      "- positive: offers much to enjoy ... and a lot to mull over in terms of love, loyalty and the nature of staying friends.\n",
      "- positive: building slowly and subtly, the film, sporting a breezy spontaneity and realistically drawn characterizations, develops into a significant character study that is both moving and wise.\n",
      "- positive: this is a winning ensemble comedy that shows canadians can put gentle laughs and equally gentle sentiments on the button, just as easily as their counterparts anywhere else in the world.\n",
      "- positive: a densely constructed, highly referential film, and an audacious return to form that can comfortably sit among jean-luc godard's finest work.\n",
      "\n",
      "review: at times funny and at other times candidly revealing, it's an intriguing look at two performers who put themselves out there because they love what they do.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.00%\n",
      "\n",
      "neutral probability: 0.22%\n",
      "\n",
      "positive probability: 99.78%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: the service was excellent and the atmosphere was delightful.\n",
      "- negative: i was really disappointed with the quality of the food.\n",
      "- neutral: the restaurant was okay, nothing special but not bad either.\n",
      "- positive: this is the best coffee i've ever had!\n",
      "- negative: i will never return to this place again; it was a terrible experience.\n",
      "\n",
      "review: but i keep coming back because of the food...\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.06%\n",
      "\n",
      "neutral probability: 0.56%\n",
      "\n",
      "positive probability: 99.38%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: the ambiance of the restaurant was delightful, and the staff was incredibly friendly.\n",
      "- negative: i was disappointed with the service; my order was wrong and took too long to fix.\n",
      "- neutral: the food was okay, nothing special but not bad either.\n",
      "- positive: this is the best pizza i've ever had; the crust was perfect and the toppings were fresh.\n",
      "- negative: i will never return to this place; the experience was terrible from start to finish.\n",
      "\n",
      "review: the movie was just average; it had some good moments but overall felt too long and boring.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 12.30%\n",
      "\n",
      "neutral probability: 75.50%\n",
      "\n",
      "positive probability: 12.20%\n",
      "\n",
      "classification: neutral      1\n",
      "- negative: the food was cold and the service was terrible. i will not be returning.\n",
      "- positive: the ambiance was lovely and the staff were incredibly friendly. i had a wonderful time.\n",
      "- neutral: the restaurant was okay, but nothing stood out as particularly good or bad.\n",
      "- positive: i absolutely loved the dessert! it was the highlight of my meal.\n",
      "- negative: i was disappointed with my experience; the wait was too long and the food was bland.\n",
      "\n",
      "review: we liked the place. there was no notice or sign about the closure so i'm not quite sure what is going on.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 1.18%\n",
      "\n",
      "neutral probability: 0.16%\n",
      "\n",
      "positive probability: 98.67%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: i had an amazing experience at the restaurant, the food was delicious and the service was excellent.\n",
      "- negative: the movie was a complete waste of time, i regretted watching it.\n",
      "- neutral: the book was okay, not great but not terrible either.\n",
      "- positive: i love this new phone, it has all the features i need and works perfectly.\n",
      "- negative: the hotel was dirty and the staff were rude, i will never stay there again.\n",
      "\n",
      "review: i first came here with a group of friends and we all had our fill of undecorated drink and merriment.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.08%\n",
      "\n",
      "neutral probability: 0.04%\n",
      "\n",
      "positive probability: 99.88%\n",
      "\n",
      "classification: positive      1\n",
      "examples:\n",
      "- negative: the service was terrible and the food was cold.\n",
      "- negative: i will never come back to this restaurant again.\n",
      "- neutral: the place was okay, nothing special.\n",
      "- neutral: i visited the cafe, and it was just average.\n",
      "- positive: the staff were friendly and the atmosphere was lovely.\n",
      "\n",
      "review: the food was delicious, but the wait time was too long.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 15.00%\n",
      "\n",
      "neutral probability: 70.00%\n",
      "\n",
      "positive probability: 15.00%\n",
      "\n",
      "classification: neutral      1\n",
      "negative: the pork chop is their signature dish and somehow it tastes different every time you try.      1\n",
      "- positive: the food was absolutely delicious and the service was fantastic.\n",
      "- negative: i had a terrible experience; the staff was rude and the food was cold.\n",
      "- neutral: the restaurant was okay, nothing special but not bad either.\n",
      "- positive: i love this place! the atmosphere is great and the drinks are amazing.\n",
      "- negative: i will never come back; the wait was too long and the meal was disappointing.\n",
      "\n",
      "review: the service was slow and the food was average at best.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 25.67%\n",
      "\n",
      "neutral probability: 50.12%\n",
      "\n",
      "positive probability: 24.21%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: the genius even went so far as to clean the entire laptop as it was dirty.\n",
      "\n",
      "review: the service was slow and the food was cold when it arrived.\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 85.12%\n",
      "\n",
      "neutral probability: 10.45%\n",
      "\n",
      "positive probability: 4.43%\n",
      "\n",
      "classification: negative      1\n",
      "- positive: so now i'm at a salon down the street as i type, relaxing with my feet soaking and so far it looks as i have found my new spot.\n",
      "- positive: however, we made the right choice and went to a sam fox restaurant where we were taken care of.\n",
      "- positive: when it's time to replace it...they'll be my first call.\n",
      "- positive: i explained exactly what i needed and the two (very nice and personable) students seemed to know what to do.\n",
      "- positive: our vehicles always look as though they just came off the showroom floor when he is done.\n",
      "\n",
      "review: every time i have been there i compared prices with several other tire shops and they are always the cheapest.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.01%\n",
      "\n",
      "neutral probability: 0.02%\n",
      "\n",
      "positive probability: 99.98%\n",
      "\n",
      "classification: positive      1\n",
      "negative: i was really disappointed with the service at this restaurant; the food was cold and took forever to arrive.      1\n",
      "- positive: the food was absolutely delicious and the service was outstanding.\n",
      "- negative: i had a terrible experience; the staff was rude and the food was cold.\n",
      "- neutral: the restaurant was okay, nothing special but not bad either.\n",
      "- positive: i love the ambiance here; it's perfect for a date night.\n",
      "- negative: i will never come back; the wait time was unacceptable.\n",
      "\n",
      "review: the gelato here is the best i've ever had, but the service could be improved.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 15.00%\n",
      "\n",
      "neutral probability: 70.00%\n",
      "\n",
      "positive probability: 15.00%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: i absolutely loved the cupcakes at gigi's! they were moist and flavorful, and the staff was incredibly friendly.\n",
      "- neutral: the location of gigi's is convenient, and i noticed they have a variety of flavors available.\n",
      "- neutral: i went to gigi's on a saturday afternoon, and it was quite busy with customers.\n",
      "- negative: i was disappointed with my experience at gigi's; the cupcakes were dry and lacked flavor.\n",
      "- positive: gigi's has the best red velvet cupcakes i've ever tasted! i can't wait to go back.\n",
      "\n",
      "review: this was my first time going to gigi's and was looking forward to it since the previous cupcake store closed.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 0.10%\n",
      "\n",
      "neutral probability: 59.34%\n",
      "\n",
      "positive probability: 40.55%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: the atmosphere was vibrant and the staff were incredibly friendly.\n",
      "- negative: i waited for over an hour and my order was still wrong.\n",
      "- neutral: the food was okay, nothing special but not bad either.\n",
      "- positive: this place has the best coffee i've ever tasted!\n",
      "- negative: i was disappointed with the service; it felt rushed and unwelcoming.\n",
      "\n",
      "review: the dessert was too sweet for my taste, but the presentation was lovely.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 15.67%\n",
      "\n",
      "neutral probability: 70.12%\n",
      "\n",
      "positive probability: 14.21%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: pacino is brilliant as the sleep-deprived dormer, his increasing weariness as much existential as it is physical.\n",
      "- positive: it's one heck of a character study -- not of hearst or davies but of the unique relationship between them.\n",
      "- positive: this is a story of two misfits who don't stand a chance alone, but together they are magnificent.\n",
      "- positive: what's surprising about full frontal is that despite its overt self-awareness, parts of the movie still manage to break past the artifice and thoroughly engage you.\n",
      "- positive: but taken as a stylish and energetic one-shot, the queen of the damned cannot be said to suck.\n",
      "\n",
      "review: i don't know precisely what to make of steven soderbergh's full frontal, though that didn't stop me from enjoying much of it.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.16%\n",
      "\n",
      "neutral probability: 2.36%\n",
      "\n",
      "positive probability: 97.48%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: the food was absolutely delicious and the service was impeccable.\n",
      "- negative: i waited for over an hour and my order was wrong.\n",
      "- neutral: the restaurant was okay, nothing special but not terrible either.\n",
      "- positive: this is the best coffee i've ever had!\n",
      "- negative: i was really disappointed with the quality of the product.\n",
      "\n",
      "review: i could die eating the gnocchi and chicken (the $17 dish).\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.10%\n",
      "\n",
      "neutral probability: 2.34%\n",
      "\n",
      "positive probability: 97.57%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: the staff was incredibly friendly and the food was delicious.\n",
      "- negative: i waited for over an hour and my order was wrong.\n",
      "- neutral: the restaurant was okay, nothing special but not terrible either.\n",
      "- positive: i absolutely loved the ambiance and the service was top-notch!\n",
      "- negative: the coffee was cold and tasted burnt.\n",
      "\n",
      "review: the service was slow and the food was mediocre at best.\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 87.45%\n",
      "\n",
      "neutral probability: 10.12%\n",
      "\n",
      "positive probability: 2.43%\n",
      "\n",
      "classification: negative      1\n",
      "- positive: the waitress was delightful and prompt while serving our table despite just finding out that her dog had passed away.\n",
      "- positive: the one thing better than their food is their customer service.\n",
      "- positive: i will offer the that complimentary french fries are good.\n",
      "- positive: fries also never have hardly any seasoning, which is one of the best things about their fries.\n",
      "- positive: the food at takos was to die for no wonder the foodnetwork was there filming while we were eating\n",
      "\n",
      "review: add bacon and then drowned them in butter, which was lovely for a vegan.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.49%\n",
      "\n",
      "neutral probability: 0.01%\n",
      "\n",
      "positive probability: 99.50%\n",
      "\n",
      "classification: positive      1\n",
      "- negative: i keep saying each time i go here: always hungry never lucky.\n",
      "- negative: workers were chatting with each other and took their time with drinks.\n",
      "- negative: it's so powerful that if you have any history of allergies, asthma, or migraines be careful staying here.\n",
      "- negative: never in my life have i gone anywhere where they wouldn't throw in something a little extra to earn a new regular customer and a five dollar toner wouldn't have killed them.\n",
      "- negative: that wire was taped down so well that even the kids were tripping over it.\n",
      "\n",
      "review: we were told that our table would be ready after a 50 minute wait and it was ready after a 50 minute rate.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 5.12%\n",
      "\n",
      "neutral probability: 84.67%\n",
      "\n",
      "positive probability: 10.21%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: when i lived in the east valley some five years ago, i used to go here all the time, loved the chili dogs.\n",
      "- positive: the meat was so tender that it just fell apart when it touched my fork.\n",
      "- positive: the vibe is below low key and you might accidentally write a poem or sonnet while you blissfully day dream about fresh baked bread and the goodness that will appear between those slices of heaven.\n",
      "- positive: he was kind and tried his best to rake up all the broken glass that was around.\n",
      "- positive: also, they have a pretty decent lunch and dinner menu but we didn't try any.\n",
      "\n",
      "review: we were too full to try dessert but the ones we saw being served to our neighboring tables looked good.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.05%\n",
      "\n",
      "neutral probability: 0.05%\n",
      "\n",
      "positive probability: 99.90%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: back in the early 2000s i thought this location was staffed by employees who walked on water - i rented regularly and they treated me extraordinarily well.\n",
      "- positive: i tried their \"bacon\" that's made out of rice paper and honestly it tastes better than real bacon!\n",
      "- positive: prices are reasonable until you try the food and realize you would pay whatever it takes to try this place again.\n",
      "- positive: the food at takos was to die for no wonder the foodnetwork was there filming while we were eating\n",
      "- negative: \"edible\" is the highest compliment i can offer!\n",
      "\n",
      "review: the place didn't have that hipster downtown vibe, bonus!\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 1.02%\n",
      "\n",
      "neutral probability: 0.02%\n",
      "\n",
      "positive probability: 98.96%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: the food and ambiance could make this my #1 go-to nice dinner spot.\n",
      "- positive: i ended up using my friend's recommendation of first class auto service where she always takes her car nearby and they were fantastic!\n",
      "- positive: after mentioning the problem to our server, he did the right thing and gave us the meal free.\n",
      "- positive: he was extra nice to us afterwards and he ended up not charging us for leftovers.\n",
      "- positive: the only name i got was at pick up and nicole did an excellent job quickly and efficiently.\n",
      "\n",
      "review: still, the service, and having dessert and coffee on the terrace afterwards, with a front row view of the fabulous fountain show more than made up for it.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.04%\n",
      "\n",
      "neutral probability: 0.01%\n",
      "\n",
      "positive probability: 99.95%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: you will leave with your stomach full and not spend a lot.\n",
      "- positive: been here many times, food ok and most importantly its close to where we live, our fav stuff is downtown and in the avenues.\n",
      "- positive: i ended up going to robert cromea inside mandalay bay and butch did a great job cutting my hair he saved whatever was left of it\n",
      "- positive: i'm pretty certain he is more knowledgable about my diabetes than my internist.\n",
      "- positive: clubs are fun but if you want to talk to your friends and be welcomed then this is your place.\n",
      "\n",
      "review: if you want a true honest boat mechanic i strongly suggest coming to gerry's marine...\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.08%\n",
      "\n",
      "neutral probability: 0.04%\n",
      "\n",
      "positive probability: 99.88%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: the food was absolutely delicious and the service was impeccable!\n",
      "- negative: i was really disappointed with the quality of the product; it broke after one use.\n",
      "- neutral: the event was held on a saturday, and it started at 3 pm.\n",
      "- positive: the staff were friendly and made us feel welcome throughout our stay.\n",
      "- negative: i had to wait for over an hour, and the staff were unhelpful.\n",
      "\n",
      "review: the atmosphere was nice, but the service was slow and the food was just average.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 15.00%\n",
      "\n",
      "neutral probability: 70.00%\n",
      "\n",
      "positive probability: 15.00%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: i'll come back for the beer though! it was good.\n",
      "- positive: most likely between 3 and 4 stars. it was a great place.\n",
      "- positive: the one nice thing i will say is the house keeping and maitence staff are really nice.\n",
      "- positive: asked about oysters - we tried one after 10 minutes ordered a dozen and they were excellent.\n",
      "- positive: the donuts were truly about the best part of the entire meal.\n",
      "\n",
      "review: my boyfriend had sausage and omelet with chicken that was on par with something that you would get at a hobees (california breakfast chain). it was great.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.08%\n",
      "\n",
      "neutral probability: 0.00%\n",
      "\n",
      "positive probability: 99.92%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: the ambiance was delightful, and the service was impeccable.\n",
      "- negative: i was extremely disappointed with the quality of the food; it was cold and bland.\n",
      "- neutral: the restaurant was okay, nothing special but not terrible either.\n",
      "- positive: the dessert was absolutely divine, a perfect end to the meal.\n",
      "- negative: i will never return; the staff was rude and unhelpful.\n",
      "\n",
      "review: the service was slow, and the food was just average.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 12.67%\n",
      "\n",
      "neutral probability: 75.34%\n",
      "\n",
      "positive probability: 12.99%\n",
      "\n",
      "classification: neutral      1\n",
      "- neutral: my wife and i were visiting the area and stopped in, deciding to purchase a porcelain tea set from them.\n",
      "- positive: remembering the last time i had their pizza, i decided to relive the experience.\n",
      "- neutral: on my journey to find hardwood floors, the peoria lumber liquidators was my first stop.\n",
      "- neutral: they arranged to have a pole position shuttle pick us up from the hotel & drop us back.\n",
      "- neutral: i had been to their flower shop 4 months before my wedding, met with the owner and her assistant to go over all our arrangements for the big day.\n",
      "\n",
      "review: i printed the ad and drove across phoenix to test drive and purchase the car from this dealer.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 0.02%\n",
      "\n",
      "neutral probability: 87.73%\n",
      "\n",
      "positive probability: 12.26%\n",
      "\n",
      "classification: neutral      1\n",
      "examples:\n",
      "- positive: our bill came out to around $27 and we ate like the wealthy.\n",
      "- positive: i could have eaten that entire foie terrine myself and what i couldn't eat i would have rubbed all over my body.\n",
      "- positive: i had a cavity filled and to my surprise dr. nelson was able to get me completely numb.\n",
      "- positive: thank you mad greens you got me for life.\n",
      "- positive: he knew we were in a hurry and he was literally running back and forth to make arrangements.\n",
      "\n",
      "review: while i wouldn't really call it cuban, it was one mean sandwich.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.25%\n",
      "\n",
      "neutral probability: 0.11%\n",
      "\n",
      "positive probability: 99.63%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: the food was absolutely delicious and the service was impeccable!\n",
      "- negative: i waited for over an hour and my order was still wrong when it arrived.\n",
      "- neutral: the restaurant was okay, nothing special but not terrible either.\n",
      "- positive: the ambiance was lovely and the staff were very friendly.\n",
      "- negative: i was really disappointed with the quality of the product; it broke after one use.\n",
      "\n",
      "review: the atmosphere was nice, but the food was just average.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 12.45%\n",
      "\n",
      "neutral probability: 65.32%\n",
      "\n",
      "positive probability: 22.23%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: what better message than ` love thyself ' could young women of any size receive ?\n",
      "- positive: after he looked at them i got double what i thought i was going to get.\n",
      "- positive: does this place live up to hype? does it ever!\n",
      "- positive: they brought out peppers in hot oil with the bread, they know me too well.\n",
      "- positive: i took an appointment, missed it but they still took my car in.\n",
      "\n",
      "review: he makes it to where you kinda forget your getting your teeth drilled out.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.45%\n",
      "\n",
      "neutral probability: 3.77%\n",
      "\n",
      "positive probability: 95.78%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: the ambiance was delightful and the service was impeccable.\n",
      "- negative: i waited for over an hour for my food, and when it finally arrived, it was cold.\n",
      "- neutral: the restaurant has a variety of options on the menu, but i didn't find anything particularly exciting.\n",
      "- positive: the dessert was absolutely divine, and i can't wait to come back for more!\n",
      "- negative: the staff was rude and unhelpful, which ruined my dining experience.\n",
      "\n",
      "review: after puzzling through their menu of mouth-watering items (whole platter of burnt ends?\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.24%\n",
      "\n",
      "neutral probability: 17.66%\n",
      "\n",
      "positive probability: 82.10%\n",
      "\n",
      "classification: positive      1\n",
      "- negative: once you start going there, they expect you to use them for life.\n",
      "- negative: i was excited about the snow in the way someone is excited about getting a root canal.\n",
      "- negative: to call the other side of heaven `` appalling '' would be to underestimate just how dangerous entertainments like it can be.\n",
      "- positive: i would stick to caramel for my next visit...which will probably will on taco tuesday $8 for 3 tacos!!!\n",
      "- negative: gee, how generous that they give you one small pack of cream cheese now instead of putting it in the bagel.\n",
      "\n",
      "review: i swear i took 3 eating breaks and still couldn't finish it (and for those who don't know me, i'm known to chow down). that is really weird\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 27.86%\n",
      "\n",
      "neutral probability: 35.60%\n",
      "\n",
      "positive probability: 36.53%\n",
      "\n",
      "classification: positive      1\n",
      "- neutral: the weather was okay, not too hot or too cold.\n",
      "- negative: i was really disappointed with the service at the restaurant.\n",
      "- positive: the concert was absolutely amazing and i had a great time!\n",
      "- neutral: the book was just average, nothing special about it.\n",
      "- positive: i love my new phone; it works perfectly and has a great camera.\n",
      "\n",
      "review: i can't believe how bad the food was at that place.\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 85.34%\n",
      "\n",
      "neutral probability: 10.12%\n",
      "\n",
      "positive probability: 4.54%\n",
      "\n",
      "classification: negative      1\n",
      "- positive: i was very excited to come get my nails done and since i moved to the northwest side i had not been here.\n",
      "- positive: came here because we bought a groupon, excited about the location and the outdoor feel.\n",
      "- positive: we came here because the reviews were good, and it's conveniently located in our hotel.\n",
      "- positive: my friend and i are in from out of town and walked in, excited to try some ice cream.\n",
      "- positive: we liked the reviews it had and all the hype it had about how good the food is so we decided to give this place a try.\n",
      "\n",
      "review: it was our honeymoon trip this week and we had a previous good experience at flamingo so decided to give it another shot.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.04%\n",
      "\n",
      "neutral probability: 11.95%\n",
      "\n",
      "positive probability: 88.00%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: i will offer the that complimentary french fries are good.\n",
      "- positive: the one thing better than their food is their customer service.\n",
      "- positive: we were on the hunt in minneapolis for the best biscuits & gravy and this place would have won that competition. we were glad to come close.\n",
      "- positive: prices are reasonable until you try the food and realize you would pay whatever it takes to try this place again.\n",
      "- positive: the donuts were truly about the best part of the entire meal.\n",
      "\n",
      "review: we got the banana egg rolls for dessert too and that was beyond what we expected.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.27%\n",
      "\n",
      "neutral probability: 0.01%\n",
      "\n",
      "positive probability: 99.73%\n",
      "\n",
      "classification: positive      1\n",
      "- negative: u would recommend this place and certainly go back if i i want to clean out my colon.\n",
      "- positive: and i'm kicking myself for missing out on a year of good eats!\n",
      "- positive: if his manager reads this, give this guy a raise.....or your job lol.\n",
      "- positive: somehow they make hot dogs taste better than eating a good steak.\n",
      "- positive: we made an extra donation as it seemed way too cheap for the pleasure it afforded.\n",
      "\n",
      "review: will definitely be going back to blo when pigs fly!\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 85.34%\n",
      "\n",
      "neutral probability: 5.12%\n",
      "\n",
      "positive probability: 9.54%\n",
      "\n",
      "classification: negative      1\n",
      "- positive: the food was absolutely delicious and the service was excellent.\n",
      "- negative: i was really disappointed with the quality of the meal and the rude staff.\n",
      "- positive: this place has a great atmosphere and the drinks are fantastic.\n",
      "- neutral: the restaurant is located downtown and has a decent selection of dishes.\n",
      "- negative: i will never come back here again after my last experience.\n",
      "\n",
      "review: the ambiance was nice, but the food was just average.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 15.30%\n",
      "\n",
      "neutral probability: 50.12%\n",
      "\n",
      "positive probability: 34.58%\n",
      "\n",
      "classification: neutral      1\n",
      "- neutral: the weather was okay, not too hot or too cold.\n",
      "- negative: the service was terrible and the food was cold.\n",
      "- positive: i absolutely loved the ambiance and the staff were very friendly.\n",
      "- negative: i was disappointed with the quality of the product; it broke after one use.\n",
      "- positive: this is the best movie i've seen all year!\n",
      "\n",
      "review: it's simple really, put someone in charge, wait should never be more than 5 minutes, check your inventory and make sure you have spoons :)\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 95.73%\n",
      "\n",
      "neutral probability: 4.00%\n",
      "\n",
      "positive probability: 0.27%\n",
      "\n",
      "classification: negative      1\n",
      "- positive: the ambiance was lovely and the staff were incredibly friendly.\n",
      "- negative: i was disappointed with the service; it took too long to get our food.\n",
      "- neutral: the restaurant was average, nothing stood out but nothing was terrible either.\n",
      "- positive: the dessert was absolutely delicious and worth every penny!\n",
      "- negative: i found the meal to be bland and not worth the price.\n",
      "\n",
      "review: the service was slow, and the food was just okay.\n",
      "\n",
      "classifier decision: negative\n",
      "\n",
      "negative probability: 85.00%\n",
      "\n",
      "neutral probability: 10.00%\n",
      "\n",
      "positive probability: 5.00%\n",
      "\n",
      "classification: negative      1\n",
      "- positive: although german cooking does not come readily to mind when considering the world 's best cuisine, mostly martha could make deutchland a popular destination for hungry tourists.\n",
      "- neutral: i plan to test the claim that it's the best sushi ever.\n",
      "- positive: the sign said absolutely no parking at any time in front so i was very worried that we would never find parking anywhere but then there was a nice parking lot right behind the store.\n",
      "- positive: generally, clockstoppers will fulfill your wildest fantasies about being a different kind of time traveler, while happily killing 94 minutes.\n",
      "- positive: the band's courage in the face of official repression is inspiring, especially for aging hippies -lrb- this one included -rrb-.\n",
      "\n",
      "review: when i'm on this block again, i know i should try one of the other stalls but damn, it'll be hard not to just return to t&t for the exact same order.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.88%\n",
      "\n",
      "neutral probability: 1.41%\n",
      "\n",
      "positive probability: 97.72%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: i took a taxi back to rio casino which is only a 5 min drive, when i got back i put the juices in the fridge of my room and then my friend comes up less than an hr later and tried the juice bc i was speaking highly of the juices.\n",
      "- positive: in brief, my husband and i came to this place for breakfast, in particular, for the ham that i always liked in there; ordered a couple of portions.\n",
      "- positive: our friends ordered scallops and the trout which the winning dish seemed to be the trout.\n",
      "- positive: i ordered the original hot dog, the fries and the beer, and after a few minutes, the food was ready.\n",
      "- neutral: despite this, i was able to make a follow up appointment instead for next sunday (despite what yelp says, the charleston office is actually open on sundays).\n",
      "\n",
      "review: you can get the food cheaper because they do take coupons. what is on your mind\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.13%\n",
      "\n",
      "neutral probability: 17.92%\n",
      "\n",
      "positive probability: 81.95%\n",
      "\n",
      "classification: positive      1\n",
      "- positive: my husband and i went about 5pm during the week. we loved this place.\n",
      "- positive: try akira or shinano, both are delicious and have gracious kind people who appreciate their customers.\n",
      "- positive: the fries were very thin but tasted good. i really recommend.\n",
      "- positive: we ended up sitting at the bar at mesa grill, which i like because sometimes you can meet interesting people at the bar. it was awesome.\n",
      "- positive: tony was lovely, and took care of an ingrown toenail, and my nails looked lovely when i left.\n",
      "\n",
      "review: i went to chill because i saw the good review on happycow and was very impressed! highly recommend\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.03%\n",
      "\n",
      "neutral probability: 0.00%\n",
      "\n",
      "positive probability: 99.96%\n",
      "\n",
      "classification: positive      1\n",
      "- neutral: the weather today is neither too hot nor too cold, just perfect for a walk.\n",
      "- negative: i was really disappointed with the service at the restaurant; the staff was rude and unhelpful.\n",
      "- positive: i absolutely loved the new movie! the storyline was captivating and the acting was superb.\n",
      "- neutral: i received my package on time, but the contents were not what i expected.\n",
      "- positive: the concert was amazing! the energy in the crowd was electric and the band played all my favorite songs.\n",
      "\n",
      "review: the presentation was okay, but i expected more from the speaker.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 15.30%\n",
      "\n",
      "neutral probability: 70.45%\n",
      "\n",
      "positive probability: 14.25%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: my photographer saved the day she did my daughter's make-up & mine.\n",
      "- positive: they think they are vegas with the pool party. they are better than vegas.\n",
      "- positive: we are moving significantly farther away in the not so distant future, yet will keep on coming here for dental consideration as long as we live close to phoenix!\n",
      "- positive: a lot less from what i was expected to pay (hey i'm not complaining) but regardless of it being a mistake or not, i'll happily return.\n",
      "- positive: on a scale of 1 to 10, i have to give the representative and service a 10.\n",
      "\n",
      "review: i would be there way too often and would probably give them all of my money.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.34%\n",
      "\n",
      "neutral probability: 0.03%\n",
      "\n",
      "positive probability: 99.63%\n",
      "\n",
      "classification: positive      1\n",
      "negative: the food was cold and the service was terrible, i will never come back here again.  \n",
      "positive: the ambiance was lovely and the staff were very attentive, i had a great time!  \n",
      "neutral: the restaurant was busy, and we had to wait a bit longer than expected for our table.  \n",
      "\n",
      "review: the atmosphere was nice, but the food was just average and the service was slow.  \n",
      "\n",
      "classifier decision: neutral  \n",
      "\n",
      "negative probability: 22.34%  \n",
      "\n",
      "neutral probability: 55.67%  \n",
      "\n",
      "positive probability: 22.99%  \n",
      "\n",
      "classification: neutral      1\n",
      "- positive: the service was excellent and the food was delicious.\n",
      "- negative: i was really disappointed with the quality of the meal.\n",
      "- neutral: the restaurant was okay, nothing special.\n",
      "- positive: this is the best coffee i've ever had!\n",
      "- negative: i will never come back to this place again.\n",
      "\n",
      "review: the atmosphere was nice, but the food was just average.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 15.00%\n",
      "\n",
      "neutral probability: 70.00%\n",
      "\n",
      "positive probability: 15.00%\n",
      "\n",
      "classification: neutral      1\n",
      "- neutral: the weather was neither too hot nor too cold today.\n",
      "- negative: the service was terrible and the food was cold.\n",
      "- positive: i absolutely loved the dessert; it was delicious!\n",
      "- neutral: i went to the store to buy some groceries.\n",
      "- positive: the movie was fantastic and i would watch it again!\n",
      "\n",
      "review: the restaurant was okay, but the wait was too long.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 15.67%\n",
      "\n",
      "neutral probability: 72.45%\n",
      "\n",
      "positive probability: 11.88%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: the service was excellent and the food was delicious.\n",
      "- negative: i waited for an hour and my order was still wrong.\n",
      "- neutral: the restaurant was average, nothing special but not bad either.\n",
      "- positive: the ambiance was lovely and the staff were very friendly.\n",
      "- negative: the meal was cold and lacked flavor.\n",
      "\n",
      "review: the food was decent, but the service was slow and unresponsive.\n",
      "\n",
      "classifier decision: neutral\n",
      "\n",
      "negative probability: 15.00%\n",
      "\n",
      "neutral probability: 70.00%\n",
      "\n",
      "positive probability: 15.00%\n",
      "\n",
      "classification: neutral      1\n",
      "- positive: i have been to benihana in san francisco and our chef put on a helluva show..so i was excited to find one in scottsdale.\n",
      "- positive: also, they have a pretty decent lunch and dinner menu but we didn't try any.\n",
      "- positive: and they're open all hours of the night so it's got that going for them.\n",
      "- positive: onsite carlos n charlies has good mexican food.\n",
      "- positive: in fact, last year i left my nook color and was happy when the hotel returned it to me.\n",
      "\n",
      "review: we did go to dickey's (7th street and greenway) and had some decent bar-be-cue.\n",
      "\n",
      "classifier decision: positive\n",
      "\n",
      "negative probability: 0.04%\n",
      "\n",
      "neutral probability: 0.07%\n",
      "\n",
      "positive probability: 99.88%\n",
      "\n",
      "classification: positive      1\n",
      "\n",
      "Found 82 invalid predictions at row indices:\n",
      "[206, 226, 347, 495, 508, 554, 573, 587, 647, 730, 810, 960, 990, 994, 1026, 1042, 1058, 1105, 1249, 1482, 1525, 1567, 1577, 1618, 1888, 1896, 1933, 2173, 2302, 2310, 2392, 2461, 2518, 2631, 2699, 2914, 3070, 3101, 3125, 3127, 3135, 3333, 3418, 3440, 3472, 3512, 3639, 3690, 3696, 3894, 4088, 4123, 4283, 4326, 4407, 4528, 4653, 4669, 4695, 4729, 4887, 4984, 5014, 5065, 5081, 5102, 5189, 5424, 5440, 5478, 5485, 5556, 5718, 5751, 5851, 5938, 5965, 5998, 6030, 6086, 6233, 6456]\n",
      "\n",
      "Invalid prediction rows:\n",
      "                                                                                                                                                        sentence  \\\n",
      "206                                                               I printed the ad and drove across phoenix to test drive and purchase the car from this dealer.   \n",
      "226                                                                       I wish I could learn how to make the guacamole that the salmon entrÃ©e got served with.   \n",
      "347   My boyfriend had sausage and omelet with chicken that was on par with something that you would get at a Hobees (California breakfast chain). It was great.   \n",
      "495                       Prices were ok the pictures online looks great so I booked.We arrived and were excited, the lobby was full of people and it look good.   \n",
      "508                                                                       If you want a true honest boat mechanic I strongly suggest coming to Gerry's Marine...   \n",
      "...                                                                                                                                                          ...   \n",
      "5998                                                                                                       And if The Hours wins ` Best Picture ' I just might .   \n",
      "6030                                                                    They have many coupons available on the app and in store, just ask about them. will you?   \n",
      "6086                                                                                Been here twice in the past week, once today for brunch and once for dinner.   \n",
      "6233                                                                       (Eventually a delicious red sangria was selected, which went down entirely too fast.)   \n",
      "6456                                                                             We did go to Dickey's (7th street and Greenway) and had some decent bar-be-cue.   \n",
      "\n",
      "         label       source split  \n",
      "206    neutral  dynasent_r1  test  \n",
      "226   positive  dynasent_r2  test  \n",
      "347   positive  dynasent_r2  test  \n",
      "495   positive  dynasent_r1  test  \n",
      "508   positive  dynasent_r1  test  \n",
      "...        ...          ...   ...  \n",
      "5998  positive    sst_local  test  \n",
      "6030   neutral  dynasent_r2  test  \n",
      "6086   neutral  dynasent_r2  test  \n",
      "6233  positive  dynasent_r1  test  \n",
      "6456  positive  dynasent_r1  test  \n",
      "\n",
      "[82 rows x 4 columns]\n",
      "\n",
      "WARNING: Invalid predictions found. Please review the above details and re-run problematic cases.\n"
     ]
    }
   ],
   "source": [
    "e18_g4om_elft_ex_pr2_df, e18_g4om_elft_ex_pr2_metrics = evaluate_experiment(\n",
    "    name='E18-G4OM-ELFT-EX-PR2',\n",
    "    notes='Experiment 18: Evaluate prompt-based model collaboration with prediction, probabilities, and similar examples',\n",
    "    lm='gpt-4o-mini-2024-07-18',\n",
    "    instance='electra_large_gpt_sentiment_pred_probs_examples',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "id": "cfbba151",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_indices = [206, 226, 347, 495, 508, 554, 573, 587, 647, 730, 810, 960, 990, 994, 1026, 1042, 1058, 1105, 1249, 1482, 1525, 1567, 1577, 1618, 1888, 1896, 1933, 2173, 2302, 2310, 2392, 2461, 2518, 2631, 2699, 2914, 3070, 3101, 3125, 3127, 3135, 3333, 3418, 3440, 3472, 3512, 3639, 3690, 3696, 3894, 4088, 4123, 4283, 4326, 4407, 4528, 4653, 4669, 4695, 4729, 4887, 4984, 5014, 5065, 5081, 5102, 5189, 5424, 5440, 5478, 5485, 5556, 5718, 5751, 5851, 5938, 5965, 5998, 6030, 6086, 6233, 6456]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "id": "e0a8c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "e18_g4om_elft_ex_pr2_df_orig = pd.DataFrame.copy(e18_g4om_elft_ex_pr2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "e15e2006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the invalid rows\n",
    "e18_g4om_elft_ex_pr2_df.loc[invalid_indices, 'prediction'] = e18_g4om_elft_ex_pr2_df.loc[invalid_indices].apply(update_prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "4b9705c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction\n",
       "negative                                                                                                                  2424\n",
       "positive                                                                                                                  2278\n",
       "neutral                                                                                                                   1825\n",
       "negative: i was really disappointed with the service at this restaurant; the food was cold and took forever to arrive.       1\n",
       "negative: i was extremely disappointed with the service i received; it was slow and unprofessional.                          1\n",
       "negative: the pork chop is their signature dish and somehow it tastes different every time you try.                          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 837,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique values for prediction\n",
    "e18_g4om_elft_ex_pr2_df['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "id": "ba0d788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows that start with 'negative:' in the 'prediction' column\n",
    "negative_indices = e18_g4om_elft_ex_pr2_df[e18_g4om_elft_ex_pr2_df['prediction'].str.startswith('negative:')].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "id": "ea3b909f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([1058, 4887, 5014], dtype='int64')"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "id": "fa552ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>classification</th>\n",
       "      <th>prediction</th>\n",
       "      <th>match</th>\n",
       "      <th>classifier_decision</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>At only 70K I had to get a new transmission for my 2007 VW Jetta Wolfsburg edition.</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative: i was really disappointed with the service at this restaurant; the food was cold and took forever to arrive.</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4887</th>\n",
       "      <td>I booked the appointment for 12 sharp and she arrived at 1:15.</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative: i was extremely disappointed with the service i received; it was slow and unprofessional.</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dynasent_r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5014</th>\n",
       "      <td>The pork chop is their signature dish and somehow it tastes different every time you try.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative: the pork chop is their signature dish and somehow it tastes different every time you try.</td>\n",
       "      <td>False</td>\n",
       "      <td>positive</td>\n",
       "      <td>dynasent_r2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         review  \\\n",
       "1058        At only 70K I had to get a new transmission for my 2007 VW Jetta Wolfsburg edition.   \n",
       "4887                             I booked the appointment for 12 sharp and she arrived at 1:15.   \n",
       "5014  The pork chop is their signature dish and somehow it tastes different every time you try.   \n",
       "\n",
       "     classification  \\\n",
       "1058       negative   \n",
       "4887       negative   \n",
       "5014        neutral   \n",
       "\n",
       "                                                                                                                  prediction  \\\n",
       "1058  negative: i was really disappointed with the service at this restaurant; the food was cold and took forever to arrive.   \n",
       "4887                     negative: i was extremely disappointed with the service i received; it was slow and unprofessional.   \n",
       "5014                     negative: the pork chop is their signature dish and somehow it tastes different every time you try.   \n",
       "\n",
       "      match classifier_decision       source  \n",
       "1058  False             neutral  dynasent_r1  \n",
       "4887  False             neutral  dynasent_r1  \n",
       "5014  False            positive  dynasent_r2  "
      ]
     },
     "execution_count": 840,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the rows with negative indices\n",
    "e18_g4om_elft_ex_pr2_df.loc[negative_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "id": "0c0fe4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "e18_1058_redo = electra_large_gpt_sentiment_pred_probs_examples(review='At only 70K I had to get a new transmission for my 2007 VW Jetta Wolfsburg edition. (Please only respond with one word for the classification)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "id": "ec901f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    examples=[{'index': 2287, 'similarity_score': 0.9982556700706482, 'review': 'So in March (30th) of this year we had Maricopa AC come service 3 AC units (2 personal and a rental property).', 'classification': 'neutral'}, {'index': 2956, 'similarity_score': 0.9982511401176453, 'review': 'I need to mention that i booked my two night stay on Groupon and i did not require me to state how many people were staying in the room.', 'classification': 'neutral'}, {'index': 256, 'similarity_score': 0.9981163740158081, 'review': 'My home warranty co.  sent them to me for AC repair.', 'classification': 'neutral'}, {'index': 2807, 'similarity_score': 0.9980205297470093, 'review': 'My hubby and I got a late start to our morning but were still looking for a place with breakfast on their menu.', 'classification': 'neutral'}, {'index': 1957, 'similarity_score': 0.9979831576347351, 'review': 'We have had questions in the past, and have dropped in at both the Biltmore and Chandler Apple stores.', 'classification': 'neutral'}],\n",
       "    classification='negative',\n",
       "    classifier_decision='neutral',\n",
       "    negative_probability='0.09%',\n",
       "    neutral_probability='98.37%',\n",
       "    positive_probability='1.54%'\n",
       ")"
      ]
     },
     "execution_count": 844,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e18_1058_redo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "id": "77df1532",
   "metadata": {},
   "outputs": [],
   "source": [
    "e18_4887_redo = electra_large_gpt_sentiment_pred_probs_examples(review='I booked the appointment for 12 sharp and she arrived at 1:15. (Please only respond with one word for the classification)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "id": "95b7144e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    examples=[{'index': 2909, 'similarity_score': 0.9961537718772888, 'review': \"the bartender came to us after 3 minutes, gave us menus, and pointed out which drinks we couldn't order (due to availability)\", 'classification': 'neutral'}, {'index': 264, 'similarity_score': 0.9939299821853638, 'review': 'so loudly that the other patrons in the store flocked to the counter to find out what I ate.', 'classification': 'positive'}, {'index': 2754, 'similarity_score': 0.9930704832077026, 'review': 'Before our meals arrived, the manager came to our table and told us \\nthat the mixed vegetables we ordered would have to be substituted,\\nand would we be OK with salads?', 'classification': 'neutral'}, {'index': 835, 'similarity_score': 0.9929916858673096, 'review': 'The Iditarod lasts for days - this just felt like it did .', 'classification': 'negative'}, {'index': 289, 'similarity_score': 0.9927797317504883, 'review': \"You wo n't like Roger , but you will quickly recognize him .\", 'classification': 'negative'}],\n",
       "    classification='negative',\n",
       "    classifier_decision='neutral',\n",
       "    negative_probability='18.40%',\n",
       "    neutral_probability='75.73%',\n",
       "    positive_probability='5.87%'\n",
       ")"
      ]
     },
     "execution_count": 846,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e18_4887_redo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "id": "35a57b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "e18_5014_redo = electra_large_gpt_sentiment_pred_probs_examples(review='The pork chop is their signature dish and somehow it tastes different every time you try. (Please only respond with one word for the classification)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "id": "db15a570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    examples=[{'index': 4595, 'similarity_score': 0.9967144727706909, 'review': 'I used to come here often, every time I needed to go grocery shopping I went to this store.', 'classification': 'neutral'}, {'index': 4365, 'similarity_score': 0.9966409802436829, 'review': 'I took it to UBreakIFix and they knew right away what was wrong, the power button needed to be replaced.', 'classification': 'positive'}, {'index': 741, 'similarity_score': 0.9963210821151733, 'review': 'Replace the Oil Control Valve and sent me on my way.', 'classification': 'neutral'}, {'index': 304, 'similarity_score': 0.9963037967681885, 'review': 'He just fixed my painted skin and sent me on my way.', 'classification': 'neutral'}, {'index': 705, 'similarity_score': 0.996152937412262, 'review': 'You could return your rental vehicle a day before to save some money and they will take you back.', 'classification': 'positive'}],\n",
       "    classification='neutral',\n",
       "    classifier_decision='neutral',\n",
       "    negative_probability='0.34%',\n",
       "    neutral_probability='53.42%',\n",
       "    positive_probability='46.24%'\n",
       ")"
      ]
     },
     "execution_count": 848,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e18_5014_redo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "id": "a1dd75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "e18_5014_redo.classification\n",
    "# Update the prediction for the review\n",
    "e18_g4om_elft_ex_pr2_df.at[5014, 'prediction'] = e18_5014_redo.classification\n",
    "e18_g4om_elft_ex_pr2_df.at[4887, 'prediction'] = e18_4887_redo.classification\n",
    "e18_g4om_elft_ex_pr2_df.at[1058, 'prediction'] = e18_1058_redo.classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "id": "c65cff85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction\n",
       "negative    2426\n",
       "positive    2278\n",
       "neutral     1826\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique values for prediction\n",
    "e18_g4om_elft_ex_pr2_df['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "id": "5175c1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E18-G4OM-ELFT-EX-PR2\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-11-03 06:05:57\n",
      "Notes: Experiment 18: Evaluate prompt-based model collaboration with prediction, probabilities, and similar examples\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Instance: electra_large_gpt_sentiment_pred_probs_examples\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Results shape: [6530, 6]\n",
      "Save directory: research\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2426\n",
      "positive               2278\n",
      "neutral                1826\n",
      "\n",
      "E18-G4OM-ELFT-EX-PR2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8483    0.8750    0.8614      2352\n",
      "     neutral     0.7503    0.7490    0.7497      1829\n",
      "    positive     0.8898    0.8629    0.8762      2349\n",
      "\n",
      "    accuracy                         0.8354      6530\n",
      "   macro avg     0.8295    0.8290    0.8291      6530\n",
      "weighted avg     0.8358    0.8354    0.8354      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2058      241        53\n",
      "neutral         261     1370       198\n",
      "positive        107      215      2027\n",
      "\n",
      "\n",
      "E18-G4OM-ELFT-EX-PR2-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9021    0.8525    0.8766      1200\n",
      "     neutral     0.7782    0.9092    0.8386      1200\n",
      "    positive     0.9248    0.8200    0.8693      1200\n",
      "\n",
      "    accuracy                         0.8606      3600\n",
      "   macro avg     0.8684    0.8606    0.8615      3600\n",
      "weighted avg     0.8684    0.8606    0.8615      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1023      149        28\n",
      "neutral          57     1091        52\n",
      "positive         54      162       984\n",
      "\n",
      "\n",
      "E18-G4OM-ELFT-EX-PR2-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7621    0.9208    0.8340       240\n",
      "     neutral     0.8404    0.6583    0.7383       240\n",
      "    positive     0.7851    0.7917    0.7884       240\n",
      "\n",
      "    accuracy                         0.7903       720\n",
      "   macro avg     0.7959    0.7903    0.7869       720\n",
      "weighted avg     0.7959    0.7903    0.7869       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        221       12         7\n",
      "neutral          37      158        45\n",
      "positive         32       18       190\n",
      "\n",
      "\n",
      "E18-G4OM-ELFT-EX-PR2-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8124    0.8925    0.8506       912\n",
      "     neutral     0.5127    0.3111    0.3872       389\n",
      "    positive     0.8776    0.9384    0.9070       909\n",
      "\n",
      "    accuracy                         0.8090      2210\n",
      "   macro avg     0.7342    0.7140    0.7149      2210\n",
      "weighted avg     0.7864    0.8090    0.7922      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        814       80        18\n",
      "neutral         167      121       101\n",
      "positive         21       35       853\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    82.91\t   83.54\n",
      "DynaSent R1 \t    86.15\t   86.06\n",
      "DynaSent R2 \t    78.69\t   79.03\n",
      "SST-3       \t    71.49\t   80.90\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-11-03 06:05:57\n",
      "Duration: 0:00:00.464089\n"
     ]
    }
   ],
   "source": [
    "e18_g4om_elft_ex_pr2_metrics = evaluate_experiment(\n",
    "    name='E18-G4OM-ELFT-EX-PR2',\n",
    "    notes='Experiment 18: Evaluate prompt-based model collaboration with prediction, probabilities, and similar examples',\n",
    "    lm='gpt-4o-mini-2024-07-18',\n",
    "    instance='electra_large_gpt_sentiment_pred_probs_examples',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex,\n",
    "    results=e18_g4om_elft_ex_pr2_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e77bb",
   "metadata": {},
   "source": [
    "### E19-G4OM-ELFT-BEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "id": "7224a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-mini-2024-07-18', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "id": "ce8f269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "e19_g4om_elft_bex_result = electra_large_gpt_sentiment_examples_balanced(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "id": "43bf2b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    examples=[{'index': 57, 'similarity_score': 0.9977191090583801, 'review': 'This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.', 'classification': 'negative'}, {'index': 3410, 'similarity_score': 0.9972444176673889, 'review': 'This was my second time here and there will not be a third time.', 'classification': 'negative'}, {'index': 4792, 'similarity_score': 0.9952158331871033, 'review': 'This is brand spankin new so a longer soft opening is definitely necessary especially when dealing with us downtown folks during the lunch rush and after work dinner pickup, they need work.', 'classification': 'neutral'}, {'index': 2243, 'similarity_score': 0.9945549368858337, 'review': 'Nasty , ugly , pointless and depressing , even if you hate clowns .', 'classification': 'neutral'}, {'index': 2235, 'similarity_score': 0.9956791996955872, 'review': \"But if i had the choice, I'd go anywhere but here!\", 'classification': 'positive'}, {'index': 474, 'similarity_score': 0.9929326772689819, 'review': 'The colors and tech were something to be desired.', 'classification': 'positive'}],\n",
       "    classification='negative',\n",
       "    classifier_decision='negative',\n",
       "    probabilities=[0.9984808564186096, 0.0003502856707200408, 0.0011688611702993512]\n",
       ")"
      ]
     },
     "execution_count": 912,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e19_g4om_elft_bex_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "id": "a6398505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Examples: A list of examples that demonstrate different sentiment classes.\n",
      "\n",
      "Review: The review text to classify.\n",
      "\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Examples:\n",
      "- negative: This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.\n",
      "- negative: This was my second time here and there will not be a third time.\n",
      "- neutral: This is brand spankin new so a longer soft opening is definitely necessary especially when dealing with us downtown folks during the lunch rush and after work dinner pickup, they need work.\n",
      "- neutral: Nasty , ugly , pointless and depressing , even if you hate clowns .\n",
      "- positive: But if i had the choice, I'd go anywhere but here!\n",
      "- positive: The colors and tech were something to be desired.\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "\n",
      "Classifier Decision: negative\n",
      "\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nExamples: A list of examples that demonstrate different sentiment classes.\\n\\nReview: The review text to classify.\\n\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\n\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nExamples:\\n- negative: This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.\\n- negative: This was my second time here and there will not be a third time.\\n- neutral: This is brand spankin new so a longer soft opening is definitely necessary especially when dealing with us downtown folks during the lunch rush and after work dinner pickup, they need work.\\n- neutral: Nasty , ugly , pointless and depressing , even if you hate clowns .\\n- positive: But if i had the choice, I'd go anywhere but here!\\n- positive: The colors and tech were something to be desired.\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\n\\nClassifier Decision: negative\\n\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 913,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "id": "5d4137e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E19-G4OM-ELFT-BEX\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-11-03 07:40:07\n",
      "Notes: Experiment 19: Evaluate prompt-based model collaboration that includes similar, balanced examples\n",
      "Model: gpt-4o-mini-2024-07-18\n",
      "Instance: electra_large_gpt_sentiment_examples_balanced\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 5438 / 6530  (83.3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [1:18:55<00:00,  1.38it/s]\n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "classifier_decision    6530 values\n",
      "probabilities          6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2456\n",
      "positive               2191\n",
      "neutral                1883\n",
      "\n",
      "E19-G4OM-ELFT-BEX Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8408    0.8780    0.8590      2352\n",
      "     neutral     0.7323    0.7540    0.7430      1829\n",
      "    positive     0.9101    0.8489    0.8784      2349\n",
      "\n",
      "    accuracy                         0.8328      6530\n",
      "   macro avg     0.8277    0.8269    0.8268      6530\n",
      "weighted avg     0.8353    0.8328    0.8335      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2065      259        28\n",
      "neutral         281     1379       169\n",
      "positive        110      245      1994\n",
      "\n",
      "\n",
      "E19-G4OM-ELFT-BEX-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9017    0.8483    0.8742      1200\n",
      "     neutral     0.7608    0.9117    0.8294      1200\n",
      "    positive     0.9419    0.8108    0.8715      1200\n",
      "\n",
      "    accuracy                         0.8569      3600\n",
      "   macro avg     0.8681    0.8569    0.8584      3600\n",
      "weighted avg     0.8681    0.8569    0.8584      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1018      168        14\n",
      "neutral          60     1094        46\n",
      "positive         51      176       973\n",
      "\n",
      "\n",
      "E19-G4OM-ELFT-BEX-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7567    0.9458    0.8407       240\n",
      "     neutral     0.8449    0.6583    0.7400       240\n",
      "    positive     0.8197    0.7958    0.8076       240\n",
      "\n",
      "    accuracy                         0.8000       720\n",
      "   macro avg     0.8071    0.8000    0.7961       720\n",
      "weighted avg     0.8071    0.8000    0.7961       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        227        9         4\n",
      "neutral          44      158        38\n",
      "positive         29       20       191\n",
      "\n",
      "\n",
      "E19-G4OM-ELFT-BEX-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7984    0.8991    0.8458       912\n",
      "     neutral     0.4922    0.3265    0.3926       389\n",
      "    positive     0.8973    0.9131    0.9051       909\n",
      "\n",
      "    accuracy                         0.8041      2210\n",
      "   macro avg     0.7293    0.7129    0.7145      2210\n",
      "weighted avg     0.7852    0.8041    0.7904      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        820       82        10\n",
      "neutral         177      127        85\n",
      "positive         30       49       830\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    82.68\t   83.28\n",
      "DynaSent R1 \t    85.84\t   85.69\n",
      "DynaSent R2 \t    79.61\t   80.00\n",
      "SST-3       \t    71.45\t   80.41\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-11-03 08:59:04\n",
      "Duration: 1:18:56.551522\n"
     ]
    }
   ],
   "source": [
    "e19_g4om_elft_bex_df, e19_g4om_elft_bex_metrics = evaluate_experiment(\n",
    "    name='E19-G4OM-ELFT-BEX',\n",
    "    notes='Experiment 19: Evaluate prompt-based model collaboration that includes similar, balanced examples',\n",
    "    lm='gpt-4o-mini-2024-07-18',\n",
    "    instance='electra_large_gpt_sentiment_examples_balanced',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d412811d",
   "metadata": {},
   "source": [
    "### E20-G4O-ELFT-EX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "id": "f2b7137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-2024-08-06', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "78742792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "e20_g4o_elft_ex_result = electra_large_gpt_sentiment_examples(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "id": "6fa413ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    examples=[{'index': 57, 'similarity_score': 0.9977191090583801, 'review': 'This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.', 'classification': 'negative'}, {'index': 3410, 'similarity_score': 0.9972444176673889, 'review': 'This was my second time here and there will not be a third time.', 'classification': 'negative'}, {'index': 3476, 'similarity_score': 0.9971747398376465, 'review': 'Portioning of the Octopus could have been better.', 'classification': 'negative'}, {'index': 3679, 'similarity_score': 0.9971185922622681, 'review': 'My only complaint was the bill for a long distance phone call - $4 for a 3 minutes to Seattle.', 'classification': 'negative'}, {'index': 3160, 'similarity_score': 0.9971152544021606, 'review': 'I received big-time attitude from multiple \"fabulous\" employees when I pulled up at 7:29 a.m. to get my car washed.', 'classification': 'negative'}],\n",
       "    classification='negative',\n",
       "    classifier_decision='negative',\n",
       "    probabilities=[0.9984808564186096, 0.0003502856707200408, 0.0011688611702993512]\n",
       ")"
      ]
     },
     "execution_count": 917,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e20_g4o_elft_ex_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "id": "d29f2748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Examples: A list of examples that demonstrate different sentiment classes.\n",
      "\n",
      "Review: The review text to classify.\n",
      "\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Examples:\n",
      "- negative: This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.\n",
      "- negative: This was my second time here and there will not be a third time.\n",
      "- negative: Portioning of the Octopus could have been better.\n",
      "- negative: My only complaint was the bill for a long distance phone call - $4 for a 3 minutes to Seattle.\n",
      "- negative: I received big-time attitude from multiple \"fabulous\" employees when I pulled up at 7:29 a.m. to get my car washed.\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "\n",
      "Classifier Decision: negative\n",
      "\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nClassify the sentiment of a review as either \\'negative\\', \\'neutral\\', or \\'positive\\'.\\n\\n---\\n\\nFollow the following format.\\n\\nExamples: A list of examples that demonstrate different sentiment classes.\\n\\nReview: The review text to classify.\\n\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\n\\nClassification: One word representing the sentiment classification: \\'negative\\', \\'neutral\\', or \\'positive\\' (do not repeat the field name, do not use \\'mixed\\')\\n\\n---\\n\\nExamples:\\n- negative: This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.\\n- negative: This was my second time here and there will not be a third time.\\n- negative: Portioning of the Octopus could have been better.\\n- negative: My only complaint was the bill for a long distance phone call - $4 for a 3 minutes to Seattle.\\n- negative: I received big-time attitude from multiple \"fabulous\" employees when I pulled up at 7:29 a.m. to get my car washed.\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\n\\nClassifier Decision: negative\\n\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 918,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "id": "da3ed3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E20-G4O-ELFT-EX\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-11-03 14:44:03\n",
      "Notes: Experiment 20: Evaluate prompt-based model collaboration that includes similar examples\n",
      "Model: gpt-4o-2024-08-06\n",
      "Instance: electra_large_gpt_sentiment_examples\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 5463 / 6530  (83.7): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [1:04:55<00:00,  1.68it/s]\n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "classifier_decision    6530 values\n",
      "probabilities          6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2369\n",
      "positive               2228\n",
      "neutral                1933\n",
      "\n",
      "E20-G4O-ELFT-EX Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8649    0.8712    0.8680      2352\n",
      "     neutral     0.7258    0.7671    0.7459      1829\n",
      "    positive     0.9026    0.8561    0.8787      2349\n",
      "\n",
      "    accuracy                         0.8366      6530\n",
      "   macro avg     0.8311    0.8315    0.8309      6530\n",
      "weighted avg     0.8395    0.8366    0.8377      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2049      264        39\n",
      "neutral         248     1403       178\n",
      "positive         72      266      2011\n",
      "\n",
      "\n",
      "E20-G4O-ELFT-EX-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9185    0.8542    0.8851      1200\n",
      "     neutral     0.7565    0.9192    0.8299      1200\n",
      "    positive     0.9386    0.8025    0.8652      1200\n",
      "\n",
      "    accuracy                         0.8586      3600\n",
      "   macro avg     0.8712    0.8586    0.8601      3600\n",
      "weighted avg     0.8712    0.8586    0.8601      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1025      158        17\n",
      "neutral          51     1103        46\n",
      "positive         40      197       963\n",
      "\n",
      "\n",
      "E20-G4O-ELFT-EX-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8097    0.9042    0.8543       240\n",
      "     neutral     0.8376    0.6875    0.7551       240\n",
      "    positive     0.8118    0.8625    0.8364       240\n",
      "\n",
      "    accuracy                         0.8181       720\n",
      "   macro avg     0.8197    0.8181    0.8153       720\n",
      "weighted avg     0.8197    0.8181    0.8153       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        217       14         9\n",
      "neutral          36      165        39\n",
      "positive         15       18       207\n",
      "\n",
      "\n",
      "E20-G4O-ELFT-EX-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8193    0.8849    0.8508       912\n",
      "     neutral     0.4856    0.3470    0.4048       389\n",
      "    positive     0.8881    0.9252    0.9062       909\n",
      "\n",
      "    accuracy                         0.8068      2210\n",
      "   macro avg     0.7310    0.7190    0.7206      2210\n",
      "weighted avg     0.7888    0.8068    0.7951      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        807       92        13\n",
      "neutral         161      135        93\n",
      "positive         17       51       841\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    83.09\t   83.66\n",
      "DynaSent R1 \t    86.01\t   85.86\n",
      "DynaSent R2 \t    81.53\t   81.81\n",
      "SST-3       \t    72.06\t   80.68\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-11-03 15:48:59\n",
      "Duration: 1:04:56.627542\n"
     ]
    }
   ],
   "source": [
    "e20_g4o_elft_ex_results_df, e20_g4o_elft_ex_metrics = evaluate_experiment(\n",
    "    name='E20-G4O-ELFT-EX',\n",
    "    notes='Experiment 20: Evaluate prompt-based model collaboration that includes similar examples',\n",
    "    lm='gpt-4o-2024-08-06',\n",
    "    instance='electra_large_gpt_sentiment_examples',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae731205",
   "metadata": {},
   "source": [
    "### E21-G4O-ELFT-BEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "e0233627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-2024-08-06', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "b029ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "e21_g4o_elft_bex_result = electra_large_gpt_sentiment_examples_balanced(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "id": "9027c83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    examples=[{'index': 57, 'similarity_score': 0.9977191090583801, 'review': 'This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.', 'classification': 'negative'}, {'index': 3410, 'similarity_score': 0.9972444176673889, 'review': 'This was my second time here and there will not be a third time.', 'classification': 'negative'}, {'index': 4792, 'similarity_score': 0.9952158331871033, 'review': 'This is brand spankin new so a longer soft opening is definitely necessary especially when dealing with us downtown folks during the lunch rush and after work dinner pickup, they need work.', 'classification': 'neutral'}, {'index': 2243, 'similarity_score': 0.9945549368858337, 'review': 'Nasty , ugly , pointless and depressing , even if you hate clowns .', 'classification': 'neutral'}, {'index': 2235, 'similarity_score': 0.9956791996955872, 'review': \"But if i had the choice, I'd go anywhere but here!\", 'classification': 'positive'}, {'index': 474, 'similarity_score': 0.9929326772689819, 'review': 'The colors and tech were something to be desired.', 'classification': 'positive'}],\n",
       "    classification='negative',\n",
       "    classifier_decision='negative',\n",
       "    probabilities=[0.9984808564186096, 0.0003502856707200408, 0.0011688611702993512]\n",
       ")"
      ]
     },
     "execution_count": 922,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e21_g4o_elft_bex_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "id": "baec1ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Examples: A list of examples that demonstrate different sentiment classes.\n",
      "\n",
      "Review: The review text to classify.\n",
      "\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Examples:\n",
      "- negative: This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.\n",
      "- negative: This was my second time here and there will not be a third time.\n",
      "- neutral: This is brand spankin new so a longer soft opening is definitely necessary especially when dealing with us downtown folks during the lunch rush and after work dinner pickup, they need work.\n",
      "- neutral: Nasty , ugly , pointless and depressing , even if you hate clowns .\n",
      "- positive: But if i had the choice, I'd go anywhere but here!\n",
      "- positive: The colors and tech were something to be desired.\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "\n",
      "Classifier Decision: negative\n",
      "\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nExamples: A list of examples that demonstrate different sentiment classes.\\n\\nReview: The review text to classify.\\n\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\n\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nExamples:\\n- negative: This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.\\n- negative: This was my second time here and there will not be a third time.\\n- neutral: This is brand spankin new so a longer soft opening is definitely necessary especially when dealing with us downtown folks during the lunch rush and after work dinner pickup, they need work.\\n- neutral: Nasty , ugly , pointless and depressing , even if you hate clowns .\\n- positive: But if i had the choice, I'd go anywhere but here!\\n- positive: The colors and tech were something to be desired.\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\n\\nClassifier Decision: negative\\n\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 923,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "id": "06f3108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E21-G4O-ELFT-BEX\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-11-03 16:20:33\n",
      "Notes: Experiment 21: Evaluate prompt-based model collaboration that includes similar, balanced examples\n",
      "Model: gpt-4o-2024-08-06\n",
      "Instance: electra_large_gpt_sentiment_examples_balanced\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 5456 / 6530  (83.6): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [1:03:04<00:00,  1.73it/s]\n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "classifier_decision    6530 values\n",
      "probabilities          6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2351\n",
      "positive               2229\n",
      "neutral                1950\n",
      "\n",
      "E21-G4O-ELFT-BEX Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8652    0.8648    0.8650      2352\n",
      "     neutral     0.7200    0.7676    0.7431      1829\n",
      "    positive     0.9053    0.8591    0.8816      2349\n",
      "\n",
      "    accuracy                         0.8355      6530\n",
      "   macro avg     0.8302    0.8305    0.8299      6530\n",
      "weighted avg     0.8390    0.8355    0.8368      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2034      283        35\n",
      "neutral         249     1404       176\n",
      "positive         68      263      2018\n",
      "\n",
      "\n",
      "E21-G4O-ELFT-BEX-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9175    0.8433    0.8789      1200\n",
      "     neutral     0.7519    0.9192    0.8271      1200\n",
      "    positive     0.9417    0.8083    0.8700      1200\n",
      "\n",
      "    accuracy                         0.8569      3600\n",
      "   macro avg     0.8704    0.8569    0.8587      3600\n",
      "weighted avg     0.8704    0.8569    0.8587      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1012      172        16\n",
      "neutral          53     1103        44\n",
      "positive         38      192       970\n",
      "\n",
      "\n",
      "E21-G4O-ELFT-BEX-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8052    0.8958    0.8481       240\n",
      "     neutral     0.7990    0.6958    0.7439       240\n",
      "    positive     0.8279    0.8417    0.8347       240\n",
      "\n",
      "    accuracy                         0.8111       720\n",
      "   macro avg     0.8107    0.8111    0.8089       720\n",
      "weighted avg     0.8107    0.8111    0.8089       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        215       20         5\n",
      "neutral          36      167        37\n",
      "positive         16       22       202\n",
      "\n",
      "\n",
      "E21-G4O-ELFT-BEX-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8226    0.8849    0.8526       912\n",
      "     neutral     0.4891    0.3445    0.4042       389\n",
      "    positive     0.8859    0.9307    0.9077       909\n",
      "\n",
      "    accuracy                         0.8086      2210\n",
      "   macro avg     0.7325    0.7200    0.7215      2210\n",
      "weighted avg     0.7899    0.8086    0.7964      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        807       91        14\n",
      "neutral         160      134        95\n",
      "positive         14       49       846\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    82.99\t   83.55\n",
      "DynaSent R1 \t    85.87\t   85.69\n",
      "DynaSent R2 \t    80.89\t   81.11\n",
      "SST-3       \t    72.15\t   80.86\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-11-03 17:23:38\n",
      "Duration: 1:03:05.126997\n"
     ]
    }
   ],
   "source": [
    "e21_g4o_elft_bex_df, e21_g4o_elft_bex_metrics = evaluate_experiment(\n",
    "    name='E21-G4O-ELFT-BEX',\n",
    "    notes='Experiment 21: Evaluate prompt-based model collaboration that includes similar, balanced examples',\n",
    "    lm='gpt-4o-2024-08-06',\n",
    "    instance='electra_large_gpt_sentiment_examples_balanced',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f882630",
   "metadata": {},
   "source": [
    "### E22-G4O-ELFT-PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "id": "59eb87f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-2024-08-06', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "id": "94f42f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "e22_g4o_elft_pr_result = electra_large_gpt_sentiment_probs(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "id": "8f7bf04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    classification='negative',\n",
       "    negative_probability='99.85%',\n",
       "    neutral_probability='0.04%',\n",
       "    positive_probability='0.12%'\n",
       ")"
      ]
     },
     "execution_count": 928,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e22_g4o_elft_pr_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "id": "2b3715ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "\n",
      "Negative Probability: Probability the review is negative from a model fine-tuned on sentiment\n",
      "\n",
      "Neutral Probability: Probability the review is neutral from a model fine-tuned on sentiment\n",
      "\n",
      "Positive Probability: Probability the review is positive from a model fine-tuned on sentiment\n",
      "\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "\n",
      "Negative Probability: 99.85%\n",
      "\n",
      "Neutral Probability: 0.04%\n",
      "\n",
      "Positive Probability: 0.12%\n",
      "\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\n\\nNegative Probability: Probability the review is negative from a model fine-tuned on sentiment\\n\\nNeutral Probability: Probability the review is neutral from a model fine-tuned on sentiment\\n\\nPositive Probability: Probability the review is positive from a model fine-tuned on sentiment\\n\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\n\\nNegative Probability: 99.85%\\n\\nNeutral Probability: 0.04%\\n\\nPositive Probability: 0.12%\\n\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 929,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "id": "2fdecb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E22-G4O-ELFT-PR\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-11-03 18:19:10\n",
      "Notes: Experiment 22: Evaluate prompt-based model collaboration with probabilities\n",
      "Model: gpt-4o-2024-08-06\n",
      "Instance: electra_large_gpt_sentiment_probs\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 5436 / 6530  (83.2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [2:01:43<00:00,  1.12s/it]  \n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "positive               2365\n",
      "negative               2319\n",
      "neutral                1846\n",
      "\n",
      "E22-G4O-ELFT-PR Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8599    0.8478    0.8538      2352\n",
      "     neutral     0.7465    0.7534    0.7499      1829\n",
      "    positive     0.8727    0.8787    0.8757      2349\n",
      "\n",
      "    accuracy                         0.8325      6530\n",
      "   macro avg     0.8264    0.8266    0.8265      6530\n",
      "weighted avg     0.8327    0.8325    0.8326      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1994      261        97\n",
      "neutral         247     1378       204\n",
      "positive         78      207      2064\n",
      "\n",
      "\n",
      "E22-G4O-ELFT-PR-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9127    0.8275    0.8680      1200\n",
      "     neutral     0.7785    0.9167    0.8419      1200\n",
      "    positive     0.9117    0.8350    0.8717      1200\n",
      "\n",
      "    accuracy                         0.8597      3600\n",
      "   macro avg     0.8676    0.8597    0.8605      3600\n",
      "weighted avg     0.8676    0.8597    0.8605      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        993      158        49\n",
      "neutral          52     1100        48\n",
      "positive         43      155      1002\n",
      "\n",
      "\n",
      "E22-G4O-ELFT-PR-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7804    0.8292    0.8040       240\n",
      "     neutral     0.8177    0.6542    0.7269       240\n",
      "    positive     0.7509    0.8542    0.7992       240\n",
      "\n",
      "    accuracy                         0.7792       720\n",
      "   macro avg     0.7830    0.7792    0.7767       720\n",
      "weighted avg     0.7830    0.7792    0.7767       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        199       19        22\n",
      "neutral          37      157        46\n",
      "positive         19       16       205\n",
      "\n",
      "\n",
      "E22-G4O-ELFT-PR-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8217    0.8794    0.8496       912\n",
      "     neutral     0.5021    0.3111    0.3841       389\n",
      "    positive     0.8630    0.9428    0.9012       909\n",
      "\n",
      "    accuracy                         0.8054      2210\n",
      "   macro avg     0.7289    0.7111    0.7116      2210\n",
      "weighted avg     0.7825    0.8054    0.7889      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        802       84        26\n",
      "neutral         158      121       110\n",
      "positive         16       36       857\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    82.65\t   83.25\n",
      "DynaSent R1 \t    86.05\t   85.97\n",
      "DynaSent R2 \t    77.67\t   77.92\n",
      "SST-3       \t    71.16\t   80.54\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-11-03 20:20:54\n",
      "Duration: 2:01:44.433746\n"
     ]
    }
   ],
   "source": [
    "e22_g4o_elft_pr_results_df, e22_g4o_elft_pr_metrics = evaluate_experiment(\n",
    "    name='E22-G4O-ELFT-PR',\n",
    "    notes='Experiment 22: Evaluate prompt-based model collaboration with probabilities',\n",
    "    lm='gpt-4o-2024-08-06',\n",
    "    instance='electra_large_gpt_sentiment_probs',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d39fb",
   "metadata": {},
   "source": [
    "### E23-G4O-ELFT-PR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "dde8c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-2024-08-06', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "id": "264137ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "e23_g4o_elft_pr2_result = electra_large_gpt_sentiment_pred_probs(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "id": "06b38a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    classification='negative',\n",
       "    classifier_decision='negative',\n",
       "    negative_probability='99.85%',\n",
       "    neutral_probability='0.04%',\n",
       "    positive_probability='0.12%'\n",
       ")"
      ]
     },
     "execution_count": 933,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e23_g4o_elft_pr2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "id": "2580cc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Review: The review text to classify.\n",
      "\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "\n",
      "Negative Probability: Probability the review is negative\n",
      "\n",
      "Neutral Probability: Probability the review is neutral\n",
      "\n",
      "Positive Probability: Probability the review is positive\n",
      "\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "\n",
      "Classifier Decision: negative\n",
      "\n",
      "Negative Probability: 99.85%\n",
      "\n",
      "Neutral Probability: 0.04%\n",
      "\n",
      "Positive Probability: 0.12%\n",
      "\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nClassify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\\n\\n---\\n\\nFollow the following format.\\n\\nReview: The review text to classify.\\n\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\n\\nNegative Probability: Probability the review is negative\\n\\nNeutral Probability: Probability the review is neutral\\n\\nPositive Probability: Probability the review is positive\\n\\nClassification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\\n\\n---\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\n\\nClassifier Decision: negative\\n\\nNegative Probability: 99.85%\\n\\nNeutral Probability: 0.04%\\n\\nPositive Probability: 0.12%\\n\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 934,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "id": "7a2487a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E23-G4O-ELFT-PR2\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-11-03 20:56:55\n",
      "Notes: Experiment 23: Evaluate prompt-based model collaboration with prediction and probabilities\n",
      "Model: gpt-4o-2024-08-06\n",
      "Instance: electra_large_gpt_sentiment_pred_probs\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 4131 / 4960  (83.3):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 4960/6530 [1:36:42<52:30,  2.01s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.exporter.otlp.proto.http.trace_exporter:Transient error Service Unavailable encountered while exporting span batch, retrying in 1s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5466 / 6530  (83.7): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [2:07:05<00:00,  1.17s/it]\n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "classifier_decision    6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "negative               2347\n",
      "positive               2332\n",
      "neutral                1851\n",
      "\n",
      "E23-G4O-ELFT-PR2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8615    0.8597    0.8606      2352\n",
      "     neutral     0.7455    0.7545    0.7500      1829\n",
      "    positive     0.8851    0.8787    0.8819      2349\n",
      "\n",
      "    accuracy                         0.8371      6530\n",
      "   macro avg     0.8307    0.8310    0.8308      6530\n",
      "weighted avg     0.8375    0.8371    0.8373      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2022      262        68\n",
      "neutral         249     1380       200\n",
      "positive         76      209      2064\n",
      "\n",
      "\n",
      "E23-G4O-ELFT-PR2-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9130    0.8400    0.8750      1200\n",
      "     neutral     0.7764    0.9175    0.8411      1200\n",
      "    positive     0.9267    0.8325    0.8771      1200\n",
      "\n",
      "    accuracy                         0.8633      3600\n",
      "   macro avg     0.8721    0.8633    0.8644      3600\n",
      "weighted avg     0.8721    0.8633    0.8644      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1008      160        32\n",
      "neutral          52     1101        47\n",
      "positive         44      157       999\n",
      "\n",
      "\n",
      "E23-G4O-ELFT-PR2-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7947    0.8708    0.8310       240\n",
      "     neutral     0.8168    0.6500    0.7239       240\n",
      "    positive     0.7820    0.8667    0.8221       240\n",
      "\n",
      "    accuracy                         0.7958       720\n",
      "   macro avg     0.7978    0.7958    0.7923       720\n",
      "weighted avg     0.7978    0.7958    0.7923       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        209       19        12\n",
      "neutral          38      156        46\n",
      "positive         16       16       208\n",
      "\n",
      "\n",
      "E23-G4O-ELFT-PR2-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8214    0.8827    0.8510       912\n",
      "     neutral     0.5083    0.3162    0.3899       389\n",
      "    positive     0.8674    0.9428    0.9035       909\n",
      "\n",
      "    accuracy                         0.8077      2210\n",
      "   macro avg     0.7324    0.7139    0.7148      2210\n",
      "weighted avg     0.7852    0.8077    0.7914      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        805       83        24\n",
      "neutral         159      123       107\n",
      "positive         16       36       857\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    83.08\t   83.71\n",
      "DynaSent R1 \t    86.44\t   86.33\n",
      "DynaSent R2 \t    79.23\t   79.58\n",
      "SST-3       \t    71.48\t   80.77\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-11-03 23:04:01\n",
      "Duration: 2:07:05.964889\n"
     ]
    }
   ],
   "source": [
    "e23_g4o_elft_pr2_df, e23_g4o_elft_pr2_metrics = evaluate_experiment(\n",
    "    name='E23-G4O-ELFT-PR2',\n",
    "    notes='Experiment 23: Evaluate prompt-based model collaboration with prediction and probabilities',\n",
    "    lm='gpt-4o-2024-08-06',\n",
    "    instance='electra_large_gpt_sentiment_pred_probs',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd855f2",
   "metadata": {},
   "source": [
    "### E24-G4O-ELFT-EX-PR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "id": "ff6cd9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the correct language model and disable experimental mode\n",
    "lm = dspy.OpenAI(model='gpt-4o-2024-08-06', api_key=openai_key, max_tokens=8192)\n",
    "dspy.settings.configure(lm=lm, experimental=False)\n",
    "dsp.settings.show_guidelines = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "id": "ec2d099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test result for the GPT sentiment module\n",
    "e24_g4o_elft_ex_pr2_result = electra_large_gpt_sentiment_pred_probs_examples(review=test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "id": "87a86afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    examples=[{'index': 57, 'similarity_score': 0.9977191090583801, 'review': 'This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.', 'classification': 'negative'}, {'index': 3410, 'similarity_score': 0.9972444176673889, 'review': 'This was my second time here and there will not be a third time.', 'classification': 'negative'}, {'index': 3476, 'similarity_score': 0.9971747398376465, 'review': 'Portioning of the Octopus could have been better.', 'classification': 'negative'}, {'index': 3679, 'similarity_score': 0.9971185922622681, 'review': 'My only complaint was the bill for a long distance phone call - $4 for a 3 minutes to Seattle.', 'classification': 'negative'}, {'index': 3160, 'similarity_score': 0.9971152544021606, 'review': 'I received big-time attitude from multiple \"fabulous\" employees when I pulled up at 7:29 a.m. to get my car washed.', 'classification': 'negative'}],\n",
       "    classification='negative',\n",
       "    classifier_decision='negative',\n",
       "    negative_probability='99.85%',\n",
       "    neutral_probability='0.04%',\n",
       "    positive_probability='0.12%'\n",
       ")"
      ]
     },
     "execution_count": 938,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e24_g4o_elft_ex_pr2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "id": "d1ad7072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify the sentiment of a review as either 'negative', 'neutral', or 'positive'.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Examples: A list of examples that demonstrate different sentiment classes.\n",
      "\n",
      "Review: The review text to classify.\n",
      "\n",
      "Classifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\n",
      "\n",
      "Negative Probability: Probability the review is negative\n",
      "\n",
      "Neutral Probability: Probability the review is neutral\n",
      "\n",
      "Positive Probability: Probability the review is positive\n",
      "\n",
      "Classification: One word representing the sentiment classification: 'negative', 'neutral', or 'positive' (do not repeat the field name, do not use 'mixed')\n",
      "\n",
      "---\n",
      "\n",
      "Examples:\n",
      "- negative: This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.\n",
      "- negative: This was my second time here and there will not be a third time.\n",
      "- negative: Portioning of the Octopus could have been better.\n",
      "- negative: My only complaint was the bill for a long distance phone call - $4 for a 3 minutes to Seattle.\n",
      "- negative: I received big-time attitude from multiple \"fabulous\" employees when I pulled up at 7:29 a.m. to get my car washed.\n",
      "\n",
      "Review: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\n",
      "\n",
      "Classifier Decision: negative\n",
      "\n",
      "Negative Probability: 99.85%\n",
      "\n",
      "Neutral Probability: 0.04%\n",
      "\n",
      "Positive Probability: 0.12%\n",
      "\n",
      "Classification:\u001b[32m negative\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nClassify the sentiment of a review as either \\'negative\\', \\'neutral\\', or \\'positive\\'.\\n\\n---\\n\\nFollow the following format.\\n\\nExamples: A list of examples that demonstrate different sentiment classes.\\n\\nReview: The review text to classify.\\n\\nClassifier Decision: The sentiment classification proposed by a model fine-tuned on sentiment.\\n\\nNegative Probability: Probability the review is negative\\n\\nNeutral Probability: Probability the review is neutral\\n\\nPositive Probability: Probability the review is positive\\n\\nClassification: One word representing the sentiment classification: \\'negative\\', \\'neutral\\', or \\'positive\\' (do not repeat the field name, do not use \\'mixed\\')\\n\\n---\\n\\nExamples:\\n- negative: This Walgreens Pharmacy is a poor excuse for a pharmacy, that is if you have only the most insane expectations.\\n- negative: This was my second time here and there will not be a third time.\\n- negative: Portioning of the Octopus could have been better.\\n- negative: My only complaint was the bill for a long distance phone call - $4 for a 3 minutes to Seattle.\\n- negative: I received big-time attitude from multiple \"fabulous\" employees when I pulled up at 7:29 a.m. to get my car washed.\\n\\nReview: Those 2 drinks are part of the HK culture and has years of history. It is so bad.\\n\\nClassifier Decision: negative\\n\\nNegative Probability: 99.85%\\n\\nNeutral Probability: 0.04%\\n\\nPositive Probability: 0.12%\\n\\nClassification:\\x1b[32m negative\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 939,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "id": "b21cc22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Experiment: E24-G4O-ELFT-EX-PR2\n",
      "--------------------------------------------------------------------------------\n",
      "Start time: 2024-11-04 01:53:46\n",
      "Notes: Experiment 24: Evaluate prompt-based model collaboration with prediction, probabilities, and similar examples\n",
      "Model: gpt-4o-2024-08-06\n",
      "Instance: electra_large_gpt_sentiment_pred_probs_examples\n",
      "Dataset shape: [6530, 4]\n",
      "Examples length: 6530\n",
      "Save directory: research\n",
      "\n",
      "Running evaluation...\n",
      "Average Metric: 5443 / 6530  (83.4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6530/6530 [1:18:06<00:00,  1.39it/s]\n",
      "\n",
      "Columns in results DataFrame:\n",
      "review                 6530 values\n",
      "classification         6530 values\n",
      "prediction             6530 values\n",
      "match                  6530 values\n",
      "classifier_decision    6530 values\n",
      "\n",
      "Source value counts:\n",
      "dynasent_r1            3600\n",
      "sst_local              2210\n",
      "dynasent_r2             720\n",
      "\n",
      "Prediction value counts:\n",
      "positive               2359\n",
      "negative               2322\n",
      "neutral                1849\n",
      "\n",
      "E24-G4O-ELFT-EX-PR2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8618    0.8508    0.8562      2352\n",
      "     neutral     0.7442    0.7523    0.7482      1829\n",
      "    positive     0.8758    0.8795    0.8777      2349\n",
      "\n",
      "    accuracy                         0.8335      6530\n",
      "   macro avg     0.8272    0.8275    0.8274      6530\n",
      "weighted avg     0.8339    0.8335    0.8337      6530\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2001      265        86\n",
      "neutral         246     1376       207\n",
      "positive         75      208      2066\n",
      "\n",
      "\n",
      "E24-G4O-ELFT-EX-PR2-DYN-R1 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9141    0.8333    0.8718      1200\n",
      "     neutral     0.7770    0.9175    0.8414      1200\n",
      "    positive     0.9210    0.8358    0.8764      1200\n",
      "\n",
      "    accuracy                         0.8622      3600\n",
      "   macro avg     0.8707    0.8622    0.8632      3600\n",
      "weighted avg     0.8707    0.8622    0.8632      3600\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       1000      161        39\n",
      "neutral          52     1101        47\n",
      "positive         42      155      1003\n",
      "\n",
      "\n",
      "E24-G4O-ELFT-EX-PR2-DYN-R2 Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7843    0.8333    0.8081       240\n",
      "     neutral     0.8158    0.6458    0.7209       240\n",
      "    positive     0.7527    0.8625    0.8039       240\n",
      "\n",
      "    accuracy                         0.7806       720\n",
      "   macro avg     0.7843    0.7806    0.7776       720\n",
      "weighted avg     0.7843    0.7806    0.7776       720\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        200       19        21\n",
      "neutral          38      155        47\n",
      "positive         17       16       207\n",
      "\n",
      "\n",
      "E24-G4O-ELFT-EX-PR2-SST Multi-Class Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8232    0.8783    0.8499       912\n",
      "     neutral     0.4959    0.3085    0.3803       389\n",
      "    positive     0.8603    0.9417    0.8992       909\n",
      "\n",
      "    accuracy                         0.8041      2210\n",
      "   macro avg     0.7265    0.7095    0.7098      2210\n",
      "weighted avg     0.7809    0.8041    0.7875      2210\n",
      "\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative        801       85        26\n",
      "neutral         156      120       113\n",
      "positive         16       37       856\n",
      "\n",
      "Saving results to 'research'...\n",
      "\n",
      "Summary Metrics:\n",
      "Dataset     \tF1 (macro)\tAccuracy\n",
      "---------------------------------------------\n",
      "Merged      \t    82.74\t   83.35\n",
      "DynaSent R1 \t    86.32\t   86.22\n",
      "DynaSent R2 \t    77.76\t   78.06\n",
      "SST-3       \t    70.98\t   80.41\n",
      "\n",
      "Evaluation completed\n",
      "End time: 2024-11-04 03:11:53\n",
      "Duration: 1:18:06.884091\n"
     ]
    }
   ],
   "source": [
    "e24_g4o_elft_ex_pr2_df, e24_g4o_elft_ex_pr2_metrics = evaluate_experiment(\n",
    "    name='E24-G4O-ELFT-EX-PR2',\n",
    "    notes='Experiment 24: Evaluate prompt-based model collaboration with prediction, probabilities, and similar examples',\n",
    "    lm='gpt-4o-2024-08-06',\n",
    "    instance='electra_large_gpt_sentiment_pred_probs_examples',\n",
    "    dataset=test_df,\n",
    "    examples=test_ex\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6289da7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
