python ./finetune.py --dataset 'merged_local' --weights_name 'google/electra-large-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 24 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Large' --wandb_run 'R2-E2-ELFT-Merged' --random_seed 123 --interactive

(nlp) ubuntu@159-54-167-99:~/nlp-west/sentiment$ python ./finetune.py --dataset 'merged_local' --weights_name 'google/electra-large-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 24 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Large' --wandb_run 'R2-E2-ELFT-Merged' --random_seed 123 --interactive

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 3 - Device: cuda:3
Rank 2 - Device: cuda:2
Rank 6 - Device: cuda:6
Rank 4 - Device: cuda:4
Rank 5 - Device: cuda:5
Rank 1 - Device: cuda:1
Rank 7 - Device: cuda:7
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/ubuntu/nlp-west/sentiment/wandb/run-20250323_184714-de78sq4r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run R2-E2-ELFT-Merged
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jimbeno/Electra%20Large
wandb: üöÄ View run at https://wandb.ai/jimbeno/Electra%20Large/runs/de78sq4r
Wand run initialized.

Initializing 'google/electra-large-discriminator' tokenizer and model...
Checking for local files...
Some files not found locally, downloading...
tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48.0/48.0 [00:00<00:00, 316kB/s]
config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 668/668 [00:00<00:00, 2.96MB/s]
vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232k/232k [00:00<00:00, 10.6MB/s]
tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 466k/466k [00:00<00:00, 5.82MB/s]
pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.34G/1.34G [00:07<00:00, 188MB/s]
model.safetensors:   0%|                                                                                                                                   | 0.00/1.34G [00:00<?, ?B/s]Download complete
All ranks loading tokenizer from local files...
All ranks loading model from local files...
model.safetensors:  10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                              | 136M/1.34G [00:03<00:12, 98.4MB/s]Tokenizer and model initialized (14s)

Loading data...
Using the same dataset for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using merged_local 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
model.safetensors:  12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                             | 157M/1.34G [00:03<00:10, 118MB/s]Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Train size: 102097
Validation size: 5421
Evaluation size: 6530
model.safetensors:  13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                           | 178M/1.34G [00:03<00:10, 111MB/s]Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 2352
	       Neutral: 1829
	      Positive: 2349
Data loaded (425ms)

Processing data...
(Batch size: 16, Pooling: Mean, Fine Tune BERT: True, Chunk size: None)...
Extracting sentences and labels...
model.safetensors:  27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                          | 367M/1.34G [00:04<00:05, 183MB/s]
Data saved to: saves/data_8_gpu_20250323-184731.npz
X Train shape: [102097], X Validation shape: [5421], X Test shape: [6530]
y Train shape: [102097], y Validation shape: [5421], y Test shape: [6530]
Data processed (931ms)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 2, Max Grad Norm: None
Batch Size: 16, Max Epochs: 50, LR: 1e-05, Early Stop: score, Fine-tune BERT: True, Fine-tune Layers: 24, Freeze BERT: False, Target Score: None, Interactive: True
Classifier initialized (22ms)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 10 iterations.
Using provided validation set for early stopping.
model.safetensors:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                        | 388M/1.34G [00:04<00:05, 175MB/s]Building dataset and dataloader...
Initializing model, graph, optimizer...
BERT's original pooler removed. Using custom pooling type: mean
BERT has 334,092,288 trainable parameters and 0 non-trainable parameters
Number of BERT layers requiring gradients: 24 out of 24
Fine-tuning the last 24 out of 24 BERT layers

Model Architecture Summary:
BERTClassifier(
  (bert): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(30522, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0-23): 24 x ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (custom_pooling): PoolingLayer()
  (classifier): Classifier(
    (layers): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=1024, out_features=1024, bias=True)
      (4): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (5): Dropout(p=0.3, inplace=False)
      (6): Linear(in_features=1024, out_features=3, bias=True)
    )
  )
)

Model Parameters:
  bert.embeddings.word_embeddings: 31,254,528 (trainable)
  bert.embeddings.position_embeddings: 524,288 (trainable)
  bert.embeddings.token_type_embeddings: 2,048 (trainable)
  bert.embeddings.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.0.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.0.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.0.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.0.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.1.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.1.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.1.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.1.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.2.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.2.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.2.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.2.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.3.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.3.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.3.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.3.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.4.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.4.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.4.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.4.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.5.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.5.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.5.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.5.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.6.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.6.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.6.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.6.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.7.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.7.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.7.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.7.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.8.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.8.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.8.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.8.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.9.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.9.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.9.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.9.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.10.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.10.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.10.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.10.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.11.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.11.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.11.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.11.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.12.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.12.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.12.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.12.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.13.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.13.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.13.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.13.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.14.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.14.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.14.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.14.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.15.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.15.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.15.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.15.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.16.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.16.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.16.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.16.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.17.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.17.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.17.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.17.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.18.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.18.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.18.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.18.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.19.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.19.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.19.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.19.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.20.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.20.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.20.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.20.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.21.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.21.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.21.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.21.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.22.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.22.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.22.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.22.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.23.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.23.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.23.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.23.output.LayerNorm: 2,048 (trainable)
  classifier.layers.0: 1,049,600 (trainable)
  classifier.layers.1.projection: 2,099,200 (trainable)
  classifier.layers.3: 1,049,600 (trainable)
  classifier.layers.6: 3,075 (trainable)

Total layers: 200
Trainable layers: 200
Total parameters: 338,293,763
Trainable parameters: 338,293,763
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Layer-wise decay factor: 0.95
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 1, Max Iterations: 50, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 10

model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.34G/1.34G [00:10<00:00, 126MB/s]

Skipping 1 epochs.
Epoch 1/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 798/798 [24:05<00:00,  1.81s/it, loss=1.7908, lr=9.06e-06, grad=M:0.3046, status=Epoch complete]
‚ñà Epoch  1: Loss T 1.790755 V 0.947151 | F1 T 0.216630 V 0.157012 B 0.157012 SC 0/10 | Acc T 0.481362 V 0.308075 | LR 9.06e-06 | Grad ‚Üì 0.000000 ‚Üë 2.157621 M 0.304588 | 24m 42s
Saved model state: checkpoints/checkpoint_epoch_1_20250323-191215.pth

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: 9
Skipping 9 epochs.
Epoch 2/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 798/798 [24:15<00:00,  1.82s/it, loss=1.2766, lr=6.59e-06, grad=M:0.2566, status=Epoch complete]
‚ñà Epoch  2: Loss T 1.276571 V 0.732512 | F1 T 0.526650 V 0.469906 B 0.469906 SC 0/10 | Acc T 0.693264 V 0.561947 | LR 6.59e-06 | Grad ‚Üì 0.000000 ‚Üë 2.081715 M 0.256550 | 25m 2s
Saved model state: checkpoints/checkpoint_epoch_2_20250323-193728.pth
Epoch 3/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 798/798 [24:32<00:00,  1.84s/it, loss=1.0018, lr=3.53e-06, grad=M:0.2034, status=Epoch complete]
‚ñà Epoch  3: Loss T 1.001823 V 0.494969 | F1 T 0.843571 V 0.821492 B 0.821492 SC 0/10 | Acc T 0.847136 V 0.823009 | LR 3.53e-06 | Grad ‚Üì 0.000000 ‚Üë 1.529029 M 0.203414 | 24m 32s
Saved model state: checkpoints/checkpoint_epoch_3_20250323-200211.pth
Epoch 4/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 798/798 [24:03<00:00,  1.81s/it, loss=0.8400, lr=1.05e-06, grad=M:0.2697, status=Epoch complete]
‚ñà Epoch  4: Loss T 0.840022 V 0.479870 | F1 T 0.850988 V 0.822985 B 0.822985 SC 0/10 | Acc T 0.854061 V 0.824484 | LR 1.05e-06 | Grad ‚Üì 0.000000 ‚Üë 1.863573 M 0.269678 | 24m 3s
Saved model state: checkpoints/checkpoint_epoch_4_20250323-202627.pth
Epoch 5/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 798/798 [24:42<00:00,  1.86s/it, loss=0.8079, lr=1.00e-07, grad=M:0.1613, status=Epoch complete]
‚ñà Epoch  5: Loss T 0.807865 V 0.490886 | F1 T 0.852582 V 0.824797 B 0.824797 SC 0/10 | Acc T 0.855637 V 0.826143 | LR 1.00e-07 | Grad ‚Üì 0.000000 ‚Üë 1.582390 M 0.161285 | 24m 42s
Saved model state: checkpoints/checkpoint_epoch_5_20250323-205120.pth
Epoch 6/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 798/798 [24:16<00:00,  1.83s/it, loss=0.8252, lr=9.06e-06, grad=M:0.2371, status=Epoch complete]
‚ñà Epoch  6: Loss T 0.825246 V 0.496114 | F1 T 0.867107 V 0.816768 B 0.824797 SC 1/10 | Acc T 0.870710 V 0.817294 | LR 9.06e-06 | Grad ‚Üì 0.000000 ‚Üë 2.995150 M 0.237135 | 24m 16s
Saved model state: checkpoints/checkpoint_epoch_6_20250323-211549.pth
Epoch 7/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 798/798 [24:18<00:00,  1.83s/it, loss=0.7483, lr=6.59e-06, grad=M:0.2277, status=Epoch complete]
‚ñà Epoch  7: Loss T 0.748250 V 0.475270 | F1 T 0.884289 V 0.827494 B 0.827494 SC 0/10 | Acc T 0.885313 V 0.828909 | LR 6.59e-06 | Grad ‚Üì 0.000000 ‚Üë 1.380734 M 0.227710 | 24m 18s
Saved model state: checkpoints/checkpoint_epoch_7_20250323-214020.pth
Epoch 8/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 798/798 [24:15<00:00,  1.82s/it, loss=0.6768, lr=3.53e-06, grad=M:0.2245, status=Epoch complete]
‚ñà Epoch  8: Loss T 0.676756 V 0.474271 | F1 T 0.895820 V 0.832622 B 0.832622 SC 0/10 | Acc T 0.896596 V 0.834440 | LR 3.53e-06 | Grad ‚Üì 0.000000 ‚Üë 1.769022 M 0.224486 | 24m 15s
Saved model state: checkpoints/checkpoint_epoch_8_20250323-220448.pth
Epoch 9/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 798/798 [24:19<00:00,  1.83s/it, loss=0.6281, lr=1.05e-06, grad=M:0.3858, status=Epoch complete]
‚ñà Epoch  9: Loss T 0.628133 V 0.491007 | F1 T 0.902642 V 0.833723 B 0.833723 SC 0/10 | Acc T 0.903089 V 0.835177 | LR 1.05e-06 | Grad ‚Üì 0.000000 ‚Üë 5.476643 M 0.385771 | 24m 19s
Saved model state: checkpoints/checkpoint_epoch_9_20250323-222919.pth
Epoch 10/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 798/798 [24:39<00:00,  1.85s/it, loss=0.6015, lr=1.00e-07, grad=M:0.2890, status=Epoch complete]
‚ñà Epoch 10: Loss T 0.601460 V 0.502019 | F1 T 0.903595 V 0.831415 B 0.833723 SC 1/10 | Acc T 0.904010 V 0.832965 | LR 1.00e-07 | Grad ‚Üì 0.000000 ‚Üë 2.525706 M 0.288964 | 24m 39s
Memory: Rank 0: 34.84 GB | Rank 1: 35.46 GB | Rank 2: 35.66 GB | Rank 3: 35.46 GB | Rank 4: 35.46 GB | Rank 5: 35.46 GB | Rank 6: 35.43 GB | Rank 7: 35.31 GB (Max: 42.41 GB, Total: 339.26 GB)
Saved model state: checkpoints/checkpoint_epoch_10_20250323-225412.pth

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: 
[E ProcessGroupNCCL.cpp:475] [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139305 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:475] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139306 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:475] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139306 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:475] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139305 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:475] [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139302 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:475] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139302 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:475] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139305 milliseconds before timing out.
Skipping 1 epochs.
Epoch 11/50:   0%|                                                                                                                                             | 0/798 [00:00<?, ?it/s][E ProcessGroupNCCL.cpp:489] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:495] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:916] [Rank 7] NCCL watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139305 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:489] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:495] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:916] [Rank 2] NCCL watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139302 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 2] NCCL watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139302 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 7] NCCL watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139305 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:489] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:495] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:916] [Rank 5] NCCL watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139305 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 5] NCCL watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139305 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:489] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:495] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:916] [Rank 4] NCCL watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139302 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 4] NCCL watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139302 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:489] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:495] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:916] [Rank 1] NCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139306 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 1] NCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139306 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:489] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:495] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:916] [Rank 3] NCCL watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139305 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 3] NCCL watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139305 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:489] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[E ProcessGroupNCCL.cpp:495] To avoid data inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:916] [Rank 6] NCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139306 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 6] NCCL watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1903464, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 2139306 milliseconds before timing out.
^C
Ctrl+C received. Terminating all processes...
Rank 0 - Current process: MainProcess cleaning up...
Terminating all child processes of MainProcess...
Terminated child process: SpawnProcess-1
Rank 0 - Exiting program...
^C
Ctrl+C received. Terminating all processes...
Rank 0 - Current process: MainProcess cleaning up...
Terminating all child processes of MainProcess...
Terminated child process: SpawnProcess-1
Rank 0 - Exiting program...
Exception ignored in atexit callback: <function _exit_function at 0x7fc4385051b0>
Traceback (most recent call last):
  File "/usr/lib/python3.10/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/usr/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.10/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/usr/lib/python3.10/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/ubuntu/nlp-west/sentiment/utils.py", line 159, in signal_handler
    cleanup_and_exit(0, True) 
  File "/home/ubuntu/nlp-west/sentiment/utils.py", line 205, in cleanup_and_exit
    sys.exit(0)
SystemExit: 0
^C/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
^C


checkpoints/checkpoint_epoch_10_20250323-225412.pth



python ./finetune.py --dataset 'merged_local' --weights_name 'google/electra-large-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 24 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Large' --wandb_run 'R2-E2-ELFT-Merged Checkpoint 10' --random_seed 123 --interactive --model_file 'checkpoint_epoch_10_20250323-225412.pth'


(nlp) ubuntu@159-54-167-99:~/nlp-west/sentiment$ python ./finetune.py --dataset 'merged_local' --weights_name 'google/electra-large-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 24 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Large' --wandb_run 'R2-E2-ELFT-Merged Checkpoint 10' --random_seed 123 --interactive --model_file 'checkpoint_epoch_10_20250323-225412.pth'

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 4 - Device: cuda:4
Rank 5 - Device: cuda:5
Rank 3 - Device: cuda:3
Rank 6 - Device: cuda:6
Rank 7 - Device: cuda:7
Rank 1 - Device: cuda:1
Rank 2 - Device: cuda:2
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/ubuntu/nlp-west/sentiment/wandb/run-20250323_233451-wukbq2dv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run R2-E2-ELFT-Merged Checkpoint 10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jimbeno/Electra%20Large
wandb: üöÄ View run at https://wandb.ai/jimbeno/Electra%20Large/runs/wukbq2dv
Wand run initialized.

Initializing 'google/electra-large-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (4s)

Loading data...
Using the same dataset for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using merged_local 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Train size: 102097
Validation size: 5421
Evaluation size: 6530
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 2352
	       Neutral: 1829
	      Positive: 2349
Data loaded (351ms)

Processing data...
(Batch size: 16, Pooling: Mean, Fine Tune BERT: True, Chunk size: None)...
Extracting sentences and labels...

Data saved to: saves/data_8_gpu_20250323-233457.npz
X Train shape: [102097], X Validation shape: [5421], X Test shape: [6530]
y Train shape: [102097], y Validation shape: [5421], y Test shape: [6530]
Data processed (907ms)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 2, Max Grad Norm: None
Batch Size: 16, Max Epochs: 50, LR: 1e-05, Early Stop: score, Fine-tune BERT: True, Fine-tune Layers: 24, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_10_20250323-225412.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_10_20250323-225412.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (40s)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 10 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Initializing model, graph, optimizer...
BERT's original pooler removed. Using custom pooling type: mean
BERT has 334,092,288 trainable parameters and 0 non-trainable parameters
Number of BERT layers requiring gradients: 24 out of 24
Fine-tuning the last 24 out of 24 BERT layers

Model Architecture Summary:
BERTClassifier(
  (bert): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(30522, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0-23): 24 x ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (custom_pooling): PoolingLayer()
  (classifier): Classifier(
    (layers): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=1024, out_features=1024, bias=True)
      (4): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (5): Dropout(p=0.3, inplace=False)
      (6): Linear(in_features=1024, out_features=3, bias=True)
    )
  )
)

Model Parameters:
  bert.embeddings.word_embeddings: 31,254,528 (trainable)
  bert.embeddings.position_embeddings: 524,288 (trainable)
  bert.embeddings.token_type_embeddings: 2,048 (trainable)
  bert.embeddings.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.0.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.0.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.0.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.0.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.1.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.1.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.1.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.1.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.2.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.2.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.2.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.2.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.3.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.3.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.3.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.3.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.4.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.4.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.4.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.4.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.5.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.5.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.5.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.5.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.6.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.6.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.6.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.6.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.7.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.7.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.7.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.7.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.8.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.8.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.8.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.8.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.9.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.9.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.9.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.9.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.10.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.10.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.10.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.10.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.11.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.11.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.11.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.11.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.12.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.12.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.12.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.12.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.13.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.13.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.13.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.13.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.14.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.14.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.14.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.14.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.15.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.15.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.15.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.15.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.16.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.16.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.16.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.16.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.17.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.17.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.17.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.17.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.18.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.18.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.18.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.18.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.19.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.19.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.19.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.19.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.20.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.20.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.20.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.20.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.21.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.21.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.21.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.21.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.22.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.22.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.22.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.22.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.23.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.23.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.23.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.23.output.LayerNorm: 2,048 (trainable)
  classifier.layers.0: 1,049,600 (trainable)
  classifier.layers.1.projection: 2,099,200 (trainable)
  classifier.layers.3: 1,049,600 (trainable)
  classifier.layers.6: 3,075 (trainable)

Total layers: 200
Trainable layers: 200
Total parameters: 338,293,763
Trainable parameters: 338,293,763
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Layer-wise decay factor: 0.95
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 11, Max Iterations: 50, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 10

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: 
Skipping 1 epochs.
Epoch 11/61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 798/798 [24:46<00:00,  1.86s/it, loss=0.6374, lr=9.06e-06, grad=M:0.2234, status=Epoch complete]
‚ñà Epoch 11: Loss T 0.637372 V 0.510308 | F1 T 0.914085 V 0.828878 B 0.828878 SC 0/10 | Acc T 0.913941 V 0.830568 | LR 9.06e-06 | Grad ‚Üì 0.000000 ‚Üë 1.667601 M 0.223388 | 24m 58s
Saved model state: checkpoints/checkpoint_epoch_11_20250324-000037.pth

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: 
Skipping 1 epochs.
Epoch 12/61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 798/798 [24:08<00:00,  1.82s/it, loss=0.7624, lr=6.59e-06, grad=M:0.1963, status=Epoch complete]
‚ñà Epoch 12: Loss T 0.762412 V 1.069692 | F1 T 0.409078 V 0.342861 B 0.828878 SC 1/10 | Acc T 0.557069 V 0.403577 | LR 6.59e-06 | Grad ‚Üì 0.000000 ‚Üë 6.865118 M 0.196298 | 50m 25s
Saved model state: checkpoints/checkpoint_epoch_12_20250324-005114.pth

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: 
Skipping 1 epochs.
Epoch 13/61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 798/798 [24:00<00:00,  1.80s/it, loss=2.0806, lr=3.53e-06, grad=M:0.1486, status=Epoch complete]
‚ñà Epoch 13: Loss T 2.080620 V 1.164757 | F1 T 0.218650 V 0.158145 B 0.828878 SC 2/10 | Acc T 0.482048 V 0.308628 | LR 3.53e-06 | Grad ‚Üì 0.000000 ‚Üë 3.466647 M 0.148569 | 25m 7s
Saved model state: checkpoints/checkpoint_epoch_13_20250324-011633.pth

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: q
Quitting training...
Saving model in ONNX format...
An error occurred during training: Module onnx is not installed!
Traceback (most recent call last):
  File "/home/ubuntu/nlp-west/sentiment/nlp/lib/python3.10/site-packages/torch/onnx/_internal/onnx_proto_utils.py", line 221, in _add_onnxscript_fn
    import onnx
ModuleNotFoundError: No module named 'onnx'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ubuntu/nlp-west/sentiment/finetune.py", line 1217, in main
    classifier.fit(X_train, X_val, y_train, y_val, rank, world_size, debug, start_epoch, model_state_dict, optimizer_state_dict,
  File "/home/ubuntu/nlp-west/sentiment/classifier.py", line 1713, in fit
    torch.onnx.export(self.model.module, dummy_input, onnx_file_path, opset_version=13)
  File "/home/ubuntu/nlp-west/sentiment/nlp/lib/python3.10/site-packages/torch/onnx/utils.py", line 516, in export
    _export(
  File "/home/ubuntu/nlp-west/sentiment/nlp/lib/python3.10/site-packages/torch/onnx/utils.py", line 1670, in _export
    proto = onnx_proto_utils._add_onnxscript_fn(
  File "/home/ubuntu/nlp-west/sentiment/nlp/lib/python3.10/site-packages/torch/onnx/_internal/onnx_proto_utils.py", line 223, in _add_onnxscript_fn
    raise errors.OnnxExporterError("Module onnx is not installed!") from e
torch.onnx.errors.OnnxExporterError: Module onnx is not installed!
wandb: 
wandb: üöÄ View run R2-E2-ELFT-Merged Checkpoint 10 at: https://wandb.ai/jimbeno/Electra%20Large/runs/wukbq2dv
wandb: Find logs at: wandb/run-20250323_233451-wukbq2dv/logs
(nlp) ubuntu@159-54-167-99:~/nlp-west/sentiment$ pip install onnx==1.17.0
Collecting onnx==1.17.0
  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)
     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 16.0/16.0 MB 79.2 MB/s eta 0:00:00
Requirement already satisfied: numpy>=1.20 in ./nlp/lib/python3.10/site-packages (from onnx==1.17.0) (1.26.4)
Requirement already satisfied: protobuf>=3.20.2 in ./nlp/lib/python3.10/site-packages (from onnx==1.17.0) (5.29.4)
Installing collected packages: onnx
Successfully installed onnx-1.17.0



python ./finetune.py --dataset 'merged_local' --weights_name 'google/electra-large-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 24 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Large' --wandb_run 'R2-E2-ELFT-Merged Checkpoint 13' --random_seed 123 --interactive --model_file 'checkpoint_epoch_13_20250324-011633.pth'


(nlp) ubuntu@159-54-167-99:~/nlp-west/sentiment$ python ./finetune.py --dataset 'merged_local' --weights_name 'google/electra-large-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 24 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Large' --wandb_run 'R2-E2-ELFT-Merged Checkpoint 13' --random_seed 123 --interactive --model_file 'checkpoint_epoch_13_20250324-011633.pth'

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 5 - Device: cuda:5
Rank 7 - Device: cuda:7
Rank 1 - Device: cuda:1
Rank 2 - Device: cuda:2
Rank 4 - Device: cuda:4
Rank 6 - Device: cuda:6
Rank 3 - Device: cuda:3
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/ubuntu/nlp-west/sentiment/wandb/run-20250324_013143-kzs98rks
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run R2-E2-ELFT-Merged Checkpoint 13
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jimbeno/Electra%20Large
wandb: üöÄ View run at https://wandb.ai/jimbeno/Electra%20Large/runs/kzs98rks
Wand run initialized.

Initializing 'google/electra-large-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (4s)

Loading data...
Using the same dataset for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using merged_local 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Train size: 102097
Validation size: 5421
Evaluation size: 6530
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 2352
	       Neutral: 1829
	      Positive: 2349
Data loaded (341ms)

Processing data...
(Batch size: 16, Pooling: Mean, Fine Tune BERT: True, Chunk size: None)...
Extracting sentences and labels...

Data saved to: saves/data_8_gpu_20250324-013148.npz
X Train shape: [102097], X Validation shape: [5421], X Test shape: [6530]
y Train shape: [102097], y Validation shape: [5421], y Test shape: [6530]
Data processed (1s)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 2, Max Grad Norm: None
Batch Size: 16, Max Epochs: 50, LR: 1e-05, Early Stop: score, Fine-tune BERT: True, Fine-tune Layers: 24, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_13_20250324-011633.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_13_20250324-011633.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (41s)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 10 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Initializing model, graph, optimizer...
BERT's original pooler removed. Using custom pooling type: mean
BERT has 334,092,288 trainable parameters and 0 non-trainable parameters
Number of BERT layers requiring gradients: 24 out of 24
Fine-tuning the last 24 out of 24 BERT layers

Model Architecture Summary:
BERTClassifier(
  (bert): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(30522, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0-23): 24 x ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (custom_pooling): PoolingLayer()
  (classifier): Classifier(
    (layers): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=1024, out_features=1024, bias=True)
      (4): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (5): Dropout(p=0.3, inplace=False)
      (6): Linear(in_features=1024, out_features=3, bias=True)
    )
  )
)

Model Parameters:
  bert.embeddings.word_embeddings: 31,254,528 (trainable)
  bert.embeddings.position_embeddings: 524,288 (trainable)
  bert.embeddings.token_type_embeddings: 2,048 (trainable)
  bert.embeddings.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.0.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.0.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.0.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.0.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.1.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.1.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.1.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.1.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.2.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.2.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.2.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.2.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.3.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.3.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.3.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.3.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.4.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.4.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.4.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.4.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.5.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.5.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.5.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.5.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.6.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.6.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.6.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.6.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.7.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.7.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.7.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.7.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.8.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.8.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.8.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.8.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.9.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.9.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.9.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.9.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.10.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.10.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.10.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.10.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.11.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.11.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.11.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.11.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.12.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.12.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.12.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.12.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.13.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.13.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.13.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.13.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.14.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.14.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.14.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.14.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.15.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.15.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.15.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.15.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.16.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.16.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.16.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.16.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.17.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.17.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.17.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.17.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.18.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.18.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.18.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.18.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.19.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.19.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.19.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.19.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.20.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.20.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.20.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.20.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.21.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.21.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.21.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.21.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.22.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.22.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.22.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.22.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.23.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.23.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.23.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.23.output.LayerNorm: 2,048 (trainable)
  classifier.layers.0: 1,049,600 (trainable)
  classifier.layers.1.projection: 2,099,200 (trainable)
  classifier.layers.3: 1,049,600 (trainable)
  classifier.layers.6: 3,075 (trainable)

Total layers: 200
Trainable layers: 200
Total parameters: 338,293,763
Trainable parameters: 338,293,763
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Layer-wise decay factor: 0.95
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 14, Max Iterations: 50, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 10

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: q
Quitting training...
Saving model in ONNX format...
Model saved in ONNX format to saves/model_20250324-013634.onnx and uploaded to Weights & Biases.
Saved model state: checkpoints/final_model_20250324-013651.pth
Saved model pickle: checkpoints/final_model_20250324-013651.pkl
Training completed (4m 40s)

Evaluating model...

R2-E2-ELFT-Merged Checkpoint 13 Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.750000  0.001276  0.002547      2352
     neutral   0.280282  0.999453  0.437792      1829
    positive   1.000000  0.001703  0.003400      2349

    accuracy                       0.281011      6530
   macro avg   0.676761  0.334144  0.147913      6530
weighted avg   0.708367  0.281011  0.124762      6530

ROC AUC: 0.546148

Predicted  negative  neutral  positive
Actual                                
negative          3     2349         0
neutral           1     1828         0
positive          0     2345         4

Saved predictions: saves/predictions_20250324-014204.csv

Macro F1 Score: 0.15

Evaluation completed (7m 19s)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: eval/macro_f1_score ‚ñÅ
wandb: 
wandb: Run summary:
wandb: eval/macro_f1_score 0.14791
wandb: 
wandb: üöÄ View run R2-E2-ELFT-Merged Checkpoint 13 at: https://wandb.ai/jimbeno/Electra%20Large/runs/kzs98rks
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jimbeno/Electra%20Large
wandb: Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20250324_013143-kzs98rks/logs
TOTAL Time: 13m 12s


checkpoint_epoch_10_20250323-225412.pth



python ./finetune.py --dataset 'merged_local' --weights_name 'google/electra-large-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 24 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Large' --wandb_run 'R2-E2-ELFT-Merged Checkpoint 10 Final' --random_seed 123 --interactive --model_file 'checkpoint_epoch_10_20250323-225412.pth'



(nlp) ubuntu@159-54-167-99:~/nlp-west/sentiment$ python ./finetune.py --dataset 'merged_local' --weights_name 'google/electra-large-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 24 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Large' --wandb_run 'R2-E2-ELFT-Merged Checkpoint 10 Final' --random_seed 123 --interactive --model_file 'checkpoint_epoch_10_20250323-225412.pth'

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 5 - Device: cuda:5
Rank 3 - Device: cuda:3
Rank 7 - Device: cuda:7
Rank 6 - Device: cuda:6
Rank 2 - Device: cuda:2
Rank 4 - Device: cuda:4
Rank 1 - Device: cuda:1
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/ubuntu/nlp-west/sentiment/wandb/run-20250324_034646-0gshv84p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run R2-E2-ELFT-Merged Checkpoint 10 Final
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jimbeno/Electra%20Large
wandb: üöÄ View run at https://wandb.ai/jimbeno/Electra%20Large/runs/0gshv84p
Wand run initialized.

Initializing 'google/electra-large-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (4s)

Loading data...
Using the same dataset for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using merged_local 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Train size: 102097
Validation size: 5421
Evaluation size: 6530
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 2352
	       Neutral: 1829
	      Positive: 2349
Data loaded (333ms)

Processing data...
(Batch size: 16, Pooling: Mean, Fine Tune BERT: True, Chunk size: None)...
Extracting sentences and labels...

Data saved to: saves/data_8_gpu_20250324-034652.npz
X Train shape: [102097], X Validation shape: [5421], X Test shape: [6530]
y Train shape: [102097], y Validation shape: [5421], y Test shape: [6530]
Data processed (880ms)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 2, Max Grad Norm: None
Batch Size: 16, Max Epochs: 50, LR: 1e-05, Early Stop: score, Fine-tune BERT: True, Fine-tune Layers: 24, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_10_20250323-225412.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_10_20250323-225412.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (11s)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 10 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Initializing model, graph, optimizer...
BERT's original pooler removed. Using custom pooling type: mean
BERT has 334,092,288 trainable parameters and 0 non-trainable parameters
Number of BERT layers requiring gradients: 24 out of 24
Fine-tuning the last 24 out of 24 BERT layers

Model Architecture Summary:
BERTClassifier(
  (bert): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(30522, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0-23): 24 x ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (custom_pooling): PoolingLayer()
  (classifier): Classifier(
    (layers): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=1024, out_features=1024, bias=True)
      (4): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (5): Dropout(p=0.3, inplace=False)
      (6): Linear(in_features=1024, out_features=3, bias=True)
    )
  )
)

Model Parameters:
  bert.embeddings.word_embeddings: 31,254,528 (trainable)
  bert.embeddings.position_embeddings: 524,288 (trainable)
  bert.embeddings.token_type_embeddings: 2,048 (trainable)
  bert.embeddings.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.0.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.0.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.0.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.0.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.1.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.1.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.1.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.1.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.2.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.2.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.2.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.2.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.3.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.3.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.3.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.3.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.4.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.4.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.4.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.4.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.5.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.5.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.5.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.5.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.6.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.6.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.6.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.6.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.7.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.7.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.7.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.7.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.8.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.8.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.8.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.8.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.9.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.9.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.9.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.9.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.10.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.10.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.10.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.10.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.11.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.11.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.11.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.11.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.12.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.12.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.12.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.12.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.13.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.13.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.13.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.13.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.14.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.14.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.14.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.14.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.15.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.15.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.15.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.15.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.16.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.16.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.16.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.16.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.17.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.17.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.17.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.17.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.18.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.18.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.18.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.18.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.19.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.19.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.19.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.19.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.20.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.20.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.20.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.20.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.21.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.21.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.21.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.21.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.22.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.22.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.22.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.22.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.23.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.23.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.23.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.23.output.LayerNorm: 2,048 (trainable)
  classifier.layers.0: 1,049,600 (trainable)
  classifier.layers.1.projection: 2,099,200 (trainable)
  classifier.layers.3: 1,049,600 (trainable)
  classifier.layers.6: 3,075 (trainable)

Total layers: 200
Trainable layers: 200
Total parameters: 338,293,763
Trainable parameters: 338,293,763
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Layer-wise decay factor: 0.95
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 11, Max Iterations: 50, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 10

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: q
Quitting training...
Saving model in ONNX format...
Model saved in ONNX format to saves/model_20250324-035206.onnx and uploaded to Weights & Biases.
Saved model state: checkpoints/final_model_20250324-035220.pth
Saved model pickle: checkpoints/final_model_20250324-035220.pkl
Training completed (5m 35s)

Evaluating model...

R2-E2-ELFT-Merged Checkpoint 10 Final Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.874178  0.847789  0.860781      2352
     neutral   0.741715  0.770913  0.756032      1829
    positive   0.878194  0.877820  0.878007      2349

    accuracy                       0.837060      6530
   macro avg   0.831362  0.832174  0.831607      6530
weighted avg   0.838521  0.837060  0.837639      6530

ROC AUC: 0.947808

Predicted  negative  neutral  positive
Actual                                
negative       1994      268        90
neutral         223     1410       196
positive         64      223      2062

Saved predictions: saves/predictions_20250324-035733.csv

Macro F1 Score: 0.83

Evaluation completed (7m 19s)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: eval/macro_f1_score ‚ñÅ
wandb: 
wandb: Run summary:
wandb: eval/macro_f1_score 0.83161
wandb: 
wandb: üöÄ View run R2-E2-ELFT-Merged Checkpoint 10 Final at: https://wandb.ai/jimbeno/Electra%20Large/runs/0gshv84p
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jimbeno/Electra%20Large
wandb: Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20250324_034646-0gshv84p/logs
TOTAL Time: 13m 38s




python ./finetune.py --dataset 'merged_local' --eval_dataset 'dynasent_r1' --weights_name 'google/electra-large-discriminator' --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 24 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Large' --wandb_run 'R2-E2-ELFT-DYN-R1' --random_seed 123 --interactive --model_file 'checkpoint_epoch_10_20250323-225412.pth'



(nlp) ubuntu@159-54-167-99:~/nlp-west/sentiment$ python ./finetune.py --dataset 'merged_local' --eval_dataset 'dynasent_r1' --weights_name 'google/electra-large-discriminator' --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 24 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Large' --wandb_run 'R2-E2-ELFT-DYN-R1' --random_seed 123 --interactive --model_file 'checkpoint_epoch_10_20250323-225412.pth'

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 5 - Device: cuda:5
Rank 1 - Device: cuda:1
Rank 4 - Device: cuda:4
Rank 3 - Device: cuda:3
Rank 7 - Device: cuda:7
Rank 2 - Device: cuda:2
Rank 6 - Device: cuda:6
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/ubuntu/nlp-west/sentiment/wandb/run-20250324_054528-tzlok964
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run R2-E2-ELFT-DYN-R1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jimbeno/Electra%20Large
wandb: üöÄ View run at https://wandb.ai/jimbeno/Electra%20Large/runs/tzlok964
Wand run initialized.

Initializing 'google/electra-large-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (4s)

Loading data...
Using different datasets for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using dynasent_r1 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: DynaSent Round 1 from Hugging Face: 'dynabench/dynasent'
Downloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16.5k/16.5k [00:00<00:00, 25.2MB/s]
Downloading metadata: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.97k/6.97k [00:00<00:00, 26.2MB/s]
Downloading readme: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13.7k/13.7k [00:00<00:00, 33.8MB/s]
Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17.1M/17.1M [00:00<00:00, 73.2MB/s]
Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80488/80488 [00:21<00:00, 3808.18 examples/s]
Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3600/3600 [00:00<00:00, 3770.75 examples/s]
Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3600/3600 [00:00<00:00, 4704.42 examples/s]
Train size: 102097
Validation size: 5421
Evaluation size: 3600
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 1200
	       Neutral: 1200
	      Positive: 1200
Data loaded (26s)

Processing data...
(Batch size: 16, Pooling: Mean, Fine Tune BERT: True, Chunk size: None)...
Extracting sentences and labels...
X Train shape: [102097], X Validation shape: [5421], X Test shape: [3600]
y Train shape: [102097], y Validation shape: [5421], y Test shape: [3600]
Data processed (3ms)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 2, Max Grad Norm: None
Batch Size: 16, Max Epochs: 50, LR: 1e-05, Early Stop: score, Fine-tune BERT: True, Fine-tune Layers: 24, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_10_20250323-225412.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_10_20250323-225412.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (12s)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 10 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Initializing model, graph, optimizer...
BERT's original pooler removed. Using custom pooling type: mean
BERT has 334,092,288 trainable parameters and 0 non-trainable parameters
Number of BERT layers requiring gradients: 24 out of 24
Fine-tuning the last 24 out of 24 BERT layers

Model Architecture Summary:
BERTClassifier(
  (bert): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(30522, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0-23): 24 x ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (custom_pooling): PoolingLayer()
  (classifier): Classifier(
    (layers): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=1024, out_features=1024, bias=True)
      (4): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (5): Dropout(p=0.3, inplace=False)
      (6): Linear(in_features=1024, out_features=3, bias=True)
    )
  )
)

Model Parameters:
  bert.embeddings.word_embeddings: 31,254,528 (trainable)
  bert.embeddings.position_embeddings: 524,288 (trainable)
  bert.embeddings.token_type_embeddings: 2,048 (trainable)
  bert.embeddings.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.0.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.0.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.0.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.0.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.1.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.1.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.1.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.1.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.2.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.2.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.2.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.2.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.3.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.3.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.3.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.3.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.4.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.4.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.4.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.4.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.5.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.5.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.5.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.5.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.6.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.6.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.6.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.6.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.7.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.7.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.7.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.7.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.8.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.8.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.8.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.8.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.9.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.9.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.9.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.9.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.10.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.10.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.10.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.10.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.11.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.11.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.11.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.11.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.12.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.12.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.12.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.12.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.13.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.13.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.13.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.13.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.14.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.14.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.14.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.14.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.15.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.15.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.15.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.15.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.16.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.16.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.16.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.16.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.17.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.17.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.17.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.17.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.18.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.18.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.18.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.18.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.19.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.19.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.19.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.19.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.20.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.20.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.20.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.20.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.21.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.21.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.21.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.21.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.22.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.22.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.22.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.22.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.23.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.23.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.23.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.23.output.LayerNorm: 2,048 (trainable)
  classifier.layers.0: 1,049,600 (trainable)
  classifier.layers.1.projection: 2,099,200 (trainable)
  classifier.layers.3: 1,049,600 (trainable)
  classifier.layers.6: 3,075 (trainable)

Total layers: 200
Trainable layers: 200
Total parameters: 338,293,763
Trainable parameters: 338,293,763
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Layer-wise decay factor: 0.95
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 11, Max Iterations: 50, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 10

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: q
Quitting training...

Evaluating model...

R2-E2-ELFT-DYN-R1 Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.925512  0.828333  0.874230      1200
     neutral   0.781536  0.924167  0.846888      1200
    positive   0.911472  0.840833  0.874729      1200

    accuracy                       0.864444      3600
   macro avg   0.872840  0.864444  0.865283      3600
weighted avg   0.872840  0.864444  0.865283      3600

ROC AUC: 0.962647

Predicted  negative  neutral  positive
Actual                                
negative        994      159        47
neutral          40     1109        51
positive         40      151      1009

Saved predictions: saves/predictions_20250324-055304.csv

Macro F1 Score: 0.87

Evaluation completed (4m 2s)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: eval/macro_f1_score ‚ñÅ
wandb: 
wandb: Run summary:
wandb: eval/macro_f1_score 0.86528
wandb: 
wandb: üöÄ View run R2-E2-ELFT-DYN-R1 at: https://wandb.ai/jimbeno/Electra%20Large/runs/tzlok964
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jimbeno/Electra%20Large
wandb: Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250324_054528-tzlok964/logs
TOTAL Time: 9m 10s


python ./finetune.py --dataset 'merged_local' --eval_dataset 'dynasent_r2' --weights_name 'google/electra-large-discriminator' --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 24 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Large' --wandb_run 'R2-E2-ELFT-DYN-R2' --random_seed 123 --interactive --model_file 'checkpoint_epoch_10_20250323-225412.pth'


(nlp) ubuntu@159-54-167-99:~/nlp-west/sentiment$ python ./finetune.py --dataset 'merged_local' --eval_dataset 'dynasent_r2' --weights_name 'google/electra-large-discriminator' --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 24 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Large' --wandb_run 'R2-E2-ELFT-DYN-R2' --random_seed 123 --interactive --model_file 'checkpoint_epoch_10_20250323-225412.pth'

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 5 - Device: cuda:5
Rank 2 - Device: cuda:2
Rank 3 - Device: cuda:3
Rank 7 - Device: cuda:7
Rank 4 - Device: cuda:4
Rank 1 - Device: cuda:1
Rank 6 - Device: cuda:6
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/ubuntu/nlp-west/sentiment/wandb/run-20250324_072858-36ytjlfy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run R2-E2-ELFT-DYN-R2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jimbeno/Electra%20Large
wandb: üöÄ View run at https://wandb.ai/jimbeno/Electra%20Large/runs/36ytjlfy
Wand run initialized.

Initializing 'google/electra-large-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (4s)

Loading data...
Using different datasets for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using dynasent_r2 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: DynaSent Round 2 from Hugging Face: 'dynabench/dynasent'
Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13065/13065 [00:03<00:00, 4115.46 examples/s]
Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 720/720 [00:00<00:00, 4299.00 examples/s]
Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 720/720 [00:00<00:00, 4337.40 examples/s]
Train size: 102097
Validation size: 5421
Evaluation size: 720
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 240
	       Neutral: 240
	      Positive: 240
Data loaded (5s)

Processing data...
(Batch size: 16, Pooling: Mean, Fine Tune BERT: True, Chunk size: None)...
Extracting sentences and labels...
X Train shape: [102097], X Validation shape: [5421], X Test shape: [720]
y Train shape: [102097], y Validation shape: [5421], y Test shape: [720]
Data processed (2ms)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 2, Max Grad Norm: None
Batch Size: 16, Max Epochs: 50, LR: 1e-05, Early Stop: score, Fine-tune BERT: True, Fine-tune Layers: 24, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_10_20250323-225412.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_10_20250323-225412.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (12s)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 10 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Initializing model, graph, optimizer...
BERT's original pooler removed. Using custom pooling type: mean
BERT has 334,092,288 trainable parameters and 0 non-trainable parameters
Number of BERT layers requiring gradients: 24 out of 24
Fine-tuning the last 24 out of 24 BERT layers

Model Architecture Summary:
BERTClassifier(
  (bert): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(30522, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0-23): 24 x ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (custom_pooling): PoolingLayer()
  (classifier): Classifier(
    (layers): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=1024, out_features=1024, bias=True)
      (4): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (5): Dropout(p=0.3, inplace=False)
      (6): Linear(in_features=1024, out_features=3, bias=True)
    )
  )
)

Model Parameters:
  bert.embeddings.word_embeddings: 31,254,528 (trainable)
  bert.embeddings.position_embeddings: 524,288 (trainable)
  bert.embeddings.token_type_embeddings: 2,048 (trainable)
  bert.embeddings.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.0.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.0.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.0.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.0.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.1.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.1.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.1.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.1.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.2.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.2.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.2.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.2.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.3.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.3.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.3.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.3.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.4.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.4.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.4.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.4.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.5.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.5.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.5.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.5.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.6.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.6.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.6.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.6.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.7.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.7.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.7.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.7.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.8.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.8.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.8.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.8.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.9.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.9.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.9.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.9.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.10.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.10.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.10.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.10.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.11.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.11.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.11.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.11.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.12.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.12.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.12.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.12.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.13.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.13.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.13.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.13.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.14.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.14.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.14.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.14.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.15.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.15.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.15.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.15.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.16.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.16.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.16.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.16.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.17.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.17.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.17.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.17.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.18.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.18.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.18.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.18.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.19.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.19.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.19.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.19.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.20.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.20.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.20.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.20.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.21.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.21.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.21.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.21.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.22.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.22.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.22.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.22.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.23.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.23.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.23.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.23.output.LayerNorm: 2,048 (trainable)
  classifier.layers.0: 1,049,600 (trainable)
  classifier.layers.1.projection: 2,099,200 (trainable)
  classifier.layers.3: 1,049,600 (trainable)
  classifier.layers.6: 3,075 (trainable)

Total layers: 200
Trainable layers: 200
Total parameters: 338,293,763
Trainable parameters: 338,293,763
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Layer-wise decay factor: 0.95
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 11, Max Iterations: 50, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 10

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: q
Quitting training...

Evaluating model...

R2-E2-ELFT-DYN-R2 Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.791339  0.837500  0.813765       240
     neutral   0.803030  0.662500  0.726027       240
    positive   0.768657  0.858333  0.811024       240

    accuracy                       0.786111       720
   macro avg   0.787675  0.786111  0.783605       720
weighted avg   0.787675  0.786111  0.783605       720

ROC AUC: 0.932089

Predicted  negative  neutral  positive
Actual                                
negative        201       18        21
neutral          40      159        41
positive         13       21       206

Saved predictions: saves/predictions_20250324-073532.csv

Macro F1 Score: 0.78

Evaluation completed (49s)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: eval/macro_f1_score ‚ñÅ
wandb: 
wandb: Run summary:
wandb: eval/macro_f1_score 0.78361
wandb: 
wandb: üöÄ View run R2-E2-ELFT-DYN-R2 at: https://wandb.ai/jimbeno/Electra%20Large/runs/36ytjlfy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jimbeno/Electra%20Large
wandb: Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250324_072858-36ytjlfy/logs
TOTAL Time: 7m 3s



python ./finetune.py --dataset 'merged_local' --eval_dataset 'sst_local' --weights_name 'google/electra-large-discriminator' --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 24 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Large' --wandb_run 'R2-E2-ELFT-SST' --random_seed 123 --interactive --model_file 'checkpoint_epoch_10_20250323-225412.pth'


(nlp) ubuntu@159-54-167-99:~/nlp-west/sentiment$ python ./finetune.py --dataset 'merged_local' --eval_dataset 'sst_local' --weights_name 'google/electra-large-discriminator' --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 24 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Large' --wandb_run 'R2-E2-ELFT-SST' --random_seed 123 --interactive --model_file 'checkpoint_epoch_10_20250323-225412.pth'

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 6 - Device: cuda:6
Rank 3 - Device: cuda:3
Rank 4 - Device: cuda:4
Rank 1 - Device: cuda:1
Rank 2 - Device: cuda:2
Rank 7 - Device: cuda:7
Rank 5 - Device: cuda:5
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/ubuntu/nlp-west/sentiment/wandb/run-20250324_074656-4wb1ub63
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run R2-E2-ELFT-SST
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jimbeno/Electra%20Large
wandb: üöÄ View run at https://wandb.ai/jimbeno/Electra%20Large/runs/4wb1ub63
Wand run initialized.

Initializing 'google/electra-large-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (4s)

Loading data...
Using different datasets for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using sst_local 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: Stanford Sentiment Treebank (SST) from Local: 'data/sentiment'
Train size: 102097
Validation size: 5421
Evaluation size: 2210
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 912
	       Neutral: 389
	      Positive: 909
Data loaded (292ms)

Processing data...
(Batch size: 16, Pooling: Mean, Fine Tune BERT: True, Chunk size: None)...
Extracting sentences and labels...
X Train shape: [102097], X Validation shape: [5421], X Test shape: [2210]
y Train shape: [102097], y Validation shape: [5421], y Test shape: [2210]
Data processed (3ms)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 2, Max Grad Norm: None
Batch Size: 16, Max Epochs: 50, LR: 1e-05, Early Stop: score, Fine-tune BERT: True, Fine-tune Layers: 24, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_10_20250323-225412.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_10_20250323-225412.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (12s)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 10 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Initializing model, graph, optimizer...
BERT's original pooler removed. Using custom pooling type: mean
BERT has 334,092,288 trainable parameters and 0 non-trainable parameters
Number of BERT layers requiring gradients: 24 out of 24
Fine-tuning the last 24 out of 24 BERT layers

Model Architecture Summary:
BERTClassifier(
  (bert): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(30522, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (token_type_embeddings): Embedding(2, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0-23): 24 x ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (custom_pooling): PoolingLayer()
  (classifier): Classifier(
    (layers): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=1024, out_features=1024, bias=True)
      (4): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (5): Dropout(p=0.3, inplace=False)
      (6): Linear(in_features=1024, out_features=3, bias=True)
    )
  )
)

Model Parameters:
  bert.embeddings.word_embeddings: 31,254,528 (trainable)
  bert.embeddings.position_embeddings: 524,288 (trainable)
  bert.embeddings.token_type_embeddings: 2,048 (trainable)
  bert.embeddings.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.0.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.0.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.0.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.0.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.0.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.1.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.1.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.1.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.1.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.1.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.2.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.2.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.2.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.2.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.2.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.3.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.3.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.3.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.3.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.3.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.4.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.4.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.4.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.4.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.4.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.5.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.5.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.5.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.5.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.5.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.6.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.6.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.6.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.6.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.6.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.7.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.7.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.7.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.7.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.7.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.8.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.8.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.8.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.8.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.8.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.9.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.9.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.9.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.9.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.9.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.10.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.10.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.10.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.10.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.10.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.11.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.11.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.11.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.11.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.11.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.12.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.12.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.12.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.12.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.12.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.13.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.13.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.13.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.13.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.13.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.14.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.14.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.14.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.14.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.14.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.15.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.15.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.15.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.15.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.15.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.16.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.16.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.16.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.16.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.16.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.17.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.17.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.17.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.17.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.17.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.18.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.18.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.18.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.18.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.18.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.19.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.19.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.19.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.19.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.19.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.20.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.20.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.20.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.20.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.20.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.21.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.21.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.21.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.21.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.21.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.22.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.22.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.22.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.22.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.22.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.23.attention.self.query: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.self.key: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.self.value: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.output.dense: 1,049,600 (trainable)
  bert.encoder.layer.23.attention.output.LayerNorm: 2,048 (trainable)
  bert.encoder.layer.23.intermediate.dense: 4,198,400 (trainable)
  bert.encoder.layer.23.output.dense: 4,195,328 (trainable)
  bert.encoder.layer.23.output.LayerNorm: 2,048 (trainable)
  classifier.layers.0: 1,049,600 (trainable)
  classifier.layers.1.projection: 2,099,200 (trainable)
  classifier.layers.3: 1,049,600 (trainable)
  classifier.layers.6: 3,075 (trainable)

Total layers: 200
Trainable layers: 200
Total parameters: 338,293,763
Trainable parameters: 338,293,763
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Layer-wise decay factor: 0.95
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 11, Max Iterations: 50, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 10

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: q
Quitting training...

Evaluating model...

R2-E2-ELFT-SST Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.838405  0.876096  0.856836       912
     neutral   0.500000  0.365039  0.421991       389
    positive   0.870504  0.931793  0.900106       909

    accuracy                       0.809050      2210
   macro avg   0.736303  0.724309  0.726311      2210
weighted avg   0.792042  0.809050  0.798093      2210

ROC AUC: 0.905255

Predicted  negative  neutral  positive
Actual                                
negative        799       91        22
neutral         143      142       104
positive         11       51       847

Saved predictions: saves/predictions_20250324-075248.csv

Macro F1 Score: 0.73

Evaluation completed (2m 29s)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: eval/macro_f1_score ‚ñÅ
wandb: 
wandb: Run summary:
wandb: eval/macro_f1_score 0.72631
wandb: 
wandb: üöÄ View run R2-E2-ELFT-SST at: https://wandb.ai/jimbeno/Electra%20Large/runs/4wb1ub63
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jimbeno/Electra%20Large
wandb: Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250324_074656-4wb1ub63/logs
TOTAL Time: 6m 55s





python ./finetune.py --dataset 'merged_local' --weights_name 'google/electra-large-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 24 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Large' --wandb_run 'R2-E2-ELFT-Merged Checkpoint 10 Final HF Save' --random_seed 123 --interactive --model_file 'checkpoint_epoch_10_20250323-225412.pth' --save_hf
















