Last login: Tue Oct 15 23:32:54 on ttys006

The default interactive shell is now zsh.
To update your account to use zsh, please run `chsh -s /bin/zsh`.
For more details, please visit https://support.apple.com/kb/HT208050.
(base) Jims-MBP:~ jim$ ssh ubuntu@104.171.202.248
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 6.2.0-37-generic x86_64)
 .============.
 ||   __      ||    _                    _         _
 ||   \_\     ||   | |    __ _ _ __ ___ | |__   __| | __ _
 ||    \_\    ||   | |   / _` | '_ ` _ \| '_ \ / _` |/ _` |
 ||   /_λ_\   ||   | |__| (_| | | | | | | |_) | (_| | (_| |
 ||  /_/ \_\  ||   |_____\__,_|_| |_| |_|_.__/ \__,_|\__,_|
  .============.                                  GPU CLOUD

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Thu Oct 17 01:45:27 UTC 2024

  System load:  66.0048828125    Processes:                887
  Usage of /:   0.4% of 5.68TB   Users logged in:          1
  Memory usage: 3%               IPv4 address for docker0: 172.17.0.1
  Swap usage:   0%               IPv4 address for eno1:    104.171.202.248


Expanded Security Maintenance for Applications is not enabled.

4 updates can be applied immediately.
4 of these updates are standard security updates.
To see these additional updates run: apt list --upgradable

17 additional security updates can be applied with ESM Apps.
Learn more about enabling ESM Apps service at https://ubuntu.com/esm


The list of available updates is more than a week old.
To check for new updates run: sudo apt update
New release '24.04.1 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Thu Oct 17 01:35:58 2024 from 69.209.27.147
ubuntu@104-171-202-248:~$ ls
nlp-test
ubuntu@104-171-202-248:~$ watch -n0.1 nvidia-smi









ubuntu@104-171-202-248:~$ watch -n0.1 nvidia-smi
ubuntu@104-171-202-248:~$ logout
Connection to 104.171.202.248 closed.
(base) Jims-MBP:~ jim$ ssh ubuntu@104.171.203.144
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 6.2.0-37-generic x86_64)
 .============.
 ||   __      ||    _                    _         _
 ||   \_\     ||   | |    __ _ _ __ ___ | |__   __| | __ _
 ||    \_\    ||   | |   / _` | '_ ` _ \| '_ \ / _` |/ _` |
 ||   /_λ_\   ||   | |__| (_| | | | | | | |_) | (_| | (_| |
 ||  /_/ \_\  ||   |_____\__,_|_| |_| |_|_.__/ \__,_|\__,_|
  .============.                                  GPU CLOUD

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Fri Oct 18 04:58:27 UTC 2024

  System load:  0.41455078125    Processes:                852
  Usage of /:   0.4% of 5.68TB   Users logged in:          1
  Memory usage: 0%               IPv4 address for docker0: 172.17.0.1
  Swap usage:   0%               IPv4 address for eno1:    104.171.203.144


Expanded Security Maintenance for Applications is not enabled.

4 updates can be applied immediately.
4 of these updates are standard security updates.
To see these additional updates run: apt list --upgradable

17 additional security updates can be applied with ESM Apps.
Learn more about enabling ESM Apps service at https://ubuntu.com/esm


The list of available updates is more than a week old.
To check for new updates run: sudo apt update
New release '24.04.1 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Fri Oct 18 04:55:44 2024 from 69.209.27.147
ubuntu@104-171-203-144:~$ watch -n0.1 nvidia-smi
ubuntu@104-171-203-144:~$ logout
Connection to 104.171.203.144 closed.
(base) Jims-MBP:~ jim$ ssh ubuntu@104.171.202.253
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 6.2.0-37-generic x86_64)
 .============.
 ||   __      ||    _                    _         _
 ||   \_\     ||   | |    __ _ _ __ ___ | |__   __| | __ _
 ||    \_\    ||   | |   / _` | '_ ` _ \| '_ \ / _` |/ _` |
 ||   /_λ_\   ||   | |__| (_| | | | | | | |_) | (_| | (_| |
 ||  /_/ \_\  ||   |_____\__,_|_| |_| |_|_.__/ \__,_|\__,_|
  .============.                                  GPU CLOUD

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Sat Oct 19 04:15:07 UTC 2024

  System load:  0.2880859375     Processes:                907
  Usage of /:   0.4% of 5.68TB   Users logged in:          1
  Memory usage: 0%               IPv4 address for docker0: 172.17.0.1
  Swap usage:   0%               IPv4 address for eno1:    104.171.202.253


Expanded Security Maintenance for Applications is not enabled.

4 updates can be applied immediately.
4 of these updates are standard security updates.
To see these additional updates run: apt list --upgradable

17 additional security updates can be applied with ESM Apps.
Learn more about enabling ESM Apps service at https://ubuntu.com/esm


The list of available updates is more than a week old.
To check for new updates run: sudo apt update
New release '24.04.1 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Sat Oct 19 04:13:00 2024 from 69.209.27.147
ubuntu@104-171-202-253:~$ watch -n0.1 nvidia-smi
ubuntu@104-171-202-253:~$ logout
Connection to 104.171.202.253 closed.
(base) Jims-MBP:~ jim$ pwd
/Users/jim
(base) Jims-MBP:~ jim$ cd ~/Desktop/
(base) Jims-MBP:Desktop jim$ ls
Amazon.com - Order 114-3715622-3234639.pdf
File Me
Gorilla Mat Order.pdf
JIM_8241.jpg
JIM_8267.jpg
JIM_8330.jpg
JIM_8624.jpg
JIM_8659.jpg
JIM_8675.jpg
JIM_8692.jpg
JIM_8717.jpg
Post-Helene-Flooding-Swannanoa-10.jpeg
Project work on triple classifier.txt
RosterTail_Signup.pdf
Screen Recording 2024-09-12 at 4.40.58 PM.mov
Screen Recording 2024-09-12 at 5.03.21 PM.mov
Screen Recording 2024-09-12 at 5.13.46 PM.mov
Screen Recording 2024-09-12 at 5.16.14 PM.mov
Screen Recording 2024-09-12 at 5.27.47 PM.mov
Screen Recording 2024-10-01 at 6.54.25 PM.mov
Screen Shot 2024-08-26 at 11.18.38 PM.png
Screen Shot 2024-08-27 at 6.49.53 PM.png
Screen Shot 2024-08-27 at 6.50.04 PM.png
Screen Shot 2024-08-27 at 6.50.11 PM.png
Screen Shot 2024-08-29 at 11.44.37 PM.png
Screen Shot 2024-08-29 at 3.47.42 PM.png
Screen Shot 2024-08-29 at 6.20.08 PM.png
Screen Shot 2024-09-02 at 10.00.14 PM.png
Screen Shot 2024-09-02 at 10.00.26 PM.png
Screen Shot 2024-09-02 at 11.40.14 PM.png
Screen Shot 2024-09-02 at 11.58.31 PM.png
Screen Shot 2024-09-03 at 12.01.04 AM.png
Screen Shot 2024-09-03 at 12.50.14 AM.png
Screen Shot 2024-09-03 at 8.17.11 AM.png
Screen Shot 2024-09-04 at 3.35.34 PM.png
Screen Shot 2024-09-04 at 4.22.17 PM.png
Screen Shot 2024-09-05 at 1.59.02 PM.png
Screen Shot 2024-09-05 at 1.59.05 PM.png
Screen Shot 2024-09-05 at 1.59.08 PM.png
Screen Shot 2024-09-05 at 1.59.10 PM.png
Screen Shot 2024-09-05 at 1.59.13 PM.png
Screen Shot 2024-09-05 at 1.59.16 PM.png
Screen Shot 2024-09-05 at 1.59.19 PM.png
Screen Shot 2024-09-05 at 1.59.21 PM.png
Screen Shot 2024-09-05 at 1.59.23 PM.png
Screen Shot 2024-09-05 at 1.59.25 PM.png
Screen Shot 2024-09-05 at 1.59.28 PM.png
Screen Shot 2024-09-05 at 1.59.31 PM.png
Screen Shot 2024-09-05 at 2.01.10 PM.png
Screen Shot 2024-09-05 at 2.01.25 PM.png
Screen Shot 2024-09-06 at 3.58.00 PM.png
Screen Shot 2024-09-06 at 8.19.04 AM.png
Screen Shot 2024-09-06 at 9.14.18 PM.png
Screen Shot 2024-09-07 at 10.51.06 AM.png
Screen Shot 2024-09-07 at 11.59.33 AM.png
Screen Shot 2024-09-07 at 3.20.56 PM.png
Screen Shot 2024-09-09 at 3.30.07 PM.png
Screen Shot 2024-09-09 at 9.32.15 AM.png
Screen Shot 2024-09-10 at 4.41.37 PM.png
Screen Shot 2024-09-11 at 8.50.46 PM.png
Screen Shot 2024-09-11 at 8.51.23 PM.png
Screen Shot 2024-09-11 at 8.53.50 PM.png
Screen Shot 2024-09-12 at 4.32.23 PM.png
Screen Shot 2024-09-12 at 4.41.51 PM.png
Screen Shot 2024-09-12 at 4.42.05 PM.png
Screen Shot 2024-09-12 at 4.54.00 PM.png
Screen Shot 2024-09-12 at 4.57.03 PM.png
Screen Shot 2024-09-12 at 8.14.32 PM.png
Screen Shot 2024-09-12 at 8.15.17 PM.png
Screen Shot 2024-09-12 at 8.43.43 PM.png
Screen Shot 2024-09-13 at 10.09.09 PM.png
Screen Shot 2024-09-15 at 10.20.05 AM.png
Screen Shot 2024-09-15 at 10.20.27 AM.png
Screen Shot 2024-09-17 at 8.08.34 PM.png
Screen Shot 2024-09-19 at 10.20.54 PM.png
Screen Shot 2024-09-19 at 10.23.27 PM.png
Screen Shot 2024-09-19 at 10.37.55 PM.png
Screen Shot 2024-09-20 at 2.30.59 PM.png
Screen Shot 2024-09-20 at 9.04.07 AM.png
Screen Shot 2024-09-22 at 1.06.50 AM.png
Screen Shot 2024-09-22 at 12.05.17 AM.png
Screen Shot 2024-09-22 at 12.09.41 AM.png
Screen Shot 2024-09-22 at 12.45.28 AM.png
Screen Shot 2024-09-22 at 12.45.40 AM.png
Screen Shot 2024-09-22 at 12.48.43 AM.png
Screen Shot 2024-09-22 at 12.48.48 AM.png
Screen Shot 2024-09-22 at 12.48.53 AM.png
Screen Shot 2024-09-22 at 12.48.58 AM.png
Screen Shot 2024-09-22 at 12.49.21 AM.png
Screen Shot 2024-09-23 at 1.08.09 PM.png
Screen Shot 2024-09-23 at 1.08.17 PM.png
Screen Shot 2024-09-23 at 1.08.40 PM.png
Screen Shot 2024-09-23 at 1.08.47 PM.png
Screen Shot 2024-09-23 at 1.42.42 PM.png
Screen Shot 2024-09-23 at 5.43.21 PM.png
Screen Shot 2024-09-23 at 5.52.16 PM.png
Screen Shot 2024-09-23 at 7.45.54 PM.png
Screen Shot 2024-09-23 at 7.45.58 PM.png
Screen Shot 2024-09-23 at 7.46.02 PM.png
Screen Shot 2024-09-23 at 7.46.07 PM.png
Screen Shot 2024-09-23 at 7.48.02 PM.png
Screen Shot 2024-09-24 at 4.58.20 PM.png
Screen Shot 2024-09-24 at 9.32.08 PM.png
Screen Shot 2024-09-26 at 11.44.38 PM.png
Screen Shot 2024-09-26 at 11.44.41 PM.png
Screen Shot 2024-09-26 at 11.44.47 PM.png
Screen Shot 2024-09-26 at 11.44.51 PM.png
Screen Shot 2024-09-26 at 11.44.54 PM.png
Screen Shot 2024-09-26 at 11.44.57 PM.png
Screen Shot 2024-09-30 at 11.40.52 PM.png
Screen Shot 2024-10-01 at 6.21.45 PM.png
Screen Shot 2024-10-01 at 6.23.05 PM.png
Screen Shot 2024-10-02 at 9.26.57 PM.png
Screen Shot 2024-10-04 at 3.31.43 PM.png
Screen Shot 2024-10-05 at 9.54.19 PM.png
Screen Shot 2024-10-07 at 10.44.16 AM.png
Screen Shot 2024-10-07 at 10.44.18 AM.png
Screen Shot 2024-10-07 at 7.46.53 PM.png
Screen Shot 2024-10-07 at 8.04.56 PM.png
Screen Shot 2024-10-07 at 8.51.50 AM.png
Screen Shot 2024-10-08 at 11.26.44 PM.png
Screen Shot 2024-10-08 at 12.35.33 AM.png
Screen Shot 2024-10-09 at 1.20.44 PM.png
Screen Shot 2024-10-09 at 1.44.03 PM.png
Screen Shot 2024-10-09 at 1.44.10 PM.png
Screen Shot 2024-10-12 at 3.06.29 PM.png
Screen Shot 2024-10-13 at 11.43.30 AM.png
Screen Shot 2024-10-13 at 9.00.33 AM.png
Screen Shot 2024-10-13 at 9.01.10 AM.png
Screen Shot 2024-10-13 at 9.51.12 AM.png
Screen Shot 2024-10-13 at 9.53.55 AM.png
Screen Shot 2024-10-14 at 12.52.41 AM.png
Screen Shot 2024-10-14 at 4.42.45 PM.png
Screen Shot 2024-10-14 at 4.44.10 PM.png
Screen Shot 2024-10-16 at 4.37.50 PM.png
Screen Shot 2024-10-16 at 5.42.46 PM.png
Screen Shot 2024-10-16 at 5.42.48 PM.png
Screen Shot 2024-10-17 at 9.34.43 PM.png
Screen Shot 2024-10-18 at 10.21.36 PM.png
Screen Shot 2024-10-18 at 10.21.44 PM.png
Screen Shot 2024-10-18 at 10.21.46 PM.png
Screen Shot 2024-10-18 at 10.21.49 PM.png
Screen Shot 2024-10-18 at 7.34.58 PM.png
assignment_1_winner.txt
assignment_3_recogs.txt
checkpoint_epoch_100_20240903-132818.pth
checkpoint_epoch_10_20240902-082054.pth
checkpoint_epoch_10_20240902-202511.pth
checkpoint_epoch_10_20240903-002113.pth
checkpoint_epoch_110_20240903-164222.pth
checkpoint_epoch_11_20240902-203344.pth
checkpoint_epoch_12_20240902-204217.pth
checkpoint_epoch_13_20240902-205051.pth
checkpoint_epoch_14_20240902-205926.pth
checkpoint_epoch_15_20240903-010402.pth
checkpoint_epoch_15_20240904-061415.pth
checkpoint_epoch_1_20240902-190757.pth
checkpoint_epoch_20_20240903-014650.pth
checkpoint_epoch_20_20240904-065003.pth
checkpoint_epoch_25_20240903-022939.pth
checkpoint_epoch_2_20240902-191631.pth
checkpoint_epoch_3_20240902-192504.pth
checkpoint_epoch_4_20240902-193338.pth
checkpoint_epoch_50_20240903-204948.pth
checkpoint_epoch_55_20240903-074504.pth
checkpoint_epoch_5_20240902-075512.pth
checkpoint_epoch_5_20240902-194212.pth
checkpoint_epoch_6_20240902-080020.pth
checkpoint_epoch_6_20240902-195048.pth
checkpoint_epoch_7_20240902-080527.pth
checkpoint_epoch_7_20240902-195925.pth
checkpoint_epoch_8_20240902-081035.pth
checkpoint_epoch_8_20240902-200759.pth
checkpoint_epoch_9_20240902-081543.pth
checkpoint_epoch_9_20240902-201632.pth
cmbs_diagram.png
cs224u assignment 1 notes.txt
cs224u assignment 3 notes.txt
cs224u-openqa-bakeoff-entry.json
cs224u-recogs-bakeoff-entry-bak.tsv
cs224u-recogs-bakeoff-entry.tsv
cs224u-sentiment-bakeoff-entry.csv
ddp_sentiment_finetune.py
final_model_20240830-063811.pth
final_model_20240830-064147.pth
final_model_20240831-060612.pth
final_model_20240902-083334.pkl
final_model_20240902-083334.pth
final_model_20240902-084234.pkl
final_model_20240902-084234.pth
final_model_20240903-031320.pkl
final_model_20240903-031320.pth
final_model_20240903-064116.pkl
final_model_20240903-064116.pth
final_model_20240903-074554.pkl
final_model_20240903-074554.pth
final_model_20240903-132823.pkl
final_model_20240903-164225.pkl
final_model_20240903-164225.pth
final_model_20240904-053342.pkl
final_model_20240904-053342.pth
hw_openqa old.ipynb
hw_openqa.ipynb
hw_recogs.ipynb
hw_sentiment.ipynb
inverse_examples.tsv
loss_data.csv
model_20240831-060612.pkl
other_model
predictions_20240830-063901.csv
predictions_20240830-064237.csv
predictions_20240831-060705.csv
predictions_20240902-084326.csv
predictions_20240903-074651.csv
predictions_20240903-132923.csv
predictions_20240903-164324.csv
predictions_20240904-053439.csv
predictions_20240904-070656.csv
query_create_artifact.ipynb
recogs_assignment3
recogs_assignment3.zip
retention-Copy1 copy-Copy2.ipynb
roberta_model
sentiment
sentiment.zip
torch_ddp_finetune_neural_classifier.py
train.tsv
train_sample.tsv
two_are_better_than_one.txt
utils.py
(base) Jims-MBP:Desktop jim$ ls
Amazon.com - Order 114-3715622-3234639.pdf
File Me
Gorilla Mat Order.pdf
JIM_8241.jpg
JIM_8267.jpg
JIM_8330.jpg
JIM_8624.jpg
JIM_8659.jpg
JIM_8675.jpg
JIM_8692.jpg
JIM_8717.jpg
Post-Helene-Flooding-Swannanoa-10.jpeg
Project work on triple classifier.txt
RosterTail_Signup.pdf
Screen Recording 2024-09-12 at 4.40.58 PM.mov
Screen Recording 2024-09-12 at 5.03.21 PM.mov
Screen Recording 2024-09-12 at 5.13.46 PM.mov
Screen Recording 2024-09-12 at 5.16.14 PM.mov
Screen Recording 2024-09-12 at 5.27.47 PM.mov
Screen Recording 2024-10-01 at 6.54.25 PM.mov
Screen Shot 2024-08-26 at 11.18.38 PM.png
Screen Shot 2024-08-27 at 6.49.53 PM.png
Screen Shot 2024-08-27 at 6.50.04 PM.png
Screen Shot 2024-08-27 at 6.50.11 PM.png
Screen Shot 2024-08-29 at 11.44.37 PM.png
Screen Shot 2024-08-29 at 3.47.42 PM.png
Screen Shot 2024-08-29 at 6.20.08 PM.png
Screen Shot 2024-09-02 at 10.00.14 PM.png
Screen Shot 2024-09-02 at 10.00.26 PM.png
Screen Shot 2024-09-02 at 11.40.14 PM.png
Screen Shot 2024-09-02 at 11.58.31 PM.png
Screen Shot 2024-09-03 at 12.01.04 AM.png
Screen Shot 2024-09-03 at 12.50.14 AM.png
Screen Shot 2024-09-03 at 8.17.11 AM.png
Screen Shot 2024-09-04 at 3.35.34 PM.png
Screen Shot 2024-09-04 at 4.22.17 PM.png
Screen Shot 2024-09-05 at 1.59.02 PM.png
Screen Shot 2024-09-05 at 1.59.05 PM.png
Screen Shot 2024-09-05 at 1.59.08 PM.png
Screen Shot 2024-09-05 at 1.59.10 PM.png
Screen Shot 2024-09-05 at 1.59.13 PM.png
Screen Shot 2024-09-05 at 1.59.16 PM.png
Screen Shot 2024-09-05 at 1.59.19 PM.png
Screen Shot 2024-09-05 at 1.59.21 PM.png
Screen Shot 2024-09-05 at 1.59.23 PM.png
Screen Shot 2024-09-05 at 1.59.25 PM.png
Screen Shot 2024-09-05 at 1.59.28 PM.png
Screen Shot 2024-09-05 at 1.59.31 PM.png
Screen Shot 2024-09-05 at 2.01.10 PM.png
Screen Shot 2024-09-05 at 2.01.25 PM.png
Screen Shot 2024-09-06 at 3.58.00 PM.png
Screen Shot 2024-09-06 at 8.19.04 AM.png
Screen Shot 2024-09-06 at 9.14.18 PM.png
Screen Shot 2024-09-07 at 10.51.06 AM.png
Screen Shot 2024-09-07 at 11.59.33 AM.png
Screen Shot 2024-09-07 at 3.20.56 PM.png
Screen Shot 2024-09-09 at 3.30.07 PM.png
Screen Shot 2024-09-09 at 9.32.15 AM.png
Screen Shot 2024-09-10 at 4.41.37 PM.png
Screen Shot 2024-09-11 at 8.50.46 PM.png
Screen Shot 2024-09-11 at 8.51.23 PM.png
Screen Shot 2024-09-11 at 8.53.50 PM.png
Screen Shot 2024-09-12 at 4.32.23 PM.png
Screen Shot 2024-09-12 at 4.41.51 PM.png
Screen Shot 2024-09-12 at 4.42.05 PM.png
Screen Shot 2024-09-12 at 4.54.00 PM.png
Screen Shot 2024-09-12 at 4.57.03 PM.png
Screen Shot 2024-09-12 at 8.14.32 PM.png
Screen Shot 2024-09-12 at 8.15.17 PM.png
Screen Shot 2024-09-12 at 8.43.43 PM.png
Screen Shot 2024-09-13 at 10.09.09 PM.png
Screen Shot 2024-09-15 at 10.20.05 AM.png
Screen Shot 2024-09-15 at 10.20.27 AM.png
Screen Shot 2024-09-17 at 8.08.34 PM.png
Screen Shot 2024-09-19 at 10.20.54 PM.png
Screen Shot 2024-09-19 at 10.23.27 PM.png
Screen Shot 2024-09-19 at 10.37.55 PM.png
Screen Shot 2024-09-20 at 2.30.59 PM.png
Screen Shot 2024-09-20 at 9.04.07 AM.png
Screen Shot 2024-09-22 at 1.06.50 AM.png
Screen Shot 2024-09-22 at 12.05.17 AM.png
Screen Shot 2024-09-22 at 12.09.41 AM.png
Screen Shot 2024-09-22 at 12.45.28 AM.png
Screen Shot 2024-09-22 at 12.45.40 AM.png
Screen Shot 2024-09-22 at 12.48.43 AM.png
Screen Shot 2024-09-22 at 12.48.48 AM.png
Screen Shot 2024-09-22 at 12.48.53 AM.png
Screen Shot 2024-09-22 at 12.48.58 AM.png
Screen Shot 2024-09-22 at 12.49.21 AM.png
Screen Shot 2024-09-23 at 1.08.09 PM.png
Screen Shot 2024-09-23 at 1.08.17 PM.png
Screen Shot 2024-09-23 at 1.08.40 PM.png
Screen Shot 2024-09-23 at 1.08.47 PM.png
Screen Shot 2024-09-23 at 1.42.42 PM.png
Screen Shot 2024-09-23 at 5.43.21 PM.png
Screen Shot 2024-09-23 at 5.52.16 PM.png
Screen Shot 2024-09-23 at 7.45.54 PM.png
Screen Shot 2024-09-23 at 7.45.58 PM.png
Screen Shot 2024-09-23 at 7.46.02 PM.png
Screen Shot 2024-09-23 at 7.46.07 PM.png
Screen Shot 2024-09-23 at 7.48.02 PM.png
Screen Shot 2024-09-24 at 4.58.20 PM.png
Screen Shot 2024-09-24 at 9.32.08 PM.png
Screen Shot 2024-09-26 at 11.44.38 PM.png
Screen Shot 2024-09-26 at 11.44.41 PM.png
Screen Shot 2024-09-26 at 11.44.47 PM.png
Screen Shot 2024-09-26 at 11.44.51 PM.png
Screen Shot 2024-09-26 at 11.44.54 PM.png
Screen Shot 2024-09-26 at 11.44.57 PM.png
Screen Shot 2024-09-30 at 11.40.52 PM.png
Screen Shot 2024-10-01 at 6.21.45 PM.png
Screen Shot 2024-10-01 at 6.23.05 PM.png
Screen Shot 2024-10-02 at 9.26.57 PM.png
Screen Shot 2024-10-04 at 3.31.43 PM.png
Screen Shot 2024-10-05 at 9.54.19 PM.png
Screen Shot 2024-10-07 at 10.44.16 AM.png
Screen Shot 2024-10-07 at 10.44.18 AM.png
Screen Shot 2024-10-07 at 7.46.53 PM.png
Screen Shot 2024-10-07 at 8.04.56 PM.png
Screen Shot 2024-10-07 at 8.51.50 AM.png
Screen Shot 2024-10-08 at 11.26.44 PM.png
Screen Shot 2024-10-08 at 12.35.33 AM.png
Screen Shot 2024-10-09 at 1.20.44 PM.png
Screen Shot 2024-10-09 at 1.44.03 PM.png
Screen Shot 2024-10-09 at 1.44.10 PM.png
Screen Shot 2024-10-12 at 3.06.29 PM.png
Screen Shot 2024-10-13 at 11.43.30 AM.png
Screen Shot 2024-10-13 at 9.00.33 AM.png
Screen Shot 2024-10-13 at 9.01.10 AM.png
Screen Shot 2024-10-13 at 9.51.12 AM.png
Screen Shot 2024-10-13 at 9.53.55 AM.png
Screen Shot 2024-10-14 at 12.52.41 AM.png
Screen Shot 2024-10-14 at 4.42.45 PM.png
Screen Shot 2024-10-14 at 4.44.10 PM.png
Screen Shot 2024-10-16 at 4.37.50 PM.png
Screen Shot 2024-10-16 at 5.42.46 PM.png
Screen Shot 2024-10-16 at 5.42.48 PM.png
Screen Shot 2024-10-17 at 9.34.43 PM.png
Screen Shot 2024-10-18 at 10.21.36 PM.png
Screen Shot 2024-10-18 at 10.21.44 PM.png
Screen Shot 2024-10-18 at 10.21.46 PM.png
Screen Shot 2024-10-18 at 10.21.49 PM.png
Screen Shot 2024-10-18 at 7.34.58 PM.png
assignment_1_winner.txt
assignment_3_recogs.txt
checkpoint_epoch_100_20240903-132818.pth
checkpoint_epoch_10_20240902-082054.pth
checkpoint_epoch_10_20240902-202511.pth
checkpoint_epoch_10_20240903-002113.pth
checkpoint_epoch_110_20240903-164222.pth
checkpoint_epoch_11_20240902-203344.pth
checkpoint_epoch_12_20240902-204217.pth
checkpoint_epoch_13_20240902-205051.pth
checkpoint_epoch_14_20240902-205926.pth
checkpoint_epoch_15_20240903-010402.pth
checkpoint_epoch_15_20240904-061415.pth
checkpoint_epoch_1_20240902-190757.pth
checkpoint_epoch_20_20240903-014650.pth
checkpoint_epoch_20_20240904-065003.pth
checkpoint_epoch_25_20240903-022939.pth
checkpoint_epoch_2_20240902-191631.pth
checkpoint_epoch_3_20240902-192504.pth
checkpoint_epoch_4_20240902-193338.pth
checkpoint_epoch_50_20240903-204948.pth
checkpoint_epoch_55_20240903-074504.pth
checkpoint_epoch_5_20240902-075512.pth
checkpoint_epoch_5_20240902-194212.pth
checkpoint_epoch_6_20240902-080020.pth
checkpoint_epoch_6_20240902-195048.pth
checkpoint_epoch_7_20240902-080527.pth
checkpoint_epoch_7_20240902-195925.pth
checkpoint_epoch_8_20240902-081035.pth
checkpoint_epoch_8_20240902-200759.pth
checkpoint_epoch_9_20240902-081543.pth
checkpoint_epoch_9_20240902-201632.pth
cmbs_diagram.png
cs224u assignment 1 notes.txt
cs224u assignment 3 notes.txt
cs224u-openqa-bakeoff-entry.json
cs224u-recogs-bakeoff-entry-bak.tsv
cs224u-recogs-bakeoff-entry.tsv
cs224u-sentiment-bakeoff-entry.csv
ddp_sentiment_finetune.py
final_model_20240830-063811.pth
final_model_20240830-064147.pth
final_model_20240831-060612.pth
final_model_20240902-083334.pkl
final_model_20240902-083334.pth
final_model_20240902-084234.pkl
final_model_20240902-084234.pth
final_model_20240903-031320.pkl
final_model_20240903-031320.pth
final_model_20240903-064116.pkl
final_model_20240903-064116.pth
final_model_20240903-074554.pkl
final_model_20240903-074554.pth
final_model_20240903-132823.pkl
final_model_20240903-164225.pkl
final_model_20240903-164225.pth
final_model_20240904-053342.pkl
final_model_20240904-053342.pth
hw_openqa old.ipynb
hw_openqa.ipynb
hw_recogs.ipynb
hw_sentiment.ipynb
inverse_examples.tsv
loss_data.csv
model_20240831-060612.pkl
other_model
predictions_20240830-063901.csv
predictions_20240830-064237.csv
predictions_20240831-060705.csv
predictions_20240902-084326.csv
predictions_20240903-074651.csv
predictions_20240903-132923.csv
predictions_20240903-164324.csv
predictions_20240904-053439.csv
predictions_20240904-070656.csv
query_create_artifact.ipynb
recogs_assignment3
recogs_assignment3.zip
retention-Copy1 copy-Copy2.ipynb
roberta_model
sentiment
sentiment.zip
torch_ddp_finetune_neural_classifier.py
train.tsv
train_sample.tsv
two_are_better_than_one.txt
utils.py
(base) Jims-MBP:Desktop jim$ pwd
/Users/jim/Desktop
(base) Jims-MBP:Desktop jim$ mkdir final_project
(base) Jims-MBP:Desktop jim$ cd final_
-bash: cd: final_: No such file or directory
(base) Jims-MBP:Desktop jim$ cd final_project/
(base) Jims-MBP:final_project jim$ ls
(base) Jims-MBP:final_project jim$ scp ubuntu@104.171.202.253:~/nlp-test/sentiment/checkpoints/final_model_20241019-055621.pkl .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
final_model_20241019-055621.pkl                             100%  431MB  17.3MB/s   00:24    
(base) Jims-MBP:final_project jim$ ls
final_model_20241019-055621.pkl
(base) Jims-MBP:final_project jim$ mkdir neutral
(base) Jims-MBP:final_project jim$ ls
final_model_20241019-055621.pkl	neutral
(base) Jims-MBP:final_project jim$ mv *.pkl neutral/
(base) Jims-MBP:final_project jim$ ls
neutral
(base) Jims-MBP:final_project jim$ cd neutral/
(base) Jims-MBP:neutral jim$ ls
final_model_20241019-055621.pkl
(base) Jims-MBP:neutral jim$ scp ubuntu@104.171.202.253:~/nlp-test/sentiment/saves/predictions_20241019-055806.csv .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
predictions_20241019-055806.csv                             100%  537KB   1.6MB/s   00:00    
(base) Jims-MBP:neutral jim$ scp ubuntu@104.171.202.253:~/nlp-test/sentiment/checkpoints/final_model_20241019-055621.pth .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
final_model_20241019-055621.pth                             100% 1291MB  11.6MB/s   01:51    
(base) Jims-MBP:neutral jim$ ls
final_model_20241019-055621.pkl	predictions_20241019-055806.csv
final_model_20241019-055621.pth
(base) Jims-MBP:neutral jim$ cd ..
(base) Jims-MBP:final_project jim$ ls
neutral
(base) Jims-MBP:final_project jim$ mkdir positive
(base) Jims-MBP:final_project jim$ cd positive/
(base) Jims-MBP:positive jim$ cd ..
(base) Jims-MBP:final_project jim$ ls
neutral		positive
(base) Jims-MBP:final_project jim$ cd neutral/
(base) Jims-MBP:neutral jim$ ls
final_model_20241019-055621.pkl	predictions_20241019-055806.csv
final_model_20241019-055621.pth
(base) Jims-MBP:neutral jim$ ubuntu@104.171.202.204:~/nlp-test/sentiment/checkpoints/checkpoint_epoch_10_20241019-055423.pth
-bash: ubuntu@104.171.202.204:~/nlp-test/sentiment/checkpoints/checkpoint_epoch_10_20241019-055423.pth: No such file or directory
(base) Jims-MBP:neutral jim$ scp ubuntu@104.171.202.204:~/nlp-test/sentiment/checkpoints/checkpoint_epoch_10_20241019-055423.pth
usage: scp [-346ABCpqrTv] [-c cipher] [-F ssh_config] [-i identity_file]
            [-J destination] [-l limit] [-o ssh_option] [-P port]
            [-S program] source ... target
(base) Jims-MBP:neutral jim$ scp ubuntu@104.171.202.204:~/nlp-test/sentiment/checkpoints/checkpoint_epoch_10_20241019-055423.pth .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
checkpoint_epoch_10_20241019-055423.pth                     100% 1292MB  11.7MB/s   01:50    
(base) Jims-MBP:neutral jim$ ls
checkpoint_epoch_10_20241019-055423.pth	final_model_20241019-055621.pth
final_model_20241019-055621.pkl		predictions_20241019-055806.csv
(base) Jims-MBP:neutral jim$ cd ..
(base) Jims-MBP:final_project jim$ cd positive/
(base) Jims-MBP:positive jim$ ls
(base) Jims-MBP:positive jim$ ls
(base) Jims-MBP:positive jim$ scp ubuntu@104.171.202.204:~/nlp-test/sentiment/checkpoints/final_model_20241019-060740.pkl .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
final_model_20241019-060740.pkl                             100%  431MB  26.5MB/s   00:16    
(base) Jims-MBP:positive jim$ scp ubuntu@104.171.202.204:~/nlp-test/sentiment/saves/predictions_20241019-060923.csv .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
predictions_20241019-060923.csv                             100%  547KB   1.2MB/s   00:00    
(base) Jims-MBP:positive jim$ scp ubuntu@104.171.202.204:~/nlp-test/sentiment/checkpoints/final_model_20241019-060740.pth .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
final_model_20241019-060740.pth                             100% 1291MB  14.9MB/s   01:26    
(base) Jims-MBP:positive jim$ scp ubuntu@104.171.202.204:~/nlp-test/sentiment/checkpoints/checkpoint_epoch_10_20241019-055934.pth .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
checkpoint_epoch_10_20241019-055934.pth                     100% 1292MB   6.8MB/s   03:11    
(base) Jims-MBP:positive jim$ ls
checkpoint_epoch_10_20241019-055934.pth	final_model_20241019-060740.pth
final_model_20241019-060740.pkl		predictions_20241019-060923.csv
(base) Jims-MBP:positive jim$ ls -alt
total 6198376
-rw-r--r--  1 jim  staff  1354237098 Oct 18 23:20 checkpoint_epoch_10_20241019-055934.pth
drwxr-xr-x  5 jim  staff         160 Oct 18 23:18 ..
drwxr-xr-x  6 jim  staff         192 Oct 18 23:17 .
-rw-r--r--  1 jim  staff  1354230506 Oct 18 23:17 final_model_20241019-060740.pth
-rw-r--r--  1 jim  staff      559618 Oct 18 23:15 predictions_20241019-060923.csv
-rw-r--r--  1 jim  staff   451911486 Oct 18 23:14 final_model_20241019-060740.pkl
(base) Jims-MBP:positive jim$ cd ..
(base) Jims-MBP:final_project jim$ ls
negative	neutral		positive
(base) Jims-MBP:final_project jim$ ls -alRt
total 0
drwxr-xr-x    6 jim  staff   192 Oct 18 23:22 negative
drwxr-xr-x    5 jim  staff   160 Oct 18 23:18 .
drwxr-xr-x    6 jim  staff   192 Oct 18 23:17 positive
drwxr-xr-x    6 jim  staff   192 Oct 18 23:11 neutral
drwxr-xr-x@ 235 jim  staff  7520 Oct 18 23:03 ..

./negative:
total 6199312
-rw-r--r--  1 jim  staff  1354237098 Oct 18 23:24 checkpoint_epoch_10_20241019-060512.pth
drwxr-xr-x  6 jim  staff         192 Oct 18 23:22 .
-rw-r--r--  1 jim  staff  1354230506 Oct 18 23:22 final_model_20241019-060753.pth
-rw-r--r--  1 jim  staff      561686 Oct 18 23:19 predictions_20241019-060938.csv
-rw-r--r--  1 jim  staff   451911486 Oct 18 23:18 final_model_20241019-060753.pkl
drwxr-xr-x  5 jim  staff         160 Oct 18 23:18 ..

./positive:
total 6198376
-rw-r--r--  1 jim  staff  1354237098 Oct 18 23:20 checkpoint_epoch_10_20241019-055934.pth
drwxr-xr-x  5 jim  staff         160 Oct 18 23:18 ..
drwxr-xr-x  6 jim  staff         192 Oct 18 23:17 .
-rw-r--r--  1 jim  staff  1354230506 Oct 18 23:17 final_model_20241019-060740.pth
-rw-r--r--  1 jim  staff      559618 Oct 18 23:15 predictions_20241019-060923.csv
-rw-r--r--  1 jim  staff   451911486 Oct 18 23:14 final_model_20241019-060740.pkl

./neutral:
total 6194888
drwxr-xr-x  5 jim  staff         160 Oct 18 23:18 ..
-rw-r--r--  1 jim  staff  1354237098 Oct 18 23:13 checkpoint_epoch_10_20241019-055423.pth
drwxr-xr-x  6 jim  staff         192 Oct 18 23:11 .
-rw-r--r--  1 jim  staff  1354230506 Oct 18 23:07 final_model_20241019-055621.pth
-rw-r--r--  1 jim  staff      550076 Oct 18 23:05 predictions_20241019-055806.csv
-rw-r--r--  1 jim  staff   451911482 Oct 18 23:04 final_model_20241019-055621.pkl
(base) Jims-MBP:final_project jim$ ssh ubuntu@104.171.203.114
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 6.2.0-37-generic x86_64)
 .============.
 ||   __      ||    _                    _         _
 ||   \_\     ||   | |    __ _ _ __ ___ | |__   __| | __ _
 ||    \_\    ||   | |   / _` | '_ ` _ \| '_ \ / _` |/ _` |
 ||   /_λ_\   ||   | |__| (_| | | | | | | |_) | (_| | (_| |
 ||  /_/ \_\  ||   |_____\__,_|_| |_| |_|_.__/ \__,_|\__,_|
  .============.                                  GPU CLOUD

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Sun Oct 20 01:41:54 UTC 2024

  System load:  9.3349609375     Processes:                888
  Usage of /:   0.4% of 5.68TB   Users logged in:          1
  Memory usage: 2%               IPv4 address for docker0: 172.17.0.1
  Swap usage:   0%               IPv4 address for eno1:    104.171.203.114

 * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
   just raised the bar for easy, resilient and secure K8s cluster deployment.

   https://ubuntu.com/engage/secure-kubernetes-at-the-edge

Expanded Security Maintenance for Applications is not enabled.

270 updates can be applied immediately.
158 of these updates are standard security updates.
To see these additional updates run: apt list --upgradable

20 additional security updates can be applied with ESM Apps.
Learn more about enabling ESM Apps service at https://ubuntu.com/esm

New release '24.04.1 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Sat Oct 19 20:23:42 2024 from 69.209.27.147
ubuntu@104-171-203-114:~$ watch -n0.1 nvidia-smi
ubuntu@104-171-203-114:~$ logout
Connection to 104.171.203.114 closed.
(base) Jims-MBP:final_project jim$ ls
negative	neutral		positive
(base) Jims-MBP:final_project jim$ cd neutral/
(base) Jims-MBP:neutral jim$ ls
checkpoint_epoch_10_20241019-055423.pth	final_model_20241019-055621.pth
final_model_20241019-055621.pkl		predictions_20241019-055806.csv
(base) Jims-MBP:neutral jim$ scp ubuntu@104.171.202.253:~/nlp-test/sentiment/checkpoints/checkpoint_epoch_30_20241020-060107.pth .
^C(base) Jims-MBP:neutral jim$ scp ubuntu@104.171.202.170:~/nlp-test/sentiment/checkpoints/chepoint_epoch_30_20241020-060107.pth .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
checkpoint_epoch_30_20241020-060107.pth                     100% 1292MB  15.8MB/s   01:21    
(base) Jims-MBP:neutral jim$ scp ubuntu@104.171.202.170:~/nlp-test/sentiment/saves/predictions_20241020-061044.csv .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
predictions_20241020-061044.csv                             100%  537KB   1.9MB/s   00:00    
(base) Jims-MBP:neutral jim$ scp ubuntu@104.171.202.170:~/nlp-test/sentiment/checkpoints/final_model_20241020-060855.pkl .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
final_model_20241020-060855.pkl                             100%  610MB  20.6MB/s   00:29    
(base) Jims-MBP:neutral jim$ ls
binary_class_plots_20241020-061042.png_0_c340adac7561b142592c.png
checkpoint_epoch_10_20241019-055423.pth
checkpoint_epoch_30_20241020-060107.pth
final_model_20241019-055621.pkl
final_model_20241019-055621.pth
final_model_20241020-060855.pkl
predictions_20241019-055806.csv
predictions_20241020-061044.csv
(base) Jims-MBP:neutral jim$ cd ..
(base) Jims-MBP:final_project jim$ ls
multiclass	negative	neutral		positive
(base) Jims-MBP:final_project jim$ cd multiclass/
(base) Jims-MBP:multiclass jim$ ls
(base) Jims-MBP:multiclass jim$ scp ubuntu@104.171.203.160:~/nlp-test/sentiment/checkpoints/checkpoint_epoch_30_20241020-064527.pth .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
checkpoint_epoch_30_20241020-064527.pth                     100% 1292MB  18.5MB/s   01:09    
(base) Jims-MBP:multiclass jim$ scp ubuntu@104.171.203.160:~/nlp-test/sentiment/saves/predictions_20241020-070119.csv .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
predictions_20241020-070119.csv                             100%  515KB   1.8MB/s   00:00    
(base) Jims-MBP:multiclass jim$ scp ubuntu@104.171.203.160:~/nlp-test/sentiment/checkpoints/final_model_20241020-065930.pkl .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
final_model_20241020-065930.pkl                             100%  610MB  17.6MB/s   00:34    
(base) Jims-MBP:multiclass jim$ ls
checkpoint_epoch_100_20241020-180015.pth
checkpoint_epoch_30_20241020-064527.pth
confusion_matrix_20241021-041721_0_4fa827cb36dc43924d08.png
final_model_20241020-065930.pkl
final_model_20241020-180024.pkl
predictions_20241020-070119.csv
predictions_20241020-180213.csv
(base) Jims-MBP:multiclass jim$ mkdir dynasent_r2
(base) Jims-MBP:multiclass jim$ cd dynasent_r2/
(base) Jims-MBP:dynasent_r2 jim$ scp ubuntu@104.171.203.160:~/nlp-test/sentiment/checkpoints/checkpoint_epoch_60_20241021-052718.pth .
^C(base) Jims-MBP:dynasent_r2 jim$ scp ubuntu@104.171.202.130:~/nlp-test/sentiment/checkpointsheckpoint_epoch_60_20241021-052718.pth .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
checkpoint_epoch_60_20241021-052718.pth                     100% 1292MB  14.8MB/s   01:27    
(base) Jims-MBP:dynasent_r2 jim$ scp ubuntu@104.171.202.130:~/nlp-test/sentiment/saves/predictions_20241021-052749.csv .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
predictions_20241021-052749.csv                             100%   62KB 446.6KB/s   00:00    
(base) Jims-MBP:dynasent_r2 jim$ scp ubuntu@104.171.202.130:~/nlp-test/sentiment/checkpoints/final_model_20241021-052728.pkl .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
final_model_20241021-052728.pkl                             100%  610MB  14.6MB/s   00:41    
(base) Jims-MBP:dynasent_r2 jim$ cd ..
(base) Jims-MBP:multiclass jim$ ls
checkpoint_epoch_100_20241020-180015.pth
checkpoint_epoch_30_20241020-064527.pth
confusion_matrix_20241021-041721_0_4fa827cb36dc43924d08.png
dynasent_r2
final_model_20241020-065930.pkl
final_model_20241020-180024.pkl
predictions_20241020-070119.csv
predictions_20241020-180213.csv
(base) Jims-MBP:multiclass jim$ mkdir sst
(base) Jims-MBP:multiclass jim$ cd sst
(base) Jims-MBP:sst jim$ scp ubuntu@104.171.202.130:~/nlp-test/sentiment/checkpoints/checkpoint_epoch_70_20241021-063236.pth .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
checkpoint_epoch_70_20241021-063236.pth                     100% 1292MB  13.3MB/s   01:36    
(base) Jims-MBP:sst jim$ scp ubuntu@104.171.202.130:~/nlp-test/sentiment/saves/predictions_20241021-070943.csv .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
predictions_20241021-070943.csv                             100%  132KB 721.8KB/s   00:00    
(base) Jims-MBP:sst jim$ scp ubuntu@104.171.202.130:~/nlp-test/sentiment/checkpoints/final_model_20241021-070915.pkl .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
final_model_20241021-070915.pkl                             100%  610MB  14.8MB/s   00:41    
(base) Jims-MBP:sst jim$ ssh ubuntu@104.171.203.140
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
Welcome to Ubuntu 22.04.5 LTS (GNU/Linux 6.8.0-47-generic x86_64)
 .============.
 ||   __      ||    _                    _         _
 ||   \_\     ||   | |    __ _ _ __ ___ | |__   __| | __ _
 ||    \_\    ||   | |   / _` | '_ ` _ \| '_ \ / _` |/ _` |
 ||   /_λ_\   ||   | |__| (_| | | | | | | |_) | (_| | (_| |
 ||  /_/ \_\  ||   |_____\__,_|_| |_| |_|_.__/ \__,_|\__,_|
  .============.                                  GPU CLOUD

Last login: Wed Oct 23 05:49:08 2024 from 69.209.27.147
ubuntu@104-171-203-140:~$ watch -n0.1 nvidia-smi
ubuntu@104-171-203-140:~$ logout
Connection to 104.171.203.140 closed.
(base) Jims-MBP:sst jim$ pip install -r requirements.txt
ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'
^CTraceback (most recent call last):
  File "/Users/jim/anaconda3/bin/pip", line 11, in <module>
    sys.exit(main())
             ^^^^^^
  File "/Users/jim/anaconda3/lib/python3.11/site-packages/pip/_internal/cli/main.py", line 79, in main
    return command.main(cmd_args)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jim/anaconda3/lib/python3.11/site-packages/pip/_internal/cli/base_command.py", line 101, in main
    return self._main(args)
           ^^^^^^^^^^^^^^^^
  File "/Users/jim/anaconda3/lib/python3.11/site-packages/pip/_internal/cli/base_command.py", line 236, in _main
    self.handle_pip_version_check(options)
  File "/Users/jim/anaconda3/lib/python3.11/site-packages/pip/_internal/cli/req_command.py", line 188, in handle_pip_version_check
    pip_self_version_check(session, options)
  File "/Users/jim/anaconda3/lib/python3.11/site-packages/pip/_internal/self_outdated_check.py", line 231, in pip_self_version_check
    installed_dist = get_default_environment().get_distribution("pip")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jim/anaconda3/lib/python3.11/site-packages/pip/_internal/metadata/importlib/_envs.py", line 189, in get_distribution
    return next(matches, None)
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/jim/anaconda3/lib/python3.11/site-packages/pip/_internal/metadata/importlib/_envs.py", line 184, in <genexpr>
    matches = (
              ^
  File "/Users/jim/anaconda3/lib/python3.11/site-packages/pip/_internal/metadata/base.py", line 626, in iter_all_distributions
    for dist in self._iter_distributions():
  File "/Users/jim/anaconda3/lib/python3.11/site-packages/pip/_internal/metadata/importlib/_envs.py", line 176, in _iter_distributions
    yield from finder.find(location)
  File "/Users/jim/anaconda3/lib/python3.11/site-packages/pip/_internal/metadata/importlib/_envs.py", line 79, in find
    for dist, info_location in self._find_impl(location):
  File "/Users/jim/anaconda3/lib/python3.11/site-packages/pip/_internal/metadata/importlib/_envs.py", line 64, in _find_impl
    raw_name = get_dist_name(dist)
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/jim/anaconda3/lib/python3.11/site-packages/pip/_internal/metadata/importlib/_compat.py", line 52, in get_dist_name
    name = cast(Any, dist).name
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/jim/anaconda3/lib/python3.11/importlib/metadata/__init__.py", line 622, in name
    return self.metadata['Name']
           ^^^^^^^^^^^^^
  File "/Users/jim/anaconda3/lib/python3.11/importlib/metadata/__init__.py", line 610, in metadata
    self.read_text('METADATA')
  File "/Users/jim/anaconda3/lib/python3.11/importlib/metadata/__init__.py", line 938, in read_text
    return self._path.joinpath(filename).read_text(encoding='utf-8')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jim/anaconda3/lib/python3.11/pathlib.py", line 1058, in read_text
    with self.open(mode='r', encoding=encoding, errors=errors) as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jim/anaconda3/lib/python3.11/pathlib.py", line 1044, in open
    return io.open(self, mode, buffering, encoding, errors, newline)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen codecs>", line 309, in __init__
KeyboardInterrupt

(base) Jims-MBP:sst jim$ 
(base) Jims-MBP:sst jim$ 
(base) Jims-MBP:sst jim$ 
(base) Jims-MBP:sst jim$ 
(base) Jims-MBP:sst jim$ ssh ubuntu@192.18.129.105
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
Welcome to Ubuntu 22.04.5 LTS (GNU/Linux 6.8.0-47-generic x86_64)
 .============.
 ||   __      ||    _                    _         _
 ||   \_\     ||   | |    __ _ _ __ ___ | |__   __| | __ _
 ||    \_\    ||   | |   / _` | '_ ` _ \| '_ \ / _` |/ _` |
 ||   /_λ_\   ||   | |__| (_| | | | | | | |_) | (_| | (_| |
 ||  /_/ \_\  ||   |_____\__,_|_| |_| |_|_.__/ \__,_|\__,_|
  .============.                                  GPU CLOUD

Last login: Wed Oct 23 06:10:15 2024 from 69.209.27.147
ubuntu@192-18-129-105:~$ htop
ubuntu@192-18-129-105:~$ cd nlp-ca/
ubuntu@192-18-129-105:~/nlp-ca$ cd sentiment/
ubuntu@192-18-129-105:~/nlp-ca/sentiment$ ls
README.md   data                   ddp_sentiment.py           dspy_sentiment_final.ipynb  sst.py         torch_ddp_finetune_neural_classifier.py  torch_shallow_neural_classifier.py
colors.py   data_processing.ipynb  ddp_sentiment_finetune.py  nlp                         test.ipynb     torch_ddp_neural_classifier.py           utils.py
compare.py  datawaza_funcs.py      dspy_sentiment.ipynb       requirements.txt            test_input.py  torch_model_base.py
ubuntu@192-18-129-105:~/nlp-ca/sentiment$ source nlp/bin/activate
(nlp) ubuntu@192-18-129-105:~/nlp-ca/sentiment$ pip list
Package                   Version
------------------------- --------------
aiohappyeyeballs          2.4.3
aiohttp                   3.10.10
aiosignal                 1.3.1
alembic                   1.13.3
annotated-types           0.7.0
anyio                     4.6.2.post1
argon2-cffi               23.1.0
argon2-cffi-bindings      21.2.0
arrow                     1.3.0
asttokens                 2.4.1
async-lru                 2.0.4
async-timeout             4.0.3
attrs                     24.2.0
babel                     2.16.0
backoff                   2.2.1
beautifulsoup4            4.12.3
bleach                    6.1.0
blis                      0.7.11
catalogue                 2.0.10
certifi                   2024.8.30
cffi                      1.17.1
charset-normalizer        3.4.0
click                     8.1.7
cloudpathlib              0.16.0
colorlog                  6.8.2
comm                      0.2.2
confection                0.1.5
contourpy                 1.3.0
cycler                    0.12.1
cymem                     2.0.8
debugpy                   1.8.7
decorator                 5.1.1
defusedxml                0.7.1
dill                      0.3.7
exceptiongroup            1.2.2
executing                 2.1.0
fastjsonschema            2.20.0
filelock                  3.16.1
fonttools                 4.54.1
fqdn                      1.5.1
frozenlist                1.4.1
fsspec                    2023.10.0
greenlet                  3.1.1
h11                       0.14.0
httpcore                  1.0.6
httpx                     0.27.2
huggingface-hub           0.26.1
idna                      3.10
iniconfig                 2.0.0
ipython                   8.28.0
isoduration               20.11.0
jedi                      0.19.1
Jinja2                    3.1.4
joblib                    1.3.2
json5                     0.9.25
jsonpointer               3.0.0
jsonschema                4.23.0
jsonschema-specifications 2024.10.1
jupyter_client            8.6.3
jupyter_core              5.7.2
jupyter_server_terminals  0.5.3
jupyterlab_pygments       0.3.0
jupyterlab_widgets        3.0.13
kiwisolver                1.4.7
langcodes                 3.4.1
language_data             1.2.0
Mako                      1.3.6
marisa-trie               1.2.1
MarkupSafe                3.0.2
matplotlib                3.9.2
matplotlib-inline         0.1.7
mistune                   3.0.2
mpmath                    1.3.0
multidict                 6.1.0
multiprocess              0.70.15
murmurhash                1.0.10
nest-asyncio              1.6.0
networkx                  3.4.2
nltk                      3.9.1
numpy                     1.26.4
nvidia-cublas-cu12        12.1.3.1
nvidia-cuda-cupti-cu12    12.1.105
nvidia-cuda-nvrtc-cu12    12.1.105
nvidia-cuda-runtime-cu12  12.1.105
nvidia-cudnn-cu12         8.9.2.26
nvidia-cufft-cu12         11.0.2.54
nvidia-curand-cu12        10.3.2.106
nvidia-cusolver-cu12      11.4.5.107
nvidia-cusparse-cu12      12.1.0.106
nvidia-nccl-cu12          2.18.1
nvidia-nvjitlink-cu12     12.6.77
nvidia-nvtx-cu12          12.1.105
optuna                    4.0.0
overrides                 7.7.0
packaging                 24.1
pandas                    2.2.3
pandocfilters             1.5.1
parso                     0.8.4
pexpect                   4.9.0
pillow                    11.0.0
pip                       22.0.2
platformdirs              4.3.6
pluggy                    1.5.0
preshed                   3.0.9
prometheus_client         0.21.0
prompt_toolkit            3.0.48
propcache                 0.2.0
psutil                    6.1.0
ptyprocess                0.7.0
pure_eval                 0.2.3
pyarrow                   17.0.0
pycparser                 2.22
pydantic                  2.9.2
pydantic_core             2.23.4
Pygments                  2.18.0
pyparsing                 3.2.0
pytest                    8.3.3
python-dateutil           2.9.0.post0
python-dotenv             1.0.1
python-json-logger        2.0.7
pytz                      2024.2
PyYAML                    6.0.2
pyzmq                     26.2.0
referencing               0.35.1
regex                     2024.9.11
requests                  2.32.3
rfc3339-validator         0.1.4
rfc3986-validator         0.1.1
rpds-py                   0.20.0
safetensors               0.4.5
scikit-learn              1.5.2
scipy                     1.14.1
Send2Trash                1.8.3
setuptools                59.6.0
six                       1.16.0
smart-open                6.4.0
sniffio                   1.3.1
soupsieve                 2.6
spacy-legacy              3.0.12
spacy-loggers             1.0.5
SQLAlchemy                2.0.36
srsly                     2.4.8
stack-data                0.6.3
sympy                     1.13.3
terminado                 0.18.1
threadpoolctl             3.5.0
tinycss2                  1.3.0
tokenizers                0.20.1
tomli                     2.0.2
torch                     2.1.2
tornado                   6.4.1
tqdm                      4.66.5
traitlets                 5.14.3
transformers              4.45.2
triton                    2.1.0
typer                     0.9.4
types-python-dateutil     2.9.0.20241003
typing_extensions         4.12.2
tzdata                    2024.2
ujson                     5.10.0
uri-template              1.3.0
urllib3                   2.2.3
wasabi                    1.1.3
wcwidth                   0.2.13
weasel                    0.3.4
webcolors                 24.8.0
webencodings              0.5.1
websocket-client          1.8.0
wget                      3.2
widgetsnbextension        4.0.13
xxhash                    3.5.0
yarl                      1.16.0
(nlp) ubuntu@192-18-129-105:~/nlp-ca/sentiment$ ls
README.md   data                   ddp_sentiment.py           dspy_sentiment_final.ipynb  sst.py         torch_ddp_finetune_neural_classifier.py  torch_shallow_neural_classifier.py
colors.py   data_processing.ipynb  ddp_sentiment_finetune.py  nlp                         test.ipynb     torch_ddp_neural_classifier.py           utils.py
compare.py  datawaza_funcs.py      dspy_sentiment.ipynb       requirements.txt            test_input.py  torch_model_base.py
(nlp) ubuntu@192-18-129-105:~/nlp-ca/sentiment$ pip install wandb
Collecting wandb
  Downloading wandb-0.18.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.0/16.0 MB 84.7 MB/s eta 0:00:00
Collecting setproctitle
  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)
Requirement already satisfied: setuptools in ./nlp/lib/python3.10/site-packages (from wandb) (59.6.0)
Requirement already satisfied: click!=8.0.0,>=7.1 in ./nlp/lib/python3.10/site-packages (from wandb) (8.1.7)
Collecting docker-pycreds>=0.4.0
  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)
Requirement already satisfied: platformdirs in ./nlp/lib/python3.10/site-packages (from wandb) (4.3.6)
Requirement already satisfied: requests<3,>=2.0.0 in ./nlp/lib/python3.10/site-packages (from wandb) (2.32.3)
Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0
  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.6/316.6 KB 57.6 MB/s eta 0:00:00
Collecting sentry-sdk>=2.0.0
  Downloading sentry_sdk-2.17.0-py2.py3-none-any.whl (314 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 314.5/314.5 KB 63.4 MB/s eta 0:00:00
Requirement already satisfied: pyyaml in ./nlp/lib/python3.10/site-packages (from wandb) (6.0.2)
Collecting gitpython!=3.1.29,>=1.0.0
  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 KB 45.5 MB/s eta 0:00:00
Requirement already satisfied: typing-extensions<5,>=4.4 in ./nlp/lib/python3.10/site-packages (from wandb) (4.12.2)
Requirement already satisfied: psutil>=5.0.0 in ./nlp/lib/python3.10/site-packages (from wandb) (6.1.0)
Requirement already satisfied: six>=1.4.0 in ./nlp/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)
Collecting gitdb<5,>=4.0.1
  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 KB 21.6 MB/s eta 0:00:00
Requirement already satisfied: certifi>=2017.4.17 in ./nlp/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)
Requirement already satisfied: idna<4,>=2.5 in ./nlp/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)
Requirement already satisfied: charset-normalizer<4,>=2 in ./nlp/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.0)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./nlp/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)
Collecting smmap<6,>=3.0.1
  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)
Installing collected packages: smmap, setproctitle, sentry-sdk, protobuf, docker-pycreds, gitdb, gitpython, wandb
Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 protobuf-5.28.3 sentry-sdk-2.17.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.5
(nlp) ubuntu@192-18-129-105:~/nlp-ca/sentiment$ pip list
Package                   Version
------------------------- --------------
aiohappyeyeballs          2.4.3
aiohttp                   3.10.10
aiosignal                 1.3.1
alembic                   1.13.3
annotated-types           0.7.0
anyio                     4.6.2.post1
argon2-cffi               23.1.0
argon2-cffi-bindings      21.2.0
arrow                     1.3.0
asttokens                 2.4.1
async-lru                 2.0.4
async-timeout             4.0.3
attrs                     24.2.0
babel                     2.16.0
backoff                   2.2.1
beautifulsoup4            4.12.3
bleach                    6.1.0
blis                      0.7.11
catalogue                 2.0.10
certifi                   2024.8.30
cffi                      1.17.1
charset-normalizer        3.4.0
click                     8.1.7
cloudpathlib              0.16.0
colorlog                  6.8.2
comm                      0.2.2
confection                0.1.5
contourpy                 1.3.0
cycler                    0.12.1
cymem                     2.0.8
datasets                  2.14.6
debugpy                   1.8.7
decorator                 5.1.1
defusedxml                0.7.1
dill                      0.3.7
docker-pycreds            0.4.0
dspy-ai                   2.3.1
exceptiongroup            1.2.2
executing                 2.1.0
fastjsonschema            2.20.0
filelock                  3.16.1
fonttools                 4.54.1
fqdn                      1.5.1
frozenlist                1.4.1
fsspec                    2023.10.0
gitdb                     4.0.11
GitPython                 3.1.43
greenlet                  3.1.1
h11                       0.14.0
httpcore                  1.0.6
httpx                     0.27.2
huggingface-hub           0.26.1
idna                      3.10
iniconfig                 2.0.0
ipykernel                 6.29.5
ipython                   8.28.0
ipywidgets                8.1.5
isoduration               20.11.0
jedi                      0.19.1
Jinja2                    3.1.4
joblib                    1.3.2
json5                     0.9.25
jsonpointer               3.0.0
jsonschema                4.23.0
jsonschema-specifications 2024.10.1
jupyter                   1.1.1
jupyter_client            8.6.3
jupyter-console           6.6.3
jupyter_core              5.7.2
jupyter-events            0.10.0
jupyter-lsp               2.2.5
jupyter_server            2.14.2
jupyter_server_terminals  0.5.3
jupyterlab                4.2.5
jupyterlab_pygments       0.3.0
jupyterlab_server         2.27.3
jupyterlab_widgets        3.0.13
kiwisolver                1.4.7
langcodes                 3.4.1
language_data             1.2.0
Mako                      1.3.6
marisa-trie               1.2.1
MarkupSafe                3.0.2
matplotlib                3.9.2
matplotlib-inline         0.1.7
mistune                   3.0.2
mpmath                    1.3.0
multidict                 6.1.0
multiprocess              0.70.15
murmurhash                1.0.10
nbclient                  0.10.0
nbconvert                 7.16.4
nbformat                  5.10.4
nest-asyncio              1.6.0
networkx                  3.4.2
nltk                      3.9.1
notebook                  7.2.2
notebook_shim             0.2.4
numpy                     1.26.4
nvidia-cublas-cu12        12.1.3.1
nvidia-cuda-cupti-cu12    12.1.105
nvidia-cuda-nvrtc-cu12    12.1.105
nvidia-cuda-runtime-cu12  12.1.105
nvidia-cudnn-cu12         8.9.2.26
nvidia-cufft-cu12         11.0.2.54
nvidia-curand-cu12        10.3.2.106
nvidia-cusolver-cu12      11.4.5.107
nvidia-cusparse-cu12      12.1.0.106
nvidia-nccl-cu12          2.18.1
nvidia-nvjitlink-cu12     12.6.77
nvidia-nvtx-cu12          12.1.105
openai                    0.28.1
optuna                    4.0.0
overrides                 7.7.0
packaging                 24.1
pandas                    2.2.3
pandocfilters             1.5.1
parso                     0.8.4
pexpect                   4.9.0
pillow                    11.0.0
pip                       22.0.2
platformdirs              4.3.6
pluggy                    1.5.0
preshed                   3.0.9
prometheus_client         0.21.0
prompt_toolkit            3.0.48
propcache                 0.2.0
protobuf                  5.28.3
psutil                    6.1.0
ptyprocess                0.7.0
pure_eval                 0.2.3
pyarrow                   17.0.0
pycparser                 2.22
pydantic                  2.9.2
pydantic_core             2.23.4
Pygments                  2.18.0
pyparsing                 3.2.0
pytest                    8.3.3
python-dateutil           2.9.0.post0
python-dotenv             1.0.1
python-json-logger        2.0.7
pytz                      2024.2
PyYAML                    6.0.2
pyzmq                     26.2.0
referencing               0.35.1
regex                     2024.9.11
requests                  2.32.3
rfc3339-validator         0.1.4
rfc3986-validator         0.1.1
rpds-py                   0.20.0
safetensors               0.4.5
scikit-learn              1.5.2
scipy                     1.14.1
Send2Trash                1.8.3
sentry-sdk                2.17.0
setproctitle              1.3.3
setuptools                59.6.0
six                       1.16.0
smart-open                6.4.0
smmap                     5.0.1
sniffio                   1.3.1
soupsieve                 2.6
spacy                     3.7.2
spacy-legacy              3.0.12
spacy-loggers             1.0.5
SQLAlchemy                2.0.36
srsly                     2.4.8
stack-data                0.6.3
sympy                     1.13.3
terminado                 0.18.1
thinc                     8.2.5
threadpoolctl             3.5.0
tinycss2                  1.3.0
tokenizers                0.20.1
tomli                     2.0.2
torch                     2.1.2
torchaudio                2.1.2
torchvision               0.16.2
tornado                   6.4.1
tqdm                      4.66.5
traitlets                 5.14.3
transformers              4.45.2
triton                    2.1.0
typer                     0.9.4
types-python-dateutil     2.9.0.20241003
typing_extensions         4.12.2
tzdata                    2024.2
ujson                     5.10.0
uri-template              1.3.0
urllib3                   2.2.3
wandb                     0.18.5
wasabi                    1.1.3
wcwidth                   0.2.13
weasel                    0.3.4
webcolors                 24.8.0
webencodings              0.5.1
websocket-client          1.8.0
wget                      3.2
widgetsnbextension        4.0.13
xxhash                    3.5.0
yarl                      1.16.0
(nlp) ubuntu@192-18-129-105:~/nlp-ca/sentiment$ watch -n0.1 nvidia-smi
(nlp) ubuntu@192-18-129-105:~/nlp-ca/sentiment$ logout
Connection to 192.18.129.105 closed.
(base) Jims-MBP:sst jim$ ssh ubuntu@138.2.58.218
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
Welcome to Ubuntu 22.04.5 LTS (GNU/Linux 6.8.0-47-generic x86_64)
 .============.
 ||   __      ||    _                    _         _
 ||   \_\     ||   | |    __ _ _ __ ___ | |__   __| | __ _
 ||    \_\    ||   | |   / _` | '_ ` _ \| '_ \ / _` |/ _` |
 ||   /_λ_\   ||   | |__| (_| | | | | | | |_) | (_| | (_| |
 ||  /_/ \_\  ||   |_____\__,_|_| |_| |_|_.__/ \__,_|\__,_|
  .============.                                  GPU CLOUD

Last login: Wed Oct 23 23:30:53 2024 from 69.209.27.147
ubuntu@138-2-58-218:~$ cd nlp-osaka/sentiment/
ubuntu@138-2-58-218:~/nlp-osaka/sentiment$ source nlp/bin/activate
(nlp) ubuntu@138-2-58-218:~/nlp-osaka/sentiment$ pip install wandb
Collecting wandb
  Downloading wandb-0.18.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.0/16.0 MB 23.4 MB/s eta 0:00:00
Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0
  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.6/316.6 KB 57.5 MB/s eta 0:00:00
Collecting setproctitle
  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)
Requirement already satisfied: typing-extensions<5,>=4.4 in ./nlp/lib/python3.10/site-packages (from wandb) (4.12.2)
Collecting platformdirs
  Using cached platformdirs-4.3.6-py3-none-any.whl (18 kB)
Collecting gitpython!=3.1.29,>=1.0.0
  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 KB 44.2 MB/s eta 0:00:00
Collecting psutil>=5.0.0
  Using cached psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)
Collecting sentry-sdk>=2.0.0
  Downloading sentry_sdk-2.17.0-py2.py3-none-any.whl (314 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 314.5/314.5 KB 55.6 MB/s eta 0:00:00
Collecting docker-pycreds>=0.4.0
  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)
Collecting pyyaml
  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)
Collecting click!=8.0.0,>=7.1
  Using cached click-8.1.7-py3-none-any.whl (97 kB)
Collecting requests<3,>=2.0.0
  Using cached requests-2.32.3-py3-none-any.whl (64 kB)
Requirement already satisfied: setuptools in ./nlp/lib/python3.10/site-packages (from wandb) (59.6.0)
Collecting six>=1.4.0
  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)
Collecting gitdb<5,>=4.0.1
  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 KB 17.6 MB/s eta 0:00:00
Collecting charset-normalizer<4,>=2
  Using cached charset_normalizer-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)
Collecting idna<4,>=2.5
  Using cached idna-3.10-py3-none-any.whl (70 kB)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./nlp/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)
Collecting certifi>=2017.4.17
  Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)
Collecting smmap<6,>=3.0.1
  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)
Installing collected packages: smmap, six, setproctitle, pyyaml, psutil, protobuf, platformdirs, idna, click, charset-normalizer, certifi, sentry-sdk, requests, gitdb, docker-pycreds, gitpython, wandb
Successfully installed certifi-2024.8.30 charset-normalizer-3.4.0 click-8.1.7 docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 idna-3.10 platformdirs-4.3.6 protobuf-5.28.3 psutil-6.1.0 pyyaml-6.0.2 requests-2.32.3 sentry-sdk-2.17.0 setproctitle-1.3.3 six-1.16.0 smmap-5.0.1 wandb-0.18.5
(nlp) ubuntu@138-2-58-218:~/nlp-osaka/sentiment$ wandb login
Traceback (most recent call last):
  File "/home/ubuntu/nlp-osaka/sentiment/nlp/bin/wandb", line 5, in <module>
    from wandb.cli.cli import cli
  File "/home/ubuntu/nlp-osaka/sentiment/nlp/lib/python3.10/site-packages/wandb/__init__.py", line 21, in <module>
    from wandb import sdk as wandb_sdk
  File "/home/ubuntu/nlp-osaka/sentiment/nlp/lib/python3.10/site-packages/wandb/sdk/__init__.py", line 25, in <module>
    from .artifacts.artifact import Artifact
  File "/home/ubuntu/nlp-osaka/sentiment/nlp/lib/python3.10/site-packages/wandb/sdk/artifacts/artifact.py", line 36, in <module>
    from wandb import data_types, env, util
  File "/home/ubuntu/nlp-osaka/sentiment/nlp/lib/python3.10/site-packages/wandb/data_types.py", line 16, in <module>
    from .sdk.data_types.audio import Audio
  File "/home/ubuntu/nlp-osaka/sentiment/nlp/lib/python3.10/site-packages/wandb/sdk/data_types/audio.py", line 8, in <module>
    from . import _dtypes
  File "/home/ubuntu/nlp-osaka/sentiment/nlp/lib/python3.10/site-packages/wandb/sdk/data_types/_dtypes.py", line 364, in <module>
    NumberType.types.append(np.byte)
  File "/home/ubuntu/nlp-osaka/sentiment/nlp/lib/python3.10/site-packages/wandb/util.py", line 219, in __getattribute__
    state.load()
  File "/home/ubuntu/nlp-osaka/sentiment/nlp/lib/python3.10/site-packages/wandb/util.py", line 212, in load
    self.module.__spec__.loader.exec_module(self.module)
  File "/home/ubuntu/nlp-osaka/sentiment/nlp/lib/python3.10/site-packages/numpy/__init__.py", line 105, in <module>
    from ._globals import _NoValue, _CopyMode
  File "/home/ubuntu/nlp-osaka/sentiment/nlp/lib/python3.10/site-packages/numpy/_globals.py", line 20, in <module>
    from ._utils import set_module as _set_module
ModuleNotFoundError: No module named 'numpy._utils'
(nlp) ubuntu@138-2-58-218:~/nlp-osaka/sentiment$ wandb login
wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)
wandb: You can find your API key in your browser here: https://wandb.ai/authorize
wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: 
wandb: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc
(nlp) ubuntu@138-2-58-218:~/nlp-osaka/sentiment$ watch -n0.1 nvidia-smi
(nlp) ubuntu@138-2-58-218:~/nlp-osaka/sentiment$ logout
Connection to 138.2.58.218 closed.
(base) Jims-MBP:sst jim$ pwd
/Users/jim/Desktop/final_project/multiclass/sst
(base) Jims-MBP:sst jim$ cd ..
(base) Jims-MBP:multiclass jim$ ls
checkpoint_epoch_100_20241020-180015.pth
checkpoint_epoch_30_20241020-064527.pth
confusion_matrix_20241021-041721_0_4fa827cb36dc43924d08.png
dynasent_r2
final_model_20241020-065930.pkl
final_model_20241020-180024.pkl
predictions_20241020-070119.csv
predictions_20241020-180213.csv
sst
(base) Jims-MBP:multiclass jim$ ls
checkpoint_epoch_100_20241020-180015.pth
checkpoint_epoch_30_20241020-064527.pth
confusion_matrix_20241021-041721_0_4fa827cb36dc43924d08.png
dynasent_r2
final_model_20241020-065930.pkl
final_model_20241020-180024.pkl
predictions_20241020-070119.csv
predictions_20241020-180213.csv
sst
(base) Jims-MBP:multiclass jim$ ls -al
total 7790024
drwxr-xr-x  12 jim  staff         384 Oct 21 00:24 .
drwxr-xr-x   7 jim  staff         224 Oct 19 23:28 ..
-rw-r--r--@  1 jim  staff        6148 Oct 21 00:24 .DS_Store
-rw-r--r--   1 jim  staff  1354250210 Oct 20 14:21 checkpoint_epoch_100_20241020-180015.pth
-rw-r--r--   1 jim  staff  1354249386 Oct 20 00:15 checkpoint_epoch_30_20241020-064527.pth
-rw-r--r--@  1 jim  staff       25051 Oct 20 21:21 confusion_matrix_20241021-041721_0_4fa827cb36dc43924d08.png
drwxr-xr-x   5 jim  staff         160 Oct 20 23:14 dynasent_r2
-rw-r--r--@  1 jim  staff   639444679 Oct 20 00:16 final_model_20241020-065930.pkl
-rw-r--r--   1 jim  staff   639445018 Oct 20 14:22 final_model_20241020-180024.pkl
-rw-r--r--   1 jim  staff      527853 Oct 20 00:15 predictions_20241020-070119.csv
-rw-r--r--   1 jim  staff      527842 Oct 20 14:21 predictions_20241020-180213.csv
drwxr-xr-x   5 jim  staff         160 Oct 21 00:18 sst
(base) Jims-MBP:multiclass jim$ cd ..
(base) Jims-MBP:final_project jim$ ls
multiclass	negative	neutral		positive
(base) Jims-MBP:final_project jim$ mkdir electra_large
(base) Jims-MBP:final_project jim$ cd electra_large/
(base) Jims-MBP:electra_large jim$ scp ubuntu@138.2.58.218:~/nlp-test/sentiment/checkpoints/checkpoint_epoch_13_20241024-215451.pth .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
scp: /home/ubuntu/nlp-test/sentiment/checkpoints/checkpoint_epoch_13_20241024-215451.pth: No such file or directory
(base) Jims-MBP:electra_large jim$ scp ubuntu@138.2.58.218:~/nlp-osaka/sentiment/checkpoints/checkpoint_epoch_13_20241024-215451.pth .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
checkpoint_epoch_13_20241024-215451.pth                      100% 3872MB   6.2MB/s   10:23    
(base) Jims-MBP:electra_large jim$ ssh ubuntu@104.171.202.167
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
Welcome to Ubuntu 22.04.5 LTS (GNU/Linux 6.8.0-47-generic x86_64)
 .============.
 ||   __      ||    _                    _         _
 ||   \_\     ||   | |    __ _ _ __ ___ | |__   __| | __ _
 ||    \_\    ||   | |   / _` | '_ ` _ \| '_ \ / _` |/ _` |
 ||   /_λ_\   ||   | |__| (_| | | | | | | |_) | (_| | (_| |
 ||  /_/ \_\  ||   |_____\__,_|_| |_| |_|_.__/ \__,_|\__,_|
  .============.                                  GPU CLOUD

Last login: Sat Oct 26 02:08:24 2024 from 69.209.27.147
ubuntu@104-171-202-167:~$ watch -n0.1 nvidia-smi
ubuntu@104-171-202-167:~$ logout
Connection to 104.171.202.167 closed.
(base) Jims-MBP:electra_large jim$ pwd
/Users/jim/Desktop/final_project/electra_large
(base) Jims-MBP:electra_large jim$ cd ..
(base) Jims-MBP:final_project jim$ ls
electra_large	multiclass	negative	neutral		positive
(base) Jims-MBP:final_project jim$ mkdir electra_base
(base) Jims-MBP:final_project jim$ cd electra_
-bash: cd: electra_: No such file or directory
(base) Jims-MBP:final_project jim$ cd electra_base/
(base) Jims-MBP:electra_base jim$ ls
(base) Jims-MBP:electra_base jim$ scp ubuntu@104.171.202.167:~/nlp-osaka/sentiment/checkpoints/checkpoint_epoch_14_20241026-042844.pth .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
scp: /home/ubuntu/nlp-osaka/sentiment/checkpoints/checkpoint_epoch_14_20241026-042844.pth: No such file or directory
(base) Jims-MBP:electra_base jim$ scp ubuntu@104.171.202.167:~/nlp-test/sentiment/checkpoints/checkpoint_epoch_14_20241026-042844.pth .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
checkpoint_epoch_14_20241026-042844.pth                      100% 1292MB   9.2MB/s   02:20    
(base) Jims-MBP:electra_base jim$ scp ubuntu@104.171.202.167:~/nlp-test/sentiment/saves/predictions_20241026-053655.csv .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
predictions_20241026-053655.csv                              100%  517KB   1.8MB/s   00:00    
(base) Jims-MBP:electra_base jim$ scp ubuntu@104.171.202.167:~/nlp-test/sentiment/checkpoints/final_model_20241026-053505.pkl .
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
final_model_20241026-053505.pkl                              100%  523MB  11.8MB/s   00:44    
(base) Jims-MBP:electra_base jim$ ssh ubuntu@104.171.202.167
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
Welcome to Ubuntu 22.04.5 LTS (GNU/Linux 6.8.0-47-generic x86_64)
 .============.
 ||   __      ||    _                    _         _
 ||   \_\     ||   | |    __ _ _ __ ___ | |__   __| | __ _
 ||    \_\    ||   | |   / _` | '_ ` _ \| '_ \ / _` |/ _` |
 ||   /_λ_\   ||   | |__| (_| | | | | | | |_) | (_| | (_| |
 ||  /_/ \_\  ||   |_____\__,_|_| |_| |_|_.__/ \__,_|\__,_|
  .============.                                  GPU CLOUD

Last login: Sat Oct 26 02:09:35 2024 from 69.209.27.147
ubuntu@104-171-202-167:~$ watch -n0.1 nvidia-smi
ubuntu@104-171-202-167:~$ logout
Connection to 104.171.202.167 closed.
(base) Jims-MBP:electra_base jim$ ssh ubuntu@104.171.203.59
The authenticity of host '104.171.203.59 (104.171.203.59)' can't be established.
ED25519 key fingerprint is SHA256:5ctZtHiOjDTkgPT/LQZwcjdXWoySRW+BMiumvsRuJIY.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '104.171.203.59' (ED25519) to the list of known hosts.
Enter passphrase for key '/Users/jim/.ssh/id_rsa': 
Welcome to Ubuntu 22.04.5 LTS (GNU/Linux 6.8.0-47-generic x86_64)
 .============.
 ||   __      ||    _                    _         _
 ||   \_\     ||   | |    __ _ _ __ ___ | |__   __| | __ _
 ||    \_\    ||   | |   / _` | '_ ` _ \| '_ \ / _` |/ _` |
 ||   /_λ_\   ||   | |__| (_| | | | | | | |_) | (_| | (_| |
 ||  /_/ \_\  ||   |_____\__,_|_| |_| |_|_.__/ \__,_|\__,_|
  .============.                                  GPU CLOUD

ubuntu@104-171-203-59:~$ cd nlp-test/
ubuntu@104-171-203-59:~/nlp-test$ cd sentiment/
ubuntu@104-171-203-59:~/nlp-test/sentiment$ source nlp/bin/activate
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ git pull
Username for 'https://github.com': jim@jimbeno.net
Password for 'https://jim@jimbeno.net@github.com': 
Already up to date.
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ git pull
Username for 'https://github.com': jim@jimbeno.net
Password for 'https://jim@jimbeno.net@github.com': 
remote: Invalid username or password.
fatal: Authentication failed for 'https://github.com/jbeno/sentiment.git/'
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ git pull
Username for 'https://github.com': jim@jimbeno.net
Password for 'https://jim@jimbeno.net@github.com': 
remote: Enumerating objects: 15, done.
remote: Counting objects: 100% (15/15), done.
remote: Compressing objects: 100% (3/3), done.
remote: Total 8 (delta 5), reused 8 (delta 5), pack-reused 0 (from 0)
Unpacking objects: 100% (8/8), 2.21 MiB | 1.54 MiB/s, done.
From https://github.com/jbeno/sentiment
   1e2607c..19dbd54  main       -> origin/main
Updating 1e2607c..19dbd54
Updating files: 100% (11/11), done.
Fast-forward
 data/merged/bad_counts_for_test/test_all.csv         |   5484 ++
 data/merged/bad_counts_for_test/test_all_binary.csv  |   5484 ++
 data/merged/bad_counts_for_test/train_all.csv        | 104322 +++++++++++++++++++++++++
 data/merged/bad_counts_for_test/train_all_binary.csv | 104322 +++++++++++++++++++++++++
 data/merged/bad_counts_for_test/train_balanced.csv   | 259608 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 data/merged/bad_counts_for_test/val_all.csv          |   5514 ++
 data/merged/bad_counts_for_test/val_all_binary.csv   |   5514 ++
 data/merged/test_all.csv                             |   7983 +-
 data/merged/test_all_binary.csv                      |   7983 +-
 data/merged/train_balanced.csv                       | 515154 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-------------------------------------------------------------
 data_processing.ipynb                                |   1271 +-
 11 files changed, 757876 insertions(+), 264763 deletions(-)
 create mode 100644 data/merged/bad_counts_for_test/test_all.csv
 create mode 100644 data/merged/bad_counts_for_test/test_all_binary.csv
 create mode 100644 data/merged/bad_counts_for_test/train_all.csv
 create mode 100644 data/merged/bad_counts_for_test/train_all_binary.csv
 create mode 100644 data/merged/bad_counts_for_test/train_balanced.csv
 create mode 100644 data/merged/bad_counts_for_test/val_all.csv
 create mode 100644 data/merged/bad_counts_for_test/val_all_binary.csv
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ ls -alt data/merged/
total 53048
-rw-rw-r-- 1 ubuntu ubuntu 27463555 Oct 29 03:11 train_balanced.csv
drwxrwxr-x 2 ubuntu ubuntu     4096 Oct 29 03:11 .
-rw-rw-r-- 1 ubuntu ubuntu   936187 Oct 29 03:11 test_all_binary.csv
-rw-rw-r-- 1 ubuntu ubuntu   714123 Oct 29 03:11 test_all.csv
drwxrwxr-x 2 ubuntu ubuntu     4096 Oct 29 03:11 bad_counts_for_test
-rw-rw-r-- 1 ubuntu ubuntu   787841 Oct 23 05:50 val_all_binary.csv
-rw-rw-r-- 1 ubuntu ubuntu   603483 Oct 23 05:50 val_all.csv
-rw-rw-r-- 1 ubuntu ubuntu 13642804 Oct 23 05:50 train_all_binary.csv
-rw-rw-r-- 1 ubuntu ubuntu 10171462 Oct 23 05:50 train_all.csv
drwxrwxr-x 2 ubuntu ubuntu     4096 Sep  2 03:01 ..
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --weights_name 'google/electra-base-discriminator' --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 12 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Base' --wandb_run 'E1-EBFT-Merged' --model_file 'checkpoint_epoch_14_20241026-042844.pth' --interactive

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 5 - Device: cuda:5
Rank 1 - Device: cuda:1
Rank 3 - Device: cuda:3
Rank 7 - Device: cuda:7
Rank 6 - Device: cuda:6
Rank 2 - Device: cuda:2
Rank 4 - Device: cuda:4
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
An error occurred during training: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
Traceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1135, in main
    wandb_run = wandb.init(project=wandb_project, name=wandb_run_name, config={
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 1266, in init
    wandb._sentry.reraise(e)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/wandb/analytics/sentry.py", line 155, in reraise
    raise exc.with_traceback(sys.exc_info()[2])
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 1251, in init
    wi.setup(kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 303, in setup
    wandb_login._login(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/wandb/sdk/wandb_login.py", line 347, in _login
    wlogin.prompt_api_key()
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/wandb/sdk/wandb_login.py", line 281, in prompt_api_key
    raise UsageError("api_key not configured (no-tty). call " + directive)
wandb.errors.errors.UsageError: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
^C
Ctrl+C received. Terminating all processes...
Rank 0 - Current process: MainProcess cleaning up...
Terminating all child processes of MainProcess...
Terminated child process: SpawnProcess-7
Terminated child process: SpawnProcess-2
Terminated child process: SpawnProcess-5
Terminated child process: SpawnProcess-3
Terminated child process: SpawnProcess-6
Terminated child process: SpawnProcess-4
Terminated child process: SpawnProcess-1
Terminated child process: SpawnProcess-8
Rank 0 - Exiting program...
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ wandb login
wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)
wandb: You can find your API key in your browser here: https://wandb.ai/authorize
wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: 
wandb: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ wandb login
wandb: Currently logged in as: jbeno (jimbeno). Use `wandb login --relogin` to force relogin
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --weights_name 'google/electra-base-discriminator' --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 12 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Base' --wandb_run 'E1-EBFT-Merged' --model_file 'checkpoint_epoch_14_20241026-042844.pth' --interactive

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 1 - Device: cuda:1
Rank 2 - Device: cuda:2
Rank 7 - Device: cuda:7
Rank 3 - Device: cuda:3
Rank 5 - Device: cuda:5
Rank 4 - Device: cuda:4
Rank 6 - Device: cuda:6
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /home/ubuntu/nlp-test/sentiment/wandb/run-20241029_031515-wfhep1m5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run E1-EBFT-Merged
wandb: ⭐️ View project at https://wandb.ai/jimbeno/Electra%20Base
wandb: 🚀 View run at https://wandb.ai/jimbeno/Electra%20Base/runs/wfhep1m5
Wand run initialized.

Initializing 'google/electra-base-discriminator' tokenizer and model...
Checking for local files...
Some files not found locally, downloading...
tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48.0/48.0 [00:00<00:00, 130kB/s]
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 666/666 [00:00<00:00, 1.58MB/s]
vocab.txt: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 12.9MB/s]
tokenizer.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 10.9MB/s]
pytorch_model.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 440M/440M [00:01<00:00, 240MB/s]
Download complete
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (6s)

Loading data...
Using the same dataset for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using merged_local 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Train size: 102097
Validation size: 5421
Evaluation size: 6530
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 2352
	       Neutral: 1829
	      Positive: 2349
Data loaded (269ms)

Processing data...
(Batch size: 16, Pooling: Mean, Fine Tune BERT: True, Chunk size: None)...
Extracting sentences and labels...
X Train shape: [102097], X Validation shape: [5421], X Test shape: [6530]
y Train shape: [102097], y Validation shape: [5421], y Test shape: [6530]
Data processed (3ms)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 2, Max Grad Norm: None
Batch Size: 16, Max Epochs: 50, LR: 1e-05, Early Stop: score, Fine-tune BERT: True, Fine-tune Layers: 12, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_14_20241026-042844.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_14_20241026-042844.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (10s)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 10 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Initializing model, graph, optimizer...
BERT's original pooler removed. Using custom pooling type: mean
BERT has 108,891,648 trainable parameters and 0 non-trainable parameters
Number of BERT layers requiring gradients: 12 out of 12
Fine-tuning the last 12 out of 12 BERT layers

Model Architecture Summary:
BERTClassifier(
  (bert): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0-11): 12 x ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (custom_pooling): PoolingLayer()
  (classifier): Classifier(
    (layers): Sequential(
      (0): Linear(in_features=768, out_features=1024, bias=True)
      (1): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=1024, out_features=1024, bias=True)
      (4): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (5): Dropout(p=0.3, inplace=False)
      (6): Linear(in_features=1024, out_features=3, bias=True)
    )
  )
)

Model Parameters:
  bert.embeddings.word_embeddings: 23,440,896 (trainable)
  bert.embeddings.position_embeddings: 393,216 (trainable)
  bert.embeddings.token_type_embeddings: 1,536 (trainable)
  bert.embeddings.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.0.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.0.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.0.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.0.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.0.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.0.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.0.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.0.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.1.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.1.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.1.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.1.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.1.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.1.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.1.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.1.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.2.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.2.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.2.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.2.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.2.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.2.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.2.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.2.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.3.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.3.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.3.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.3.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.3.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.3.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.3.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.3.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.4.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.4.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.4.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.4.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.4.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.4.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.4.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.4.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.5.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.5.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.5.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.5.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.5.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.5.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.5.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.5.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.6.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.6.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.6.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.6.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.6.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.6.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.6.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.6.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.7.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.7.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.7.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.7.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.7.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.7.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.7.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.7.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.8.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.8.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.8.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.8.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.8.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.8.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.8.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.8.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.9.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.9.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.9.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.9.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.9.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.9.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.9.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.9.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.10.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.10.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.10.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.10.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.10.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.10.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.10.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.10.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.11.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.11.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.11.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.11.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.11.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.11.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.11.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.11.output.LayerNorm: 1,536 (trainable)
  classifier.layers.0: 787,456 (trainable)
  classifier.layers.1.projection: 2,099,200 (trainable)
  classifier.layers.3: 1,049,600 (trainable)
  classifier.layers.6: 3,075 (trainable)

Total layers: 104
Trainable layers: 104
Total parameters: 112,830,979
Trainable parameters: 112,830,979
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Layer-wise decay factor: 0.95
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 15, Max Iterations: 50, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 10

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: q
Quitting training...
Saving model in ONNX format...
Model saved in ONNX format to saves/model_20241029-031635.onnx and uploaded to Weights & Biases.

Evaluating model...

E1-EBFT-Merged Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.847081  0.777211  0.810643      2352
     neutral   0.704453  0.761072  0.731669      1829
    positive   0.828047  0.844615  0.836249      2349

    accuracy                       0.796937      6530
   macro avg   0.793194  0.794299  0.792854      6530
weighted avg   0.800285  0.796937  0.797734      6530

ROC AUC: 0.926344

Predicted  negative  neutral  positive
Actual                                
negative       1828      331       193
neutral         218     1392       219
positive        112      253      1984

Saved predictions: saves/predictions_20241029-031843.csv

Macro F1 Score: 0.79

Evaluation completed (3m 3s)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: eval/macro_f1_score ▁
wandb: 
wandb: Run summary:
wandb: eval/macro_f1_score 0.79285
wandb: 
wandb: 🚀 View run E1-EBFT-Merged at: https://wandb.ai/jimbeno/Electra%20Base/runs/wfhep1m5
wandb: ⭐️ View project at: https://wandb.ai/jimbeno/Electra%20Base
wandb: Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20241029_031515-wfhep1m5/logs
TOTAL Time: 4m 44s
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ git pull
Username for 'https://github.com': jim@jimbeno.net
Password for 'https://jim@jimbeno.net@github.com': 
remote: Enumerating objects: 3, done.
remote: Counting objects: 100% (3/3), done.
remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)
Unpacking objects: 100% (3/3), 327 bytes | 3.00 KiB/s, done.
From https://github.com/jbeno/sentiment
   19dbd54..1cda899  main       -> origin/main
Updating 19dbd54..1cda899
Fast-forward
 torch_ddp_finetune_neural_classifier.py | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --eval_dataset 'dynasent_r1' --weights_name 'google/electra-base-discriminator' --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 12 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Base' --wandb_run 'E1-EBFT-DYN-R1' --model_file 'checkpoint_epoch_14_20241026-042844.pth' --interactive

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 4 - Device: cuda:4
Rank 6 - Device: cuda:6
Rank 2 - Device: cuda:2
Rank 5 - Device: cuda:5
Rank 1 - Device: cuda:1
Rank 7 - Device: cuda:7
Rank 3 - Device: cuda:3
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /home/ubuntu/nlp-test/sentiment/wandb/run-20241029_032502-82fzy6df
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run E1-EBFT-DYN-R1
wandb: ⭐️ View project at https://wandb.ai/jimbeno/Electra%20Base
wandb: 🚀 View run at https://wandb.ai/jimbeno/Electra%20Base/runs/82fzy6df
Wand run initialized.

Initializing 'google/electra-base-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (3s)

Loading data...
Using different datasets for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using dynasent_r1 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: DynaSent Round 1 from Hugging Face: 'dynabench/dynasent'
Downloading builder script: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16.5k/16.5k [00:00<00:00, 22.2MB/s]
Downloading metadata: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6.97k/6.97k [00:00<00:00, 16.5MB/s]
Downloading readme: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13.7k/13.7k [00:00<00:00, 17.1MB/s]
Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17.1M/17.1M [00:00<00:00, 73.4MB/s]
Generating train split: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80488/80488 [00:12<00:00, 6352.20 examples/s]
Generating validation split: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3600/3600 [00:00<00:00, 6953.43 examples/s]
Generating test split: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3600/3600 [00:00<00:00, 7042.63 examples/s]
Train size: 102097
Validation size: 5421
Evaluation size: 3600
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 1200
	       Neutral: 1200
	      Positive: 1200
Data loaded (16s)

Processing data...
(Batch size: 16, Pooling: Mean, Fine Tune BERT: True, Chunk size: None)...
Extracting sentences and labels...
X Train shape: [102097], X Validation shape: [5421], X Test shape: [3600]
y Train shape: [102097], y Validation shape: [5421], y Test shape: [3600]
Data processed (2ms)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 2, Max Grad Norm: None
Batch Size: 16, Max Epochs: 50, LR: 1e-05, Early Stop: score, Fine-tune BERT: True, Fine-tune Layers: 12, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_14_20241026-042844.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_14_20241026-042844.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (3s)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 10 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Initializing model, graph, optimizer...
BERT's original pooler removed. Using custom pooling type: mean
BERT has 108,891,648 trainable parameters and 0 non-trainable parameters
Number of BERT layers requiring gradients: 12 out of 12
Fine-tuning the last 12 out of 12 BERT layers

Model Architecture Summary:
BERTClassifier(
  (bert): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0-11): 12 x ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (custom_pooling): PoolingLayer()
  (classifier): Classifier(
    (layers): Sequential(
      (0): Linear(in_features=768, out_features=1024, bias=True)
      (1): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=1024, out_features=1024, bias=True)
      (4): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (5): Dropout(p=0.3, inplace=False)
      (6): Linear(in_features=1024, out_features=3, bias=True)
    )
  )
)

Model Parameters:
  bert.embeddings.word_embeddings: 23,440,896 (trainable)
  bert.embeddings.position_embeddings: 393,216 (trainable)
  bert.embeddings.token_type_embeddings: 1,536 (trainable)
  bert.embeddings.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.0.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.0.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.0.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.0.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.0.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.0.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.0.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.0.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.1.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.1.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.1.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.1.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.1.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.1.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.1.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.1.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.2.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.2.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.2.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.2.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.2.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.2.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.2.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.2.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.3.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.3.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.3.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.3.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.3.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.3.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.3.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.3.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.4.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.4.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.4.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.4.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.4.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.4.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.4.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.4.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.5.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.5.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.5.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.5.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.5.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.5.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.5.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.5.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.6.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.6.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.6.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.6.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.6.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.6.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.6.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.6.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.7.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.7.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.7.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.7.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.7.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.7.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.7.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.7.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.8.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.8.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.8.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.8.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.8.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.8.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.8.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.8.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.9.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.9.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.9.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.9.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.9.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.9.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.9.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.9.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.10.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.10.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.10.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.10.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.10.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.10.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.10.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.10.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.11.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.11.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.11.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.11.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.11.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.11.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.11.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.11.output.LayerNorm: 1,536 (trainable)
  classifier.layers.0: 787,456 (trainable)
  classifier.layers.1.projection: 2,099,200 (trainable)
  classifier.layers.3: 1,049,600 (trainable)
  classifier.layers.6: 3,075 (trainable)

Total layers: 104
Trainable layers: 104
Total parameters: 112,830,979
Trainable parameters: 112,830,979
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Layer-wise decay factor: 0.95
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 15, Max Iterations: 50, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 10

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: q
Quitting training...

Evaluating model...

E1-EBFT-DYN-R1 Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.901222  0.737500  0.811182      1200
     neutral   0.745957  0.922500  0.824888      1200
    positive   0.850970  0.804167  0.826907      1200

    accuracy                       0.821389      3600
   macro avg   0.832716  0.821389  0.820992      3600
weighted avg   0.832716  0.821389  0.820992      3600

ROC AUC: 0.945131

Predicted  negative  neutral  positive
Actual                                
negative        885      201       114
neutral          38     1107        55
positive         59      176       965

Saved predictions: saves/predictions_20241029-032758.csv

Macro F1 Score: 0.82

Evaluation completed (1m 41s)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: eval/macro_f1_score ▁
wandb: 
wandb: Run summary:
wandb: eval/macro_f1_score 0.82099
wandb: 
wandb: 🚀 View run E1-EBFT-DYN-R1 at: https://wandb.ai/jimbeno/Electra%20Base/runs/82fzy6df
wandb: ⭐️ View project at: https://wandb.ai/jimbeno/Electra%20Base
wandb: Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20241029_032502-82fzy6df/logs
TOTAL Time: 3m 41s
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --eval_dataset 'dynasent_r2' --weights_name 'google/electra-base-discriminator' --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 12 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Base' --wandb_run 'E1-EBFT-DYN-R2' --model_file 'checkpoint_epoch_14_20241026-042844.pth' --interactive

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 5 - Device: cuda:5
Rank 4 - Device: cuda:4
Rank 3 - Device: cuda:3
Rank 6 - Device: cuda:6
Rank 1 - Device: cuda:1
Rank 2 - Device: cuda:2
Rank 7 - Device: cuda:7
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /home/ubuntu/nlp-test/sentiment/wandb/run-20241029_033219-kh9w5cv9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run E1-EBFT-DYN-R2
wandb: ⭐️ View project at https://wandb.ai/jimbeno/Electra%20Base
wandb: 🚀 View run at https://wandb.ai/jimbeno/Electra%20Base/runs/kh9w5cv9
Wand run initialized.

Initializing 'google/electra-base-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (3s)

Loading data...
Using different datasets for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using dynasent_r2 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: DynaSent Round 2 from Hugging Face: 'dynabench/dynasent'
Generating train split: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13065/13065 [00:02<00:00, 5431.97 examples/s]
Generating validation split: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 720/720 [00:00<00:00, 5970.46 examples/s]
Generating test split: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 720/720 [00:00<00:00, 6081.19 examples/s]
Train size: 102097
Validation size: 5421
Evaluation size: 720
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 240
	       Neutral: 240
	      Positive: 240
Data loaded (3s)

Processing data...
(Batch size: 16, Pooling: Mean, Fine Tune BERT: True, Chunk size: None)...
Extracting sentences and labels...
X Train shape: [102097], X Validation shape: [5421], X Test shape: [720]
y Train shape: [102097], y Validation shape: [5421], y Test shape: [720]
Data processed (2ms)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 2, Max Grad Norm: None
Batch Size: 16, Max Epochs: 50, LR: 1e-05, Early Stop: score, Fine-tune BERT: True, Fine-tune Layers: 12, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_14_20241026-042844.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_14_20241026-042844.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (3s)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 10 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Initializing model, graph, optimizer...
BERT's original pooler removed. Using custom pooling type: mean
BERT has 108,891,648 trainable parameters and 0 non-trainable parameters
Number of BERT layers requiring gradients: 12 out of 12
Fine-tuning the last 12 out of 12 BERT layers

Model Architecture Summary:
BERTClassifier(
  (bert): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0-11): 12 x ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (custom_pooling): PoolingLayer()
  (classifier): Classifier(
    (layers): Sequential(
      (0): Linear(in_features=768, out_features=1024, bias=True)
      (1): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=1024, out_features=1024, bias=True)
      (4): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (5): Dropout(p=0.3, inplace=False)
      (6): Linear(in_features=1024, out_features=3, bias=True)
    )
  )
)

Model Parameters:
  bert.embeddings.word_embeddings: 23,440,896 (trainable)
  bert.embeddings.position_embeddings: 393,216 (trainable)
  bert.embeddings.token_type_embeddings: 1,536 (trainable)
  bert.embeddings.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.0.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.0.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.0.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.0.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.0.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.0.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.0.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.0.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.1.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.1.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.1.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.1.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.1.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.1.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.1.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.1.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.2.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.2.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.2.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.2.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.2.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.2.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.2.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.2.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.3.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.3.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.3.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.3.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.3.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.3.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.3.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.3.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.4.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.4.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.4.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.4.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.4.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.4.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.4.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.4.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.5.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.5.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.5.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.5.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.5.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.5.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.5.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.5.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.6.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.6.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.6.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.6.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.6.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.6.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.6.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.6.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.7.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.7.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.7.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.7.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.7.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.7.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.7.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.7.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.8.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.8.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.8.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.8.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.8.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.8.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.8.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.8.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.9.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.9.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.9.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.9.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.9.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.9.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.9.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.9.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.10.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.10.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.10.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.10.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.10.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.10.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.10.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.10.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.11.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.11.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.11.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.11.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.11.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.11.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.11.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.11.output.LayerNorm: 1,536 (trainable)
  classifier.layers.0: 787,456 (trainable)
  classifier.layers.1.projection: 2,099,200 (trainable)
  classifier.layers.3: 1,049,600 (trainable)
  classifier.layers.6: 3,075 (trainable)

Total layers: 104
Trainable layers: 104
Total parameters: 112,830,979
Trainable parameters: 112,830,979
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Layer-wise decay factor: 0.95
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 15, Max Iterations: 50, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 10

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: q
Quitting training...

Evaluating model...

E1-EBFT-DYN-R2 Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.696154  0.754167  0.724000       240
     neutral   0.770408  0.629167  0.692661       240
    positive   0.704545  0.775000  0.738095       240

    accuracy                       0.719444       720
   macro avg   0.723702  0.719444  0.718252       720
weighted avg   0.723702  0.719444  0.718252       720

ROC AUC: 0.88842

Predicted  negative  neutral  positive
Actual                                
negative        181       26        33
neutral          44      151        45
positive         35       19       186

Saved predictions: saves/predictions_20241029-033411.csv

Macro F1 Score: 0.72

Evaluation completed (20s)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: eval/macro_f1_score ▁
wandb: 
wandb: Run summary:
wandb: eval/macro_f1_score 0.71825
wandb: 
wandb: 🚀 View run E1-EBFT-DYN-R2 at: https://wandb.ai/jimbeno/Electra%20Base/runs/kh9w5cv9
wandb: ⭐️ View project at: https://wandb.ai/jimbeno/Electra%20Base
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20241029_033219-kh9w5cv9/logs
TOTAL Time: 2m 9s
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --eval_dataset 'sst_local' --weights_name 'google/electra-base-discriminator' --save_preds --lr 0.00001 --lr_decay 0.95 --epochs 50 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 16 --accumulation_steps 2 --finetune_bert --finetune_layers 12 --l2_strength 0.01  --checkpoint_interval 1 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Base' --wandb_run 'E1-EBFT-SST' --model_file 'checkpoint_epoch_14_20241026-042844.pth' --interactive

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 6 - Device: cuda:6
Rank 7 - Device: cuda:7
Rank 4 - Device: cuda:4
Rank 5 - Device: cuda:5
Rank 3 - Device: cuda:3
Rank 1 - Device: cuda:1
Rank 2 - Device: cuda:2
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /home/ubuntu/nlp-test/sentiment/wandb/run-20241029_033659-wild6i10
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run E1-EBFT-SST
wandb: ⭐️ View project at https://wandb.ai/jimbeno/Electra%20Base
wandb: 🚀 View run at https://wandb.ai/jimbeno/Electra%20Base/runs/wild6i10
Wand run initialized.

Initializing 'google/electra-base-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (3s)

Loading data...
Using different datasets for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using sst_local 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: Stanford Sentiment Treebank (SST) from Local: 'data/sentiment'
Train size: 102097
Validation size: 5421
Evaluation size: 2210
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 912
	       Neutral: 389
	      Positive: 909
Data loaded (309ms)

Processing data...
(Batch size: 16, Pooling: Mean, Fine Tune BERT: True, Chunk size: None)...
Extracting sentences and labels...
X Train shape: [102097], X Validation shape: [5421], X Test shape: [2210]
y Train shape: [102097], y Validation shape: [5421], y Test shape: [2210]
Data processed (3ms)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 2, Max Grad Norm: None
Batch Size: 16, Max Epochs: 50, LR: 1e-05, Early Stop: score, Fine-tune BERT: True, Fine-tune Layers: 12, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_14_20241026-042844.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_14_20241026-042844.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (3s)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 10 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Initializing model, graph, optimizer...
BERT's original pooler removed. Using custom pooling type: mean
BERT has 108,891,648 trainable parameters and 0 non-trainable parameters
Number of BERT layers requiring gradients: 12 out of 12
Fine-tuning the last 12 out of 12 BERT layers

Model Architecture Summary:
BERTClassifier(
  (bert): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0-11): 12 x ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (custom_pooling): PoolingLayer()
  (classifier): Classifier(
    (layers): Sequential(
      (0): Linear(in_features=768, out_features=1024, bias=True)
      (1): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=1024, out_features=1024, bias=True)
      (4): SwishGLU(
        (projection): Linear(in_features=1024, out_features=2048, bias=True)
        (activation): SiLU()
      )
      (5): Dropout(p=0.3, inplace=False)
      (6): Linear(in_features=1024, out_features=3, bias=True)
    )
  )
)

Model Parameters:
  bert.embeddings.word_embeddings: 23,440,896 (trainable)
  bert.embeddings.position_embeddings: 393,216 (trainable)
  bert.embeddings.token_type_embeddings: 1,536 (trainable)
  bert.embeddings.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.0.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.0.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.0.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.0.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.0.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.0.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.0.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.0.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.1.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.1.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.1.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.1.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.1.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.1.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.1.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.1.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.2.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.2.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.2.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.2.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.2.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.2.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.2.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.2.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.3.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.3.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.3.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.3.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.3.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.3.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.3.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.3.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.4.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.4.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.4.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.4.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.4.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.4.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.4.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.4.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.5.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.5.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.5.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.5.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.5.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.5.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.5.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.5.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.6.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.6.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.6.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.6.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.6.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.6.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.6.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.6.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.7.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.7.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.7.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.7.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.7.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.7.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.7.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.7.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.8.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.8.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.8.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.8.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.8.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.8.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.8.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.8.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.9.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.9.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.9.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.9.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.9.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.9.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.9.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.9.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.10.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.10.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.10.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.10.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.10.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.10.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.10.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.10.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.11.attention.self.query: 590,592 (trainable)
  bert.encoder.layer.11.attention.self.key: 590,592 (trainable)
  bert.encoder.layer.11.attention.self.value: 590,592 (trainable)
  bert.encoder.layer.11.attention.output.dense: 590,592 (trainable)
  bert.encoder.layer.11.attention.output.LayerNorm: 1,536 (trainable)
  bert.encoder.layer.11.intermediate.dense: 2,362,368 (trainable)
  bert.encoder.layer.11.output.dense: 2,360,064 (trainable)
  bert.encoder.layer.11.output.LayerNorm: 1,536 (trainable)
  classifier.layers.0: 787,456 (trainable)
  classifier.layers.1.projection: 2,099,200 (trainable)
  classifier.layers.3: 1,049,600 (trainable)
  classifier.layers.6: 3,075 (trainable)

Total layers: 104
Trainable layers: 104
Total parameters: 112,830,979
Trainable parameters: 112,830,979
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Layer-wise decay factor: 0.95
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 15, Max Iterations: 50, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 10

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: q
Quitting training...

Evaluating model...

E1-EBFT-SST Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.831878  0.835526  0.833698       912
     neutral   0.452703  0.344473  0.391241       389
    positive   0.834669  0.916392  0.873623       909

    accuracy                       0.782353      2210
   macro avg   0.706417  0.698797  0.699521      2210
weighted avg   0.766284  0.782353  0.772239      2210

ROC AUC: 0.885009

Predicted  negative  neutral  positive
Actual                                
negative        762      104        46
neutral         136      134       119
positive         18       58       833

Saved predictions: saves/predictions_20241029-033953.csv

Macro F1 Score: 0.70

Evaluation completed (1m 2s)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: eval/macro_f1_score ▁
wandb: 
wandb: Run summary:
wandb: eval/macro_f1_score 0.69952
wandb: 
wandb: 🚀 View run E1-EBFT-SST at: https://wandb.ai/jimbeno/Electra%20Base/runs/wild6i10
wandb: ⭐️ View project at: https://wandb.ai/jimbeno/Electra%20Base
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20241029_033659-wild6i10/logs
TOTAL Time: 3m 26s
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --weights_name 'google/electra-base-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --epochs 100 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 32 --l2_strength 0.01  --checkpoint_interval 5 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --show_progress --wandb --wandb_project 'Electra Base' --wandb_run 'B1-EB-Merged'

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 4 - Device: cuda:4
Rank 2 - Device: cuda:2
Rank 1 - Device: cuda:1
Rank 5 - Device: cuda:5
Rank 6 - Device: cuda:6
Rank 3 - Device: cuda:3
Rank 7 - Device: cuda:7
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /home/ubuntu/nlp-test/sentiment/wandb/run-20241029_040221-k6b9mvmn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run B1-EB-Merged
wandb: ⭐️ View project at https://wandb.ai/jimbeno/Electra%20Base
wandb: 🚀 View run at https://wandb.ai/jimbeno/Electra%20Base/runs/k6b9mvmn
Wand run initialized.

Initializing 'google/electra-base-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (3s)

Loading data...
Using the same dataset for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using merged_local 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Train size: 102097
Validation size: 5421
Evaluation size: 6530
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 2352
	       Neutral: 1829
	      Positive: 2349
Data loaded (291ms)

Processing data...
(Batch size: 32, Pooling: Mean, Fine Tune BERT: False, Chunk size: None)...
Extracting sentences and labels...

Displaying samples from Train data:
Train[84457]: she blurted. - NEUTRAL
Tokens: ['she', 'blurted', '.']
Embedding: [ 0.0033753   0.17586878  0.32458916 -0.54597694  0.37396565  0.2769468 ] ...

Train[33315]: $10 is the minimum tip they ask for. - NEUTRAL
Tokens: ['$', '10', 'is', 'the', 'minimum', 'tip', 'they', 'ask', 'for', '.']
Embedding: [-0.12106011  0.10919575  0.07619347 -0.54361504 -0.21918344  0.24642695] ...

Train[95755]: Once arriving home, we opened our food. - NEUTRAL
Tokens: ['once', 'arriving', 'home', ',', 'we', 'opened', 'our', 'food', '.']
Embedding: [ 0.23126072  0.0258252   0.22760166 -0.25229746  0.00471391  0.27857935] ...


Encoding Train data of 102097 texts distributed across 8 GPUs...
Batch Size: 32, Pooling: Mean, Empty Cache: False
Padding Train data to 102104 texts for even distribution across 8 ranks...
Texts per rank: 12763, Total batches: 3191
Rank 1: Processing 12763 texts (indices 12763 to 25525) in 399 batches...
Rank 2: Processing 12763 texts (indices 25526 to 38288) in 399 batches...
Rank 4: Processing 12763 texts (indices 51052 to 63814) in 399 batches...
Rank 5: Processing 12763 texts (indices 63815 to 76577) in 399 batches...
Rank 3: Processing 12763 texts (indices 38289 to 51051) in 399 batches...
Rank 7: Processing 12763 texts (indices 89341 to 102103) in 399 batches...
Rank 6: Processing 12763 texts (indices 76578 to 89340) in 399 batches...
Rank 0: Processing 12763 texts (indices 0 to 12762) in 399 batches...
Rank 0: Batch  1 / 399, Shape: [32, 768], Time: 26ms
Rank 1: Batch  1 / 399, Shape: [32, 768], Time: 212ms
Rank 7: Batch  1 / 399, Shape: [32, 768], Time: 250ms
Rank 4: Batch  1 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch  1 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch  1 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch  1 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch  1 / 399, Shape: [32, 768], Time: 290ms
Rank 7: Batch  2 / 399, Shape: [32, 768], Time: 248ms
Rank 1: Batch  2 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch  2 / 399, Shape: [32, 768], Time: 292ms
Rank 5: Batch  2 / 399, Shape: [32, 768], Time: 291ms
Rank 3: Batch  2 / 399, Shape: [32, 768], Time: 288ms
Rank 2: Batch  2 / 399, Shape: [32, 768], Time: 292ms
Rank 6: Batch  2 / 399, Shape: [32, 768], Time: 291ms
Rank 1: Batch  3 / 399, Shape: [32, 768], Time: 272ms
Rank 7: Batch  3 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch  3 / 399, Shape: [32, 768], Time: 263ms
Rank 3: Batch  3 / 399, Shape: [32, 768], Time: 263ms
Rank 5: Batch  3 / 399, Shape: [32, 768], Time: 266ms
Rank 6: Batch  3 / 399, Shape: [32, 768], Time: 273ms
Rank 2: Batch  3 / 399, Shape: [32, 768], Time: 278ms
Rank 1: Batch  4 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch  4 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch  4 / 399, Shape: [32, 768], Time: 278ms
Rank 5: Batch  4 / 399, Shape: [32, 768], Time: 278ms
Rank 6: Batch  4 / 399, Shape: [32, 768], Time: 275ms
Rank 3: Batch  4 / 399, Shape: [32, 768], Time: 292ms
Rank 2: Batch  4 / 399, Shape: [32, 768], Time: 281ms
Rank 1: Batch  5 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch  5 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch  5 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch  5 / 399, Shape: [32, 768], Time: 292ms
Rank 6: Batch  5 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch  5 / 399, Shape: [32, 768], Time: 281ms
Rank 2: Batch  5 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch  6 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch  6 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch  6 / 399, Shape: [32, 768], Time: 277ms
Rank 4: Batch  6 / 399, Shape: [32, 768], Time: 278ms
Rank 5: Batch  6 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch  6 / 399, Shape: [32, 768], Time: 279ms
Rank 2: Batch  6 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch  7 / 399, Shape: [32, 768], Time: 280ms
Rank 7: Batch  7 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch  7 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch  7 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch  7 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch  7 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch  7 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch  8 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch  8 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch  8 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch  8 / 399, Shape: [32, 768], Time: 280ms
Rank 3: Batch  8 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch  8 / 399, Shape: [32, 768], Time: 279ms
Rank 2: Batch  8 / 399, Shape: [32, 768], Time: 284ms
Memory: Rank 0: 2.65 GB | Rank 1: 3.00 GB | Rank 2: 2.95 GB | Rank 3: 2.96 GB | Rank 4: 3.03 GB | Rank 5: 3.05 GB | Rank 6: 3.00 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 0: Batch  2 / 399, Shape: [32, 768], Time: 29ms
Rank 1: Batch  9 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch  9 / 399, Shape: [32, 768], Time: 291ms
Rank 4: Batch  9 / 399, Shape: [32, 768], Time: 279ms
Rank 5: Batch  9 / 399, Shape: [32, 768], Time: 279ms
Rank 6: Batch  9 / 399, Shape: [32, 768], Time: 279ms
Rank 3: Batch  9 / 399, Shape: [32, 768], Time: 280ms
Rank 2: Batch  9 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch  3 / 399, Shape: [32, 768], Time: 273ms
Rank 1: Batch 10 / 399, Shape: [32, 768], Time: 280ms
Rank 7: Batch 10 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 10 / 399, Shape: [32, 768], Time: 282ms
Rank 5: Batch 10 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 10 / 399, Shape: [32, 768], Time: 280ms
Rank 3: Batch 10 / 399, Shape: [32, 768], Time: 281ms
Rank 2: Batch 10 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch  4 / 399, Shape: [32, 768], Time: 278ms
Rank 1: Batch 11 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch 11 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 11 / 399, Shape: [32, 768], Time: 276ms
Rank 5: Batch 11 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 11 / 399, Shape: [32, 768], Time: 280ms
Rank 3: Batch 11 / 399, Shape: [32, 768], Time: 281ms
Rank 2: Batch 11 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch  5 / 399, Shape: [32, 768], Time: 279ms
Rank 1: Batch 12 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch 12 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 12 / 399, Shape: [32, 768], Time: 279ms
Rank 5: Batch 12 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 12 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 12 / 399, Shape: [32, 768], Time: 281ms
Rank 2: Batch 12 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch  6 / 399, Shape: [32, 768], Time: 280ms
Memory: Rank 0: 2.70 GB | Rank 1: 3.06 GB | Rank 2: 2.95 GB | Rank 3: 2.96 GB | Rank 4: 3.03 GB | Rank 5: 3.05 GB | Rank 6: 3.00 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 13 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 13 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 13 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch 13 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 13 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 13 / 399, Shape: [32, 768], Time: 281ms
Rank 2: Batch 13 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch  7 / 399, Shape: [32, 768], Time: 271ms
Rank 1: Batch 14 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch 14 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 14 / 399, Shape: [32, 768], Time: 279ms
Rank 5: Batch 14 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 14 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 14 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 14 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch  8 / 399, Shape: [32, 768], Time: 280ms
Rank 1: Batch 15 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 15 / 399, Shape: [32, 768], Time: 279ms
Rank 7: Batch 15 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 15 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 15 / 399, Shape: [32, 768], Time: 279ms
Rank 3: Batch 15 / 399, Shape: [32, 768], Time: 281ms
Rank 2: Batch 15 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch  9 / 399, Shape: [32, 768], Time: 279ms
Rank 1: Batch 16 / 399, Shape: [32, 768], Time: 281ms
Rank 4: Batch 16 / 399, Shape: [32, 768], Time: 280ms
Rank 7: Batch 16 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 16 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 16 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 16 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 16 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 10 / 399, Shape: [32, 768], Time: 279ms
Rank 1: Batch 17 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 17 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 17 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch 17 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 17 / 399, Shape: [32, 768], Time: 280ms
Rank 3: Batch 17 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 17 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 11 / 399, Shape: [32, 768], Time: 279ms
Memory: Rank 0: 2.70 GB | Rank 1: 3.06 GB | Rank 2: 2.95 GB | Rank 3: 2.96 GB | Rank 4: 3.03 GB | Rank 5: 3.05 GB | Rank 6: 3.00 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 18 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 18 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 18 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch 18 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 18 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 18 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 18 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 12 / 399, Shape: [32, 768], Time: 276ms
Rank 1: Batch 19 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 19 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch 19 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 19 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 19 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 19 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 19 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 13 / 399, Shape: [32, 768], Time: 280ms
Rank 1: Batch 20 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 20 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 20 / 399, Shape: [32, 768], Time: 279ms
Rank 5: Batch 20 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 20 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch 20 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 20 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 14 / 399, Shape: [32, 768], Time: 280ms
Rank 1: Batch 21 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 21 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 21 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch 21 / 399, Shape: [32, 768], Time: 280ms
Rank 3: Batch 21 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 21 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 21 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 15 / 399, Shape: [32, 768], Time: 279ms
Rank 1: Batch 22 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 22 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 22 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 22 / 399, Shape: [32, 768], Time: 280ms
Rank 3: Batch 22 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 22 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 22 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 16 / 399, Shape: [32, 768], Time: 280ms
Memory: Rank 0: 2.70 GB | Rank 1: 3.06 GB | Rank 2: 2.96 GB | Rank 3: 2.96 GB | Rank 4: 3.04 GB | Rank 5: 3.05 GB | Rank 6: 3.00 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 23 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 23 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 23 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch 23 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 23 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch 23 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 23 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 17 / 399, Shape: [32, 768], Time: 277ms
Rank 1: Batch 24 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 24 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 24 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch 24 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 24 / 399, Shape: [32, 768], Time: 280ms
Rank 7: Batch 24 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 24 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 18 / 399, Shape: [32, 768], Time: 279ms
Rank 1: Batch 25 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 25 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 25 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 25 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 25 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 25 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 25 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 19 / 399, Shape: [32, 768], Time: 280ms
Rank 1: Batch 26 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 26 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 26 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 26 / 399, Shape: [32, 768], Time: 280ms
Rank 3: Batch 26 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 26 / 399, Shape: [32, 768], Time: 296ms
Rank 2: Batch 26 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 20 / 399, Shape: [32, 768], Time: 280ms
Rank 1: Batch 27 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 27 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 27 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch 27 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 27 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 27 / 399, Shape: [32, 768], Time: 280ms
Rank 0: Batch 21 / 399, Shape: [32, 768], Time: 278ms
Rank 2: Batch 27 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.71 GB | Rank 1: 3.06 GB | Rank 2: 2.96 GB | Rank 3: 2.96 GB | Rank 4: 3.04 GB | Rank 5: 3.05 GB | Rank 6: 3.00 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 28 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 28 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 28 / 399, Shape: [32, 768], Time: 279ms
Rank 5: Batch 28 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 28 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 28 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 22 / 399, Shape: [32, 768], Time: 274ms
Rank 2: Batch 28 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 29 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 29 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 29 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 29 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 29 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 23 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch 29 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 29 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 30 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 30 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 30 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 30 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 30 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 24 / 399, Shape: [32, 768], Time: 280ms
Rank 7: Batch 30 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 30 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 31 / 399, Shape: [32, 768], Time: 281ms
Rank 4: Batch 31 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 31 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch 31 / 399, Shape: [32, 768], Time: 280ms
Rank 3: Batch 31 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 25 / 399, Shape: [32, 768], Time: 280ms
Rank 7: Batch 31 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 31 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 32 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 32 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 32 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch 32 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 32 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 26 / 399, Shape: [32, 768], Time: 280ms
Memory: Rank 0: 2.71 GB | Rank 1: 3.06 GB | Rank 2: 2.96 GB | Rank 3: 2.96 GB | Rank 4: 3.04 GB | Rank 5: 3.05 GB | Rank 6: 3.00 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 32 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 32 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 33 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 33 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 33 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 33 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 33 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 27 / 399, Shape: [32, 768], Time: 278ms
Rank 2: Batch 33 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 33 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 34 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 34 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 34 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch 34 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 34 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 28 / 399, Shape: [32, 768], Time: 279ms
Rank 2: Batch 34 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 34 / 399, Shape: [32, 768], Time: 295ms
Rank 1: Batch 35 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 35 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 35 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch 35 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 35 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 29 / 399, Shape: [32, 768], Time: 280ms
Rank 2: Batch 35 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 35 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 36 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 36 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 36 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 36 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 36 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 30 / 399, Shape: [32, 768], Time: 281ms
Rank 2: Batch 36 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 36 / 399, Shape: [32, 768], Time: 290ms
Rank 1: Batch 37 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 37 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 37 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch 37 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 37 / 399, Shape: [32, 768], Time: 281ms
Rank 0: Batch 31 / 399, Shape: [32, 768], Time: 278ms
Memory: Rank 0: 2.71 GB | Rank 1: 3.06 GB | Rank 2: 2.96 GB | Rank 3: 2.96 GB | Rank 4: 3.04 GB | Rank 5: 3.05 GB | Rank 6: 3.00 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 37 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 37 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 38 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 38 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 38 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 38 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 38 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 32 / 399, Shape: [32, 768], Time: 278ms
Rank 2: Batch 38 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 38 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 39 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 39 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 39 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 39 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 39 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 33 / 399, Shape: [32, 768], Time: 280ms
Rank 2: Batch 39 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 39 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 40 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 40 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 40 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 40 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 40 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 34 / 399, Shape: [32, 768], Time: 280ms
Rank 2: Batch 40 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 40 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 41 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 41 / 399, Shape: [32, 768], Time: 295ms
Rank 6: Batch 41 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 41 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 41 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 35 / 399, Shape: [32, 768], Time: 281ms
Rank 2: Batch 41 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 41 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 42 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 42 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 42 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch 42 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 42 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 36 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.71 GB | Rank 1: 3.06 GB | Rank 2: 2.96 GB | Rank 3: 2.96 GB | Rank 4: 3.04 GB | Rank 5: 3.06 GB | Rank 6: 3.00 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 42 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 42 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 43 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 43 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 43 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 43 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 43 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 37 / 399, Shape: [32, 768], Time: 278ms
Rank 2: Batch 43 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 43 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 44 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 44 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 44 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 44 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 44 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 38 / 399, Shape: [32, 768], Time: 281ms
Rank 2: Batch 44 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 44 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 45 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 45 / 399, Shape: [32, 768], Time: 279ms
Rank 6: Batch 45 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 45 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 45 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 39 / 399, Shape: [32, 768], Time: 280ms
Rank 2: Batch 45 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 45 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 46 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 46 / 399, Shape: [32, 768], Time: 281ms
Rank 4: Batch 46 / 399, Shape: [32, 768], Time: 282ms
Rank 5: Batch 46 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 46 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 40 / 399, Shape: [32, 768], Time: 281ms
Rank 2: Batch 46 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 46 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 47 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 47 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 47 / 399, Shape: [32, 768], Time: 282ms
Rank 5: Batch 47 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 47 / 399, Shape: [32, 768], Time: 281ms
Rank 0: Batch 41 / 399, Shape: [32, 768], Time: 284ms
Memory: Rank 0: 2.71 GB | Rank 1: 3.06 GB | Rank 2: 2.96 GB | Rank 3: 2.96 GB | Rank 4: 3.04 GB | Rank 5: 3.06 GB | Rank 6: 3.00 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 47 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 47 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 48 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 48 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 48 / 399, Shape: [32, 768], Time: 282ms
Rank 5: Batch 48 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 48 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 42 / 399, Shape: [32, 768], Time: 277ms
Rank 2: Batch 48 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 48 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 49 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 49 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 49 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 49 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 43 / 399, Shape: [32, 768], Time: 280ms
Rank 3: Batch 49 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 49 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 49 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 50 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 50 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 50 / 399, Shape: [32, 768], Time: 282ms
Rank 5: Batch 50 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 44 / 399, Shape: [32, 768], Time: 280ms
Rank 3: Batch 50 / 399, Shape: [32, 768], Time: 281ms
Rank 2: Batch 50 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 50 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 51 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 51 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 51 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 51 / 399, Shape: [32, 768], Time: 281ms
Rank 0: Batch 45 / 399, Shape: [32, 768], Time: 280ms
Rank 3: Batch 51 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 51 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 51 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 52 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 52 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 52 / 399, Shape: [32, 768], Time: 282ms
Rank 5: Batch 52 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 46 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 52 / 399, Shape: [32, 768], Time: 284ms
Memory: Rank 0: 2.71 GB | Rank 1: 3.06 GB | Rank 2: 2.96 GB | Rank 3: 2.96 GB | Rank 4: 3.04 GB | Rank 5: 3.06 GB | Rank 6: 3.00 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 52 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch 52 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 53 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 53 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 53 / 399, Shape: [32, 768], Time: 282ms
Rank 5: Batch 53 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 47 / 399, Shape: [32, 768], Time: 274ms
Rank 3: Batch 53 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 53 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 53 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 54 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 54 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 54 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch 54 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 48 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 54 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 54 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 54 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 55 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 55 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 55 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 55 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 49 / 399, Shape: [32, 768], Time: 280ms
Rank 3: Batch 55 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 55 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 55 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 56 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 56 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 56 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 56 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 50 / 399, Shape: [32, 768], Time: 280ms
Rank 3: Batch 56 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 56 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 56 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 57 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 57 / 399, Shape: [32, 768], Time: 281ms
Rank 1: Batch 57 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 57 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 51 / 399, Shape: [32, 768], Time: 280ms
Memory: Rank 0: 2.71 GB | Rank 1: 3.06 GB | Rank 2: 2.96 GB | Rank 3: 2.96 GB | Rank 4: 3.04 GB | Rank 5: 3.06 GB | Rank 6: 3.00 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch 57 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 57 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 57 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 58 / 399, Shape: [32, 768], Time: 281ms
Rank 4: Batch 58 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 58 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 58 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 52 / 399, Shape: [32, 768], Time: 278ms
Rank 3: Batch 58 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 58 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 58 / 399, Shape: [32, 768], Time: 290ms
Rank 6: Batch 59 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 59 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 59 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 59 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 53 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 59 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 59 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 59 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 60 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 60 / 399, Shape: [32, 768], Time: 281ms
Rank 1: Batch 60 / 399, Shape: [32, 768], Time: 288ms
Rank 5: Batch 60 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 54 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 60 / 399, Shape: [32, 768], Time: 292ms
Rank 2: Batch 60 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 60 / 399, Shape: [32, 768], Time: 295ms
Rank 6: Batch 61 / 399, Shape: [32, 768], Time: 281ms
Rank 4: Batch 61 / 399, Shape: [32, 768], Time: 281ms
Rank 1: Batch 61 / 399, Shape: [32, 768], Time: 282ms
Rank 5: Batch 61 / 399, Shape: [32, 768], Time: 281ms
Rank 0: Batch 55 / 399, Shape: [32, 768], Time: 280ms
Rank 3: Batch 61 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 61 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 61 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 62 / 399, Shape: [32, 768], Time: 281ms
Rank 4: Batch 62 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 62 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 62 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 56 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.71 GB | Rank 1: 3.06 GB | Rank 2: 2.96 GB | Rank 3: 2.97 GB | Rank 4: 3.04 GB | Rank 5: 3.06 GB | Rank 6: 3.01 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch 62 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 62 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 62 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 63 / 399, Shape: [32, 768], Time: 281ms
Rank 4: Batch 63 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 63 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 63 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 57 / 399, Shape: [32, 768], Time: 278ms
Rank 3: Batch 63 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 63 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 63 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 64 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 64 / 399, Shape: [32, 768], Time: 281ms
Rank 1: Batch 64 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 64 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 58 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 64 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 64 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 65 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch 64 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 65 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 65 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 65 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 59 / 399, Shape: [32, 768], Time: 280ms
Rank 3: Batch 65 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 65 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 66 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 66 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 65 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 66 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 66 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 60 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 66 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 66 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 67 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 67 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 66 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 67 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 67 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 61 / 399, Shape: [32, 768], Time: 280ms
Memory: Rank 0: 2.71 GB | Rank 1: 3.06 GB | Rank 2: 2.96 GB | Rank 3: 2.97 GB | Rank 4: 3.04 GB | Rank 5: 3.06 GB | Rank 6: 3.01 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch 67 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 67 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 68 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 68 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 67 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 68 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 68 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 62 / 399, Shape: [32, 768], Time: 273ms
Rank 3: Batch 68 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 68 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 69 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 69 / 399, Shape: [32, 768], Time: 280ms
Rank 7: Batch 68 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 69 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 69 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 63 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 69 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 69 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 70 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 70 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch 69 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 70 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 64 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch 70 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 70 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 70 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 71 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 71 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 71 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 70 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 65 / 399, Shape: [32, 768], Time: 282ms
Rank 5: Batch 71 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 71 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 71 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 72 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 72 / 399, Shape: [32, 768], Time: 280ms
Rank 1: Batch 72 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 71 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 66 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.71 GB | Rank 1: 3.06 GB | Rank 2: 2.96 GB | Rank 3: 2.97 GB | Rank 4: 3.04 GB | Rank 5: 3.06 GB | Rank 6: 3.01 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 5: Batch 72 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 72 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 72 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 73 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 73 / 399, Shape: [32, 768], Time: 281ms
Rank 1: Batch 73 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 72 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 67 / 399, Shape: [32, 768], Time: 277ms
Rank 5: Batch 73 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 73 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 73 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 74 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 74 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 74 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 73 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 68 / 399, Shape: [32, 768], Time: 282ms
Rank 5: Batch 74 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 74 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 74 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 75 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 75 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 75 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 69 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 74 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 75 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 75 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 75 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 76 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 76 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 76 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 70 / 399, Shape: [32, 768], Time: 280ms
Rank 5: Batch 76 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 75 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 76 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 76 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 77 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 77 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 77 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 71 / 399, Shape: [32, 768], Time: 281ms
Memory: Rank 0: 2.71 GB | Rank 1: 3.06 GB | Rank 2: 2.96 GB | Rank 3: 2.97 GB | Rank 4: 3.04 GB | Rank 5: 3.06 GB | Rank 6: 3.01 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 5: Batch 77 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 76 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 77 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 77 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 78 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 78 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 78 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 72 / 399, Shape: [32, 768], Time: 279ms
Rank 5: Batch 78 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 77 / 399, Shape: [32, 768], Time: 291ms
Rank 3: Batch 78 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 78 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 79 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 79 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 79 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 73 / 399, Shape: [32, 768], Time: 281ms
Rank 5: Batch 79 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 78 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 79 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 79 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 80 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 80 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 74 / 399, Shape: [32, 768], Time: 281ms
Rank 1: Batch 80 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 80 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 79 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 80 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 80 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 81 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 81 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 75 / 399, Shape: [32, 768], Time: 281ms
Rank 1: Batch 81 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 81 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 80 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 81 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 81 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 82 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 82 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 76 / 399, Shape: [32, 768], Time: 281ms
Memory: Rank 0: 2.71 GB | Rank 1: 3.06 GB | Rank 2: 2.96 GB | Rank 3: 2.97 GB | Rank 4: 3.04 GB | Rank 5: 3.06 GB | Rank 6: 3.01 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 82 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 82 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 81 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 82 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 82 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 83 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 83 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 77 / 399, Shape: [32, 768], Time: 278ms
Rank 1: Batch 83 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 83 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 82 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 83 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 83 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 84 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 84 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 78 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 84 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 84 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 83 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 84 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 84 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 85 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 85 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 79 / 399, Shape: [32, 768], Time: 280ms
Rank 1: Batch 85 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 85 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 84 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 85 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 85 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 86 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 86 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 80 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 86 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 86 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 85 / 399, Shape: [32, 768], Time: 291ms
Rank 3: Batch 86 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 86 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 87 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 87 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 81 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.71 GB | Rank 1: 3.06 GB | Rank 2: 2.96 GB | Rank 3: 2.97 GB | Rank 4: 3.04 GB | Rank 5: 3.06 GB | Rank 6: 3.01 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 87 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 87 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 86 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 87 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 88 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 87 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 88 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 82 / 399, Shape: [32, 768], Time: 278ms
Rank 1: Batch 88 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 88 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 88 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 87 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 89 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 88 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 89 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 83 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 89 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 89 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 89 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 88 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 90 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 90 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 89 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 84 / 399, Shape: [32, 768], Time: 281ms
Rank 1: Batch 90 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 90 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 90 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 89 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 91 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 91 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 90 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 85 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 91 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 91 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 91 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 90 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 92 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 92 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 91 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 86 / 399, Shape: [32, 768], Time: 281ms
Memory: Rank 0: 2.71 GB | Rank 1: 3.06 GB | Rank 2: 2.96 GB | Rank 3: 2.97 GB | Rank 4: 3.04 GB | Rank 5: 3.06 GB | Rank 6: 3.01 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 92 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 92 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 92 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 91 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 93 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 93 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 92 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 87 / 399, Shape: [32, 768], Time: 279ms
Rank 1: Batch 93 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 93 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 93 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 92 / 399, Shape: [32, 768], Time: 298ms
Rank 4: Batch 94 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 94 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 93 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 88 / 399, Shape: [32, 768], Time: 280ms
Rank 1: Batch 94 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 94 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 94 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 93 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 95 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 95 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 94 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 89 / 399, Shape: [32, 768], Time: 280ms
Rank 1: Batch 95 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 95 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 95 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 94 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 96 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 96 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 95 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 90 / 399, Shape: [32, 768], Time: 280ms
Rank 1: Batch 96 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 96 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 96 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 95 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 97 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 97 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 96 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 91 / 399, Shape: [32, 768], Time: 280ms
Memory: Rank 0: 2.71 GB | Rank 1: 3.06 GB | Rank 2: 2.96 GB | Rank 3: 2.97 GB | Rank 4: 3.04 GB | Rank 5: 3.06 GB | Rank 6: 3.01 GB | Rank 7: 2.98 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 97 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 97 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 97 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 96 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 98 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 98 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 97 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 92 / 399, Shape: [32, 768], Time: 279ms
Rank 1: Batch 98 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 98 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 98 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 97 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 99 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 99 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 93 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 98 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 99 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 99 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 99 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 98 / 399, Shape: [32, 768], Time: 292ms
Rank 4: Batch 100 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 100 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 94 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 99 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 100 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 100 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 100 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 99 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 101 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 101 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 95 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 100 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 101 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 101 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 101 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 100 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 102 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 102 / 399, Shape: [32, 768], Time: 281ms
Rank 0: Batch 96 / 399, Shape: [32, 768], Time: 281ms
Memory: Rank 0: 2.71 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.97 GB | Rank 4: 3.04 GB | Rank 5: 3.06 GB | Rank 6: 3.01 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 101 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 102 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 102 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 102 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 101 / 399, Shape: [32, 768], Time: 291ms
Rank 4: Batch 103 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 103 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 97 / 399, Shape: [32, 768], Time: 269ms
Rank 2: Batch 102 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 103 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 103 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 103 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 102 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 104 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 104 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 98 / 399, Shape: [32, 768], Time: 281ms
Rank 2: Batch 103 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 104 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 104 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 104 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 103 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 105 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 105 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 99 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 104 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 105 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 105 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 105 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 104 / 399, Shape: [32, 768], Time: 291ms
Rank 4: Batch 106 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 106 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 100 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 105 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 106 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 106 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 106 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 105 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 107 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 107 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 101 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.97 GB | Rank 4: 3.04 GB | Rank 5: 3.06 GB | Rank 6: 3.01 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 106 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 107 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 107 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 107 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 106 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 108 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 108 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 102 / 399, Shape: [32, 768], Time: 279ms
Rank 2: Batch 107 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 108 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 108 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 108 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 107 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 109 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 109 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 103 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 108 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 109 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 109 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 109 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 108 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 110 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 110 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 104 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 109 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 110 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 110 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 110 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 111 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 109 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 111 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 105 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 110 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 111 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 111 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 111 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 112 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 110 / 399, Shape: [32, 768], Time: 291ms
Rank 6: Batch 112 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 106 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.97 GB | Rank 4: 3.04 GB | Rank 5: 3.06 GB | Rank 6: 3.01 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 111 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 112 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 112 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 112 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 113 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 111 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 113 / 399, Shape: [32, 768], Time: 279ms
Rank 0: Batch 107 / 399, Shape: [32, 768], Time: 279ms
Rank 2: Batch 112 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 113 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 113 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 113 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 114 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 112 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 114 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 108 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 113 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 114 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 114 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 114 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 115 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 115 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 113 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 109 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 114 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 115 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 115 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 115 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 116 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 116 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 114 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 110 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 115 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 116 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 116 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 116 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 117 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 117 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 111 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 115 / 399, Shape: [32, 768], Time: 288ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.97 GB | Rank 4: 3.05 GB | Rank 5: 3.06 GB | Rank 6: 3.01 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 116 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 117 / 399, Shape: [32, 768], Time: 290ms
Rank 1: Batch 117 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 117 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 118 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 118 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 112 / 399, Shape: [32, 768], Time: 276ms
Rank 7: Batch 116 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 117 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 118 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 118 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 118 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 119 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 119 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 113 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 117 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 118 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 119 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 119 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 119 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 120 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 120 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 114 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 118 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 119 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 120 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 120 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 120 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 121 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 121 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 115 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 119 / 399, Shape: [32, 768], Time: 288ms
Rank 2: Batch 120 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 121 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 121 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 121 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 122 / 399, Shape: [32, 768], Time: 278ms
Rank 6: Batch 122 / 399, Shape: [32, 768], Time: 281ms
Rank 0: Batch 116 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.97 GB | Rank 4: 3.05 GB | Rank 5: 3.06 GB | Rank 6: 3.01 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 120 / 399, Shape: [32, 768], Time: 288ms
Rank 2: Batch 121 / 399, Shape: [32, 768], Time: 288ms
Rank 5: Batch 122 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 122 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 122 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 123 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 123 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 117 / 399, Shape: [32, 768], Time: 279ms
Rank 7: Batch 121 / 399, Shape: [32, 768], Time: 291ms
Rank 2: Batch 122 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 123 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 123 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 123 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 124 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 124 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 118 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 122 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 123 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 124 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 124 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 124 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 125 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 125 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 119 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 123 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 124 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 125 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 125 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 125 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 126 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 126 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 120 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 124 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 125 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 126 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 126 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 126 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 127 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 127 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 121 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.97 GB | Rank 4: 3.05 GB | Rank 5: 3.06 GB | Rank 6: 3.01 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 125 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 126 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 127 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 127 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 127 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 128 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 128 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 122 / 399, Shape: [32, 768], Time: 279ms
Rank 7: Batch 126 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 127 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 128 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 128 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 128 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 129 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 123 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 129 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 127 / 399, Shape: [32, 768], Time: 288ms
Rank 5: Batch 129 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 128 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 129 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 129 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 130 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 124 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 130 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 128 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 130 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 129 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 130 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 130 / 399, Shape: [32, 768], Time: 282ms
Rank 4: Batch 131 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 125 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 131 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 129 / 399, Shape: [32, 768], Time: 288ms
Rank 5: Batch 131 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 130 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 131 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 131 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 132 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 126 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 132 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.97 GB | Rank 4: 3.05 GB | Rank 5: 3.06 GB | Rank 6: 3.01 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 130 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 132 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 131 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 132 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 132 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 133 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 127 / 399, Shape: [32, 768], Time: 279ms
Rank 6: Batch 133 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 131 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 133 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 132 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 133 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 133 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 134 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 128 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 134 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 132 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 134 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 133 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 134 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 134 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 135 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 129 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 135 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 133 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 135 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 134 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 135 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 135 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 136 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 130 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 136 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 134 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 136 / 399, Shape: [32, 768], Time: 280ms
Rank 2: Batch 135 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 136 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 136 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 137 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 131 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.97 GB | Rank 4: 3.05 GB | Rank 5: 3.06 GB | Rank 6: 3.02 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 137 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 135 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 137 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 137 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 136 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 137 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 138 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 132 / 399, Shape: [32, 768], Time: 279ms
Rank 6: Batch 138 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 136 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 138 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 138 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 137 / 399, Shape: [32, 768], Time: 294ms
Rank 3: Batch 138 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 139 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 133 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 139 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 139 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 137 / 399, Shape: [32, 768], Time: 291ms
Rank 1: Batch 139 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 138 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 139 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 140 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 134 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 140 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 140 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 138 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 140 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 139 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 140 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 141 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 135 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 141 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 141 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 139 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 141 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 140 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 141 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 142 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 136 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.97 GB | Rank 4: 3.05 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 142 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 142 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 140 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 142 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 142 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 141 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 143 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 137 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 143 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 143 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 141 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 143 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 143 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 142 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 144 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 138 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 144 / 399, Shape: [32, 768], Time: 282ms
Rank 5: Batch 144 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 144 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 142 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 144 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 143 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 145 / 399, Shape: [32, 768], Time: 281ms
Rank 0: Batch 139 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 145 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 145 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 145 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 143 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 145 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 144 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 146 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 140 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 146 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 146 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 146 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 146 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 144 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 145 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 147 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 141 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.97 GB | Rank 4: 3.05 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 147 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 147 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 147 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 147 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 145 / 399, Shape: [32, 768], Time: 288ms
Rank 2: Batch 146 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 148 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 142 / 399, Shape: [32, 768], Time: 279ms
Rank 6: Batch 148 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 148 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 148 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 148 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 146 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 147 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 149 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 143 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 149 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 149 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 149 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 149 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 147 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 148 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 150 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 144 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 150 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 150 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 150 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 150 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 148 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 149 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 151 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 145 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 151 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 151 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 151 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 151 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 150 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 149 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 152 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 146 / 399, Shape: [32, 768], Time: 281ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.97 GB | Rank 4: 3.05 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 152 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 152 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 152 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 152 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 151 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 150 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 153 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 147 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 153 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 153 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 153 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 153 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 152 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 151 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 154 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 148 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 154 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 154 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 154 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 154 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 153 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 152 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 155 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 149 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 155 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 155 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 155 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 155 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 154 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 153 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 156 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 150 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 156 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 156 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 156 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 156 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 155 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 154 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 157 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 151 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.98 GB | Rank 4: 3.05 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 157 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 157 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 157 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 157 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 156 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 155 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 158 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 152 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 158 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 158 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 158 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 158 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 157 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 159 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 156 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 153 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 159 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 159 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 159 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 159 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 158 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 160 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 157 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 154 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 160 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 160 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 160 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 160 / 399, Shape: [32, 768], Time: 288ms
Rank 2: Batch 159 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 161 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 158 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 155 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 161 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 161 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 161 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 161 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 160 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 162 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 159 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 156 / 399, Shape: [32, 768], Time: 291ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.98 GB | Rank 4: 3.05 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 162 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 162 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 162 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 162 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 163 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 161 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 160 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 157 / 399, Shape: [32, 768], Time: 278ms
Rank 6: Batch 163 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 163 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 163 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 163 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 164 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 162 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 161 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 158 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 164 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 164 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 164 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 164 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 165 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 163 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 159 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 162 / 399, Shape: [32, 768], Time: 290ms
Rank 6: Batch 165 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 165 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 165 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 165 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 166 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 164 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 160 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 163 / 399, Shape: [32, 768], Time: 290ms
Rank 6: Batch 166 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 166 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 166 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 166 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 167 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 165 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 161 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.98 GB | Rank 4: 3.05 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 164 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 167 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 167 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 167 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 167 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 168 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 166 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 162 / 399, Shape: [32, 768], Time: 277ms
Rank 7: Batch 165 / 399, Shape: [32, 768], Time: 291ms
Rank 6: Batch 168 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 168 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 168 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 168 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 169 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 167 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 163 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 166 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 169 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 169 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 169 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 169 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 170 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 168 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 164 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 167 / 399, Shape: [32, 768], Time: 290ms
Rank 6: Batch 170 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 170 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 170 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 170 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 171 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 169 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 165 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 168 / 399, Shape: [32, 768], Time: 290ms
Rank 6: Batch 171 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 171 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 171 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 171 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 172 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 166 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 170 / 399, Shape: [32, 768], Time: 287ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.98 GB | Rank 4: 3.05 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 172 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 169 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 172 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 172 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 172 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 173 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 167 / 399, Shape: [32, 768], Time: 271ms
Rank 2: Batch 171 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 173 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 170 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 173 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 173 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 173 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 174 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 168 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 172 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 174 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 171 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 174 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 174 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 175 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 174 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 169 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 173 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 175 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 172 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 175 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 175 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 176 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 175 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 170 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 174 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 176 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 173 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 176 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 176 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 177 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 176 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 171 / 399, Shape: [32, 768], Time: 284ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.98 GB | Rank 4: 3.05 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 175 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 177 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 174 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 177 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 177 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 178 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 177 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 172 / 399, Shape: [32, 768], Time: 281ms
Rank 2: Batch 176 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 178 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 175 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 178 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 178 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 179 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 178 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 173 / 399, Shape: [32, 768], Time: 294ms
Rank 2: Batch 177 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 179 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 176 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 179 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 179 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 180 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 179 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 174 / 399, Shape: [32, 768], Time: 276ms
Rank 2: Batch 178 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 180 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 177 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 180 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 180 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 181 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 180 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 175 / 399, Shape: [32, 768], Time: 278ms
Rank 2: Batch 179 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 181 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 178 / 399, Shape: [32, 768], Time: 291ms
Rank 3: Batch 181 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 181 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 182 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 181 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 176 / 399, Shape: [32, 768], Time: 287ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.98 GB | Rank 4: 3.05 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 180 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 182 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 179 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 182 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 182 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 183 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 182 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 177 / 399, Shape: [32, 768], Time: 279ms
Rank 2: Batch 181 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 183 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 180 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 183 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 183 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 184 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 183 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 178 / 399, Shape: [32, 768], Time: 282ms
Rank 2: Batch 182 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 184 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 181 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 184 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 184 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 185 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 184 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 179 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 183 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 185 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 182 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 185 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 185 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 186 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 185 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 180 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 184 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 186 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 183 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 186 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 187 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 186 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 186 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 181 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.98 GB | Rank 4: 3.05 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 185 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 187 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 184 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 187 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 188 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 187 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 187 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 182 / 399, Shape: [32, 768], Time: 279ms
Rank 2: Batch 186 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 188 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 185 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 188 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 189 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 188 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 188 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 183 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 189 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 187 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 186 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 189 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 190 / 399, Shape: [32, 768], Time: 282ms
Rank 5: Batch 189 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 189 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 184 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 190 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 188 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 187 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 190 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 191 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 190 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 190 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 185 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 191 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 189 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 188 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 191 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 192 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 191 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 186 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.07 GB | Rank 2: 2.97 GB | Rank 3: 2.98 GB | Rank 4: 3.05 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 191 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 192 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 190 / 399, Shape: [32, 768], Time: 289ms
Rank 7: Batch 189 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 192 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 193 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 192 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 187 / 399, Shape: [32, 768], Time: 280ms
Rank 1: Batch 192 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 193 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 191 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 190 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 193 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 194 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 193 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 188 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 193 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 194 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 192 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 191 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 194 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 195 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 194 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 189 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 194 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 195 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 193 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 192 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 195 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 196 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 195 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 190 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 195 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 196 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 194 / 399, Shape: [32, 768], Time: 289ms
Rank 7: Batch 193 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 196 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 197 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 196 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 191 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.72 GB | Rank 1: 3.08 GB | Rank 2: 2.97 GB | Rank 3: 2.98 GB | Rank 4: 3.06 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 2.99 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 196 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 197 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 195 / 399, Shape: [32, 768], Time: 289ms
Rank 7: Batch 194 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 197 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 198 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 192 / 399, Shape: [32, 768], Time: 277ms
Rank 5: Batch 197 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 197 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 198 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 196 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 195 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 198 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 199 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 193 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 198 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 198 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 199 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 197 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 196 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 199 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 200 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 194 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 199 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 199 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 200 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 198 / 399, Shape: [32, 768], Time: 289ms
Rank 7: Batch 197 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 200 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 201 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 200 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 195 / 399, Shape: [32, 768], Time: 300ms
Rank 1: Batch 200 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 201 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 199 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 198 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 201 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 202 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 196 / 399, Shape: [32, 768], Time: 271ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.98 GB | Rank 4: 3.06 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 5: Batch 201 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 201 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 202 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 200 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 199 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 202 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 203 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 197 / 399, Shape: [32, 768], Time: 276ms
Rank 5: Batch 202 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 202 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 203 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 201 / 399, Shape: [32, 768], Time: 289ms
Rank 7: Batch 200 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 203 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 204 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 198 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 203 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 203 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 204 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 202 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 204 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 201 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 205 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 199 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 204 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 204 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 205 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 203 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 205 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 206 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 202 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 200 / 399, Shape: [32, 768], Time: 282ms
Rank 5: Batch 205 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 205 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 206 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 204 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 206 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 207 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 203 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 201 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.98 GB | Rank 4: 3.06 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 5: Batch 206 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 206 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 207 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 205 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 208 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 207 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 204 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 202 / 399, Shape: [32, 768], Time: 279ms
Rank 5: Batch 207 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 207 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 208 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 206 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 209 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 208 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 205 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 203 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 208 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 208 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 209 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 207 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 210 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 209 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 206 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 204 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 209 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 209 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 210 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 208 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 211 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 210 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 207 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 205 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 210 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 210 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 211 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 209 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 212 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 211 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 208 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 206 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.98 GB | Rank 4: 3.06 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 5: Batch 211 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 211 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 212 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 210 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 213 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 212 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 207 / 399, Shape: [32, 768], Time: 279ms
Rank 7: Batch 209 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 212 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 212 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 213 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 211 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 214 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 213 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 208 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 210 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 213 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 213 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 214 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 212 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 215 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 214 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 209 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 211 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 214 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 214 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 215 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 213 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 216 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 215 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 210 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 212 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 215 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 215 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 216 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 214 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 217 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 216 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 211 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.98 GB | Rank 4: 3.06 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 213 / 399, Shape: [32, 768], Time: 291ms
Rank 5: Batch 216 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 216 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 217 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 215 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 218 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 217 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 212 / 399, Shape: [32, 768], Time: 279ms
Rank 7: Batch 214 / 399, Shape: [32, 768], Time: 294ms
Rank 5: Batch 217 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 217 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 218 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 216 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 219 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 218 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 213 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 215 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 218 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 218 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 219 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 217 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 220 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 219 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 214 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 216 / 399, Shape: [32, 768], Time: 288ms
Rank 5: Batch 219 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 219 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 220 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 218 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 221 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 220 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 215 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 217 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 220 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 220 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 221 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 219 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 222 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 221 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 216 / 399, Shape: [32, 768], Time: 281ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.98 GB | Rank 4: 3.06 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 218 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 221 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 221 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 222 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 220 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 223 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 222 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 217 / 399, Shape: [32, 768], Time: 277ms
Rank 7: Batch 219 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 222 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 222 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 223 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 221 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 224 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 223 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 218 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 220 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 223 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 223 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 224 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 222 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 225 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 224 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 219 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 221 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 224 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 224 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 225 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 223 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 226 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 225 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 220 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 222 / 399, Shape: [32, 768], Time: 288ms
Rank 5: Batch 225 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 225 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 226 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 224 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 227 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 226 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 221 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.98 GB | Rank 4: 3.06 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 223 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 226 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 226 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 227 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 228 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 225 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 227 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 222 / 399, Shape: [32, 768], Time: 279ms
Rank 7: Batch 224 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 227 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 227 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 228 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 229 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 226 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 228 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 223 / 399, Shape: [32, 768], Time: 282ms
Rank 5: Batch 228 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 225 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 228 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 229 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 230 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 227 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 229 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 224 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 229 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 226 / 399, Shape: [32, 768], Time: 291ms
Rank 1: Batch 229 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 230 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 231 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 228 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 230 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 225 / 399, Shape: [32, 768], Time: 282ms
Rank 5: Batch 230 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 227 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 230 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 231 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 232 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 229 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 231 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 226 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.98 GB | Rank 4: 3.06 GB | Rank 5: 3.07 GB | Rank 6: 3.02 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 5: Batch 231 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 228 / 399, Shape: [32, 768], Time: 291ms
Rank 1: Batch 231 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 232 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 233 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 230 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 232 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 227 / 399, Shape: [32, 768], Time: 279ms
Rank 5: Batch 232 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 232 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 229 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 233 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 234 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 231 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 233 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 228 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 233 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 233 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 234 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 230 / 399, Shape: [32, 768], Time: 291ms
Rank 4: Batch 235 / 399, Shape: [32, 768], Time: 281ms
Rank 2: Batch 232 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 234 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 229 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 234 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 235 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 234 / 399, Shape: [32, 768], Time: 289ms
Rank 7: Batch 231 / 399, Shape: [32, 768], Time: 291ms
Rank 4: Batch 236 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 233 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 235 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 230 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 236 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 235 / 399, Shape: [32, 768], Time: 291ms
Rank 1: Batch 235 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 232 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 237 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 234 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 231 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 236 / 399, Shape: [32, 768], Time: 285ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.98 GB | Rank 4: 3.06 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 5: Batch 236 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 237 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 236 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 233 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 238 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 232 / 399, Shape: [32, 768], Time: 278ms
Rank 2: Batch 235 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 237 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 238 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 237 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 237 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 234 / 399, Shape: [32, 768], Time: 298ms
Rank 4: Batch 239 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 233 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 238 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 236 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 239 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 238 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 238 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 235 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 240 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 234 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 239 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 237 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 240 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 239 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 239 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 236 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 241 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 235 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 240 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 238 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 241 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 240 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 240 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 237 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 242 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 236 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.98 GB | Rank 4: 3.06 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch 241 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 239 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 242 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 241 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 241 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 238 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 243 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 237 / 399, Shape: [32, 768], Time: 275ms
Rank 3: Batch 242 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 240 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 243 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 242 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 242 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 239 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 244 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 238 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 243 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 241 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 244 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 243 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 243 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 240 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 245 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 239 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 244 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 242 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 245 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 244 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 244 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 241 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 246 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 240 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 245 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 243 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 246 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 245 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 245 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 242 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 247 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 241 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.98 GB | Rank 4: 3.06 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch 246 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 244 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 247 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 246 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 246 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 243 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 248 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 242 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 247 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 245 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 248 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 247 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 247 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 244 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 249 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 243 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 248 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 246 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 249 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 248 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 248 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 245 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 250 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 244 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 249 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 247 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 250 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 249 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 249 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 251 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 246 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 245 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 250 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 248 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 251 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 250 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 250 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 252 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 247 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 246 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.98 GB | Rank 4: 3.06 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch 251 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 249 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 252 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 251 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 251 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 253 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 248 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 247 / 399, Shape: [32, 768], Time: 279ms
Rank 3: Batch 252 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 250 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 253 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 252 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 252 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 254 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 249 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 248 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 253 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 251 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 254 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 253 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 253 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 255 / 399, Shape: [32, 768], Time: 280ms
Rank 7: Batch 250 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 249 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 254 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 252 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 255 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 254 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 254 / 399, Shape: [32, 768], Time: 291ms
Rank 4: Batch 256 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 251 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 250 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 255 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 253 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 256 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 255 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 255 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 257 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 251 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.99 GB | Rank 4: 3.06 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 252 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 256 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 254 / 399, Shape: [32, 768], Time: 294ms
Rank 6: Batch 257 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 256 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 256 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 258 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 252 / 399, Shape: [32, 768], Time: 279ms
Rank 7: Batch 253 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 257 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 255 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 258 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 257 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 257 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 259 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 253 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 254 / 399, Shape: [32, 768], Time: 304ms
Rank 3: Batch 258 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 256 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 259 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 258 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 258 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 260 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 254 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 259 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 255 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 257 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 260 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 259 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 259 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 261 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 255 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 260 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 256 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 258 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 261 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 260 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 260 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 262 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 256 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.99 GB | Rank 4: 3.06 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch 261 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 257 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 259 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 262 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 261 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 261 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 263 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 257 / 399, Shape: [32, 768], Time: 276ms
Rank 3: Batch 262 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 258 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 260 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 263 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 262 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 262 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 264 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 258 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 263 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 259 / 399, Shape: [32, 768], Time: 291ms
Rank 2: Batch 261 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 264 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 263 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 263 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 265 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 259 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 264 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 260 / 399, Shape: [32, 768], Time: 288ms
Rank 2: Batch 262 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 265 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 264 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 264 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 266 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 260 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 265 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 261 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 263 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 266 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 265 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 267 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 265 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 261 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.99 GB | Rank 4: 3.06 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch 266 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 262 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 264 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 267 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 268 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 266 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 266 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 262 / 399, Shape: [32, 768], Time: 279ms
Rank 3: Batch 267 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 263 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 265 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 268 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 269 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 267 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 267 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 263 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 268 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 264 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 266 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 269 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 270 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 268 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 268 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 264 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 269 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 265 / 399, Shape: [32, 768], Time: 291ms
Rank 2: Batch 267 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 270 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 271 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 269 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 269 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 265 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 270 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 266 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 268 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 271 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 272 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 270 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 270 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 266 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.99 GB | Rank 4: 3.06 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch 271 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 267 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 269 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 272 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 273 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 271 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 271 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 267 / 399, Shape: [32, 768], Time: 278ms
Rank 3: Batch 272 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 268 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 270 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 273 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 274 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 272 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 272 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 268 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 273 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 269 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 271 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 274 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 275 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 273 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 273 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 269 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 274 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 270 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 272 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 275 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 276 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 274 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 274 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 270 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 275 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 271 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 273 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 276 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 277 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 275 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 275 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 271 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.99 GB | Rank 4: 3.06 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch 276 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 272 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 274 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 277 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 278 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 276 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 272 / 399, Shape: [32, 768], Time: 278ms
Rank 5: Batch 276 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 277 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 273 / 399, Shape: [32, 768], Time: 296ms
Rank 2: Batch 275 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 278 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 279 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 277 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 273 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 277 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 278 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 274 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 276 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 279 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 280 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 274 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 278 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 278 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 279 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 275 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 280 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 277 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 281 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 275 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 279 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 279 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 280 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 276 / 399, Shape: [32, 768], Time: 290ms
Rank 6: Batch 281 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 278 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 282 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 276 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.99 GB | Rank 4: 3.06 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 280 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 280 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 281 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 277 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 282 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 279 / 399, Shape: [32, 768], Time: 291ms
Rank 4: Batch 283 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 277 / 399, Shape: [32, 768], Time: 280ms
Rank 1: Batch 281 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 281 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 282 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 278 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 283 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 280 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 284 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 278 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 282 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 282 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 283 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 279 / 399, Shape: [32, 768], Time: 291ms
Rank 6: Batch 284 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 281 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 285 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 279 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 283 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 283 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 284 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 280 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 285 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 282 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 286 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 280 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 284 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 284 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 285 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 281 / 399, Shape: [32, 768], Time: 290ms
Rank 6: Batch 286 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 283 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 287 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 281 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.99 GB | Rank 4: 3.06 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 285 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 285 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 286 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 282 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 287 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 284 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 288 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 282 / 399, Shape: [32, 768], Time: 278ms
Rank 1: Batch 286 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 286 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 287 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 283 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 288 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 289 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 285 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 283 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 287 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 287 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 288 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 289 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 284 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 290 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 286 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 284 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 288 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 288 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 289 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 290 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 285 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 291 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 287 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 285 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 289 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 289 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 290 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 291 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 286 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 292 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 288 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 286 / 399, Shape: [32, 768], Time: 284ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.08 GB | Rank 2: 2.98 GB | Rank 3: 2.99 GB | Rank 4: 3.06 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 290 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 290 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 291 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 292 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 287 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 293 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 289 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 287 / 399, Shape: [32, 768], Time: 274ms
Rank 1: Batch 291 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 291 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 292 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 293 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 294 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 288 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 290 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 288 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 292 / 399, Shape: [32, 768], Time: 288ms
Rank 5: Batch 292 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 293 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 294 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 295 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 289 / 399, Shape: [32, 768], Time: 288ms
Rank 2: Batch 291 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 289 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 293 / 399, Shape: [32, 768], Time: 298ms
Rank 5: Batch 293 / 399, Shape: [32, 768], Time: 297ms
Rank 3: Batch 294 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 295 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 296 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 290 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 292 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 290 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 294 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 294 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 295 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 296 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 297 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 291 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 293 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 291 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.73 GB | Rank 1: 3.09 GB | Rank 2: 2.99 GB | Rank 3: 2.99 GB | Rank 4: 3.07 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.00 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 295 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 295 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 296 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 297 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 298 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 292 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 294 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 292 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 296 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 296 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 297 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 298 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 299 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 293 / 399, Shape: [32, 768], Time: 292ms
Rank 0: Batch 293 / 399, Shape: [32, 768], Time: 279ms
Rank 2: Batch 295 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 297 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 297 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 298 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 299 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 300 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 294 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 294 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 296 / 399, Shape: [32, 768], Time: 290ms
Rank 1: Batch 298 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 299 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 298 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 300 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 301 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch 295 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 295 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 297 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 299 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 300 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 299 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 301 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 302 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 296 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 296 / 399, Shape: [32, 768], Time: 289ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.09 GB | Rank 2: 2.99 GB | Rank 3: 2.99 GB | Rank 4: 3.07 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.01 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 298 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 300 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 301 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 300 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 302 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 303 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 297 / 399, Shape: [32, 768], Time: 279ms
Rank 7: Batch 297 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 299 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 301 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 302 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 301 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 303 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 304 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 298 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch 298 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 300 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 302 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 303 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 302 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 304 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 305 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 299 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch 299 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 301 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 304 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 303 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 303 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 305 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 306 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 300 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 300 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 302 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 305 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 304 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 304 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 306 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 307 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 301 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.09 GB | Rank 2: 2.99 GB | Rank 3: 2.99 GB | Rank 4: 3.07 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.01 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 301 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 303 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 306 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 305 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 305 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 307 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 308 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 302 / 399, Shape: [32, 768], Time: 280ms
Rank 7: Batch 302 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 304 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 307 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 306 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 306 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 308 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 309 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 303 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 303 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 305 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 308 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 307 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 307 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 309 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 310 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 304 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 304 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 306 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 309 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 308 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 308 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 311 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 310 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 305 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 305 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 307 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 310 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 309 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 309 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 311 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 312 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 306 / 399, Shape: [32, 768], Time: 284ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.09 GB | Rank 2: 2.99 GB | Rank 3: 2.99 GB | Rank 4: 3.07 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.01 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 306 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 308 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 311 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 310 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 310 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 313 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 312 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 307 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 307 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 309 / 399, Shape: [32, 768], Time: 296ms
Rank 3: Batch 312 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 311 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 311 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 314 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 313 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 308 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 308 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 310 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 313 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 312 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 312 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 315 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 314 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 309 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 309 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 311 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 314 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 313 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 313 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 316 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 315 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 310 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 310 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 312 / 399, Shape: [32, 768], Time: 292ms
Rank 3: Batch 315 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 314 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 314 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 317 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 316 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 311 / 399, Shape: [32, 768], Time: 284ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.09 GB | Rank 2: 2.99 GB | Rank 3: 2.99 GB | Rank 4: 3.07 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.01 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 311 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 313 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 316 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 315 / 399, Shape: [32, 768], Time: 288ms
Rank 5: Batch 315 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 318 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 317 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 312 / 399, Shape: [32, 768], Time: 281ms
Rank 7: Batch 312 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 317 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 314 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 316 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 316 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 319 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 318 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 313 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 313 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 318 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 315 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 317 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 317 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 320 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 319 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 314 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 314 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 319 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 316 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 318 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 318 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 321 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 320 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 315 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 320 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 315 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 317 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 319 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 319 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 322 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 321 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 316 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.09 GB | Rank 2: 2.99 GB | Rank 3: 2.99 GB | Rank 4: 3.07 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.01 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch 321 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 316 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 318 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 320 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 320 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 323 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 322 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 317 / 399, Shape: [32, 768], Time: 279ms
Rank 3: Batch 322 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 317 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 319 / 399, Shape: [32, 768], Time: 290ms
Rank 1: Batch 321 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 321 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 324 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 323 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 318 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 323 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 318 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 320 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 322 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 322 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 325 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 324 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 319 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 324 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 319 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 321 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 323 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 323 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 326 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 325 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 320 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 325 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 320 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 322 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 324 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 324 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 327 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 326 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 321 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.09 GB | Rank 2: 2.99 GB | Rank 3: 2.99 GB | Rank 4: 3.07 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.01 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch 326 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 321 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 323 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 325 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 325 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 328 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 327 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 322 / 399, Shape: [32, 768], Time: 274ms
Rank 3: Batch 327 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 322 / 399, Shape: [32, 768], Time: 288ms
Rank 2: Batch 324 / 399, Shape: [32, 768], Time: 294ms
Rank 1: Batch 326 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 326 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 329 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 328 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 323 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 328 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 325 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 323 / 399, Shape: [32, 768], Time: 290ms
Rank 1: Batch 327 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 327 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 330 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 329 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 324 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 329 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 326 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 324 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 328 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 328 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 331 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 330 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 325 / 399, Shape: [32, 768], Time: 283ms
Rank 3: Batch 330 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 327 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 325 / 399, Shape: [32, 768], Time: 290ms
Rank 1: Batch 329 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 329 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 332 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 331 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 326 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.09 GB | Rank 2: 2.99 GB | Rank 3: 2.99 GB | Rank 4: 3.07 GB | Rank 5: 3.08 GB | Rank 6: 3.03 GB | Rank 7: 3.01 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch 331 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 328 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 326 / 399, Shape: [32, 768], Time: 290ms
Rank 1: Batch 330 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 330 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 333 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 332 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 327 / 399, Shape: [32, 768], Time: 281ms
Rank 3: Batch 332 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 329 / 399, Shape: [32, 768], Time: 289ms
Rank 7: Batch 327 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 331 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 331 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 334 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 333 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 328 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 333 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 330 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 328 / 399, Shape: [32, 768], Time: 290ms
Rank 1: Batch 332 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 332 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 335 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 334 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 329 / 399, Shape: [32, 768], Time: 282ms
Rank 3: Batch 334 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 331 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 333 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 329 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 333 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 336 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 330 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 335 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 335 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 332 / 399, Shape: [32, 768], Time: 290ms
Rank 1: Batch 334 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 330 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 334 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 337 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 331 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.09 GB | Rank 2: 2.99 GB | Rank 3: 2.99 GB | Rank 4: 3.07 GB | Rank 5: 3.09 GB | Rank 6: 3.04 GB | Rank 7: 3.01 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 336 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 336 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 333 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 335 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 331 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 335 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 338 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 332 / 399, Shape: [32, 768], Time: 281ms
Rank 6: Batch 337 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 337 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 334 / 399, Shape: [32, 768], Time: 288ms
Rank 1: Batch 336 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 332 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 339 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 336 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 333 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 338 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 338 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 337 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 335 / 399, Shape: [32, 768], Time: 293ms
Rank 7: Batch 333 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 340 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 337 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 334 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 339 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 339 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 336 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 338 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 334 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 341 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 338 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 335 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 340 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 340 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 339 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 337 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 342 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 335 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 339 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 336 / 399, Shape: [32, 768], Time: 284ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.09 GB | Rank 2: 2.99 GB | Rank 3: 2.99 GB | Rank 4: 3.07 GB | Rank 5: 3.09 GB | Rank 6: 3.04 GB | Rank 7: 3.01 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 341 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 341 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 340 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 338 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 343 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 336 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 340 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 337 / 399, Shape: [32, 768], Time: 279ms
Rank 6: Batch 342 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 342 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 341 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 339 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 344 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 337 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 341 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 338 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 343 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 343 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 342 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 345 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 340 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 338 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 342 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 339 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 344 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 344 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 343 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 346 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 341 / 399, Shape: [32, 768], Time: 289ms
Rank 7: Batch 339 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 343 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 340 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 345 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 345 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 347 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 344 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 342 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 340 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 344 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 341 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.09 GB | Rank 2: 2.99 GB | Rank 3: 2.99 GB | Rank 4: 3.07 GB | Rank 5: 3.09 GB | Rank 6: 3.04 GB | Rank 7: 3.01 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 346 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 346 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 348 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 345 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 343 / 399, Shape: [32, 768], Time: 288ms
Rank 5: Batch 345 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 341 / 399, Shape: [32, 768], Time: 291ms
Rank 0: Batch 342 / 399, Shape: [32, 768], Time: 279ms
Rank 6: Batch 347 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 347 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 349 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 346 / 399, Shape: [32, 768], Time: 285ms
Rank 2: Batch 344 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 346 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 343 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 342 / 399, Shape: [32, 768], Time: 293ms
Rank 6: Batch 348 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 348 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 350 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 347 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 345 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 344 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 347 / 399, Shape: [32, 768], Time: 289ms
Rank 7: Batch 343 / 399, Shape: [32, 768], Time: 285ms
Rank 6: Batch 349 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 349 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 351 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 348 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 346 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 345 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 348 / 399, Shape: [32, 768], Time: 289ms
Rank 7: Batch 344 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 350 / 399, Shape: [32, 768], Time: 285ms
Rank 3: Batch 350 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 352 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 349 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 347 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 346 / 399, Shape: [32, 768], Time: 283ms
Rank 5: Batch 349 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 345 / 399, Shape: [32, 768], Time: 290ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.09 GB | Rank 2: 2.99 GB | Rank 3: 2.99 GB | Rank 4: 3.07 GB | Rank 5: 3.09 GB | Rank 6: 3.04 GB | Rank 7: 3.01 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 351 / 399, Shape: [32, 768], Time: 294ms
Rank 3: Batch 351 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 353 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 350 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 348 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 347 / 399, Shape: [32, 768], Time: 250ms
Rank 5: Batch 350 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 346 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 352 / 399, Shape: [32, 768], Time: 284ms
Rank 3: Batch 352 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 354 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 351 / 399, Shape: [32, 768], Time: 291ms
Rank 0: Batch 348 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 349 / 399, Shape: [32, 768], Time: 288ms
Rank 5: Batch 351 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 347 / 399, Shape: [32, 768], Time: 290ms
Rank 6: Batch 353 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 353 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 355 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 352 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 349 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 350 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 352 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 348 / 399, Shape: [32, 768], Time: 290ms
Rank 6: Batch 354 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 354 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 356 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 353 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 350 / 399, Shape: [32, 768], Time: 283ms
Rank 2: Batch 351 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 353 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 349 / 399, Shape: [32, 768], Time: 290ms
Rank 6: Batch 355 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 355 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 357 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 354 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 351 / 399, Shape: [32, 768], Time: 291ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.09 GB | Rank 2: 2.99 GB | Rank 3: 3.00 GB | Rank 4: 3.07 GB | Rank 5: 3.09 GB | Rank 6: 3.04 GB | Rank 7: 3.01 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 352 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 354 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 356 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 350 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 356 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 358 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 355 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 352 / 399, Shape: [32, 768], Time: 278ms
Rank 2: Batch 353 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 355 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 357 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 351 / 399, Shape: [32, 768], Time: 304ms
Rank 3: Batch 357 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 359 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 356 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 353 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 354 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 356 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 358 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 352 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 358 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 360 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 357 / 399, Shape: [32, 768], Time: 287ms
Rank 0: Batch 354 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 355 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 357 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 359 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 353 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 359 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 361 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 358 / 399, Shape: [32, 768], Time: 286ms
Rank 0: Batch 355 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 356 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 358 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 360 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 354 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 360 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 362 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 356 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 359 / 399, Shape: [32, 768], Time: 287ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.09 GB | Rank 2: 2.99 GB | Rank 3: 3.00 GB | Rank 4: 3.07 GB | Rank 5: 3.09 GB | Rank 6: 3.04 GB | Rank 7: 3.01 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 357 / 399, Shape: [32, 768], Time: 290ms
Rank 5: Batch 359 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 361 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 355 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 361 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 363 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 357 / 399, Shape: [32, 768], Time: 279ms
Rank 1: Batch 360 / 399, Shape: [32, 768], Time: 288ms
Rank 2: Batch 358 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 360 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 362 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 356 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 362 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 364 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 358 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 361 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 359 / 399, Shape: [32, 768], Time: 288ms
Rank 5: Batch 361 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 363 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 357 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 363 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 365 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 359 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 362 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 362 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 360 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 364 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 358 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 364 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 366 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 360 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 363 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 365 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 363 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 361 / 399, Shape: [32, 768], Time: 289ms
Rank 7: Batch 359 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 365 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 367 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 361 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.09 GB | Rank 2: 2.99 GB | Rank 3: 3.00 GB | Rank 4: 3.07 GB | Rank 5: 3.09 GB | Rank 6: 3.04 GB | Rank 7: 3.01 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 364 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 366 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 364 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 362 / 399, Shape: [32, 768], Time: 289ms
Rank 7: Batch 360 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 366 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 368 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 362 / 399, Shape: [32, 768], Time: 277ms
Rank 1: Batch 365 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 367 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 365 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 363 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 361 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 367 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 369 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 363 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 366 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 368 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 366 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 364 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 362 / 399, Shape: [32, 768], Time: 291ms
Rank 3: Batch 368 / 399, Shape: [32, 768], Time: 286ms
Rank 4: Batch 370 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 364 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 367 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 369 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 367 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 365 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 363 / 399, Shape: [32, 768], Time: 291ms
Rank 3: Batch 369 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 371 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 365 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 368 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 370 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 368 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 366 / 399, Shape: [32, 768], Time: 289ms
Rank 7: Batch 364 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 370 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 372 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 366 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.09 GB | Rank 2: 2.99 GB | Rank 3: 3.00 GB | Rank 4: 3.07 GB | Rank 5: 3.09 GB | Rank 6: 3.04 GB | Rank 7: 3.01 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 369 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 371 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 369 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 367 / 399, Shape: [32, 768], Time: 288ms
Rank 7: Batch 365 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 371 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 373 / 399, Shape: [32, 768], Time: 282ms
Rank 0: Batch 367 / 399, Shape: [32, 768], Time: 277ms
Rank 1: Batch 370 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 372 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 370 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 368 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 366 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 372 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 374 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 368 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 371 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 373 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 371 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 369 / 399, Shape: [32, 768], Time: 289ms
Rank 7: Batch 367 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 373 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 375 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 369 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 372 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 374 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 372 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 370 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 374 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 368 / 399, Shape: [32, 768], Time: 293ms
Rank 4: Batch 376 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 370 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 373 / 399, Shape: [32, 768], Time: 289ms
Rank 6: Batch 375 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 373 / 399, Shape: [32, 768], Time: 288ms
Rank 2: Batch 371 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 375 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 369 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 377 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 371 / 399, Shape: [32, 768], Time: 290ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.10 GB | Rank 2: 2.99 GB | Rank 3: 3.00 GB | Rank 4: 3.07 GB | Rank 5: 3.09 GB | Rank 6: 3.04 GB | Rank 7: 3.01 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 374 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 376 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 374 / 399, Shape: [32, 768], Time: 286ms
Rank 2: Batch 372 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 376 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 370 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 378 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 372 / 399, Shape: [32, 768], Time: 278ms
Rank 1: Batch 375 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 377 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 375 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 373 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 377 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 371 / 399, Shape: [32, 768], Time: 295ms
Rank 4: Batch 379 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 373 / 399, Shape: [32, 768], Time: 284ms
Rank 1: Batch 376 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 378 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 376 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 374 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 378 / 399, Shape: [32, 768], Time: 286ms
Rank 7: Batch 372 / 399, Shape: [32, 768], Time: 288ms
Rank 4: Batch 380 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 374 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 377 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 379 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 377 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 375 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 379 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 373 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 381 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 375 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 378 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 380 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 378 / 399, Shape: [32, 768], Time: 288ms
Rank 2: Batch 376 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 380 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 374 / 399, Shape: [32, 768], Time: 291ms
Rank 4: Batch 382 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 376 / 399, Shape: [32, 768], Time: 284ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.10 GB | Rank 2: 2.99 GB | Rank 3: 3.00 GB | Rank 4: 3.07 GB | Rank 5: 3.09 GB | Rank 6: 3.04 GB | Rank 7: 3.02 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 1: Batch 379 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 381 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 379 / 399, Shape: [32, 768], Time: 288ms
Rank 2: Batch 377 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 381 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 375 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 383 / 399, Shape: [32, 768], Time: 285ms
Rank 0: Batch 377 / 399, Shape: [32, 768], Time: 278ms
Rank 1: Batch 380 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 382 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 380 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 378 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 382 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 376 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 384 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 378 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 381 / 399, Shape: [32, 768], Time: 287ms
Rank 6: Batch 383 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 381 / 399, Shape: [32, 768], Time: 288ms
Rank 2: Batch 379 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 383 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 377 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 385 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 379 / 399, Shape: [32, 768], Time: 283ms
Rank 1: Batch 382 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 384 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 382 / 399, Shape: [32, 768], Time: 288ms
Rank 2: Batch 380 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 384 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 386 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 378 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 380 / 399, Shape: [32, 768], Time: 282ms
Rank 1: Batch 383 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 385 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 383 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 381 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 385 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 387 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 379 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 381 / 399, Shape: [32, 768], Time: 284ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.10 GB | Rank 2: 2.99 GB | Rank 3: 3.00 GB | Rank 4: 3.07 GB | Rank 5: 3.09 GB | Rank 6: 3.04 GB | Rank 7: 3.02 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 386 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 384 / 399, Shape: [32, 768], Time: 286ms
Rank 5: Batch 384 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 382 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 386 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 388 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 380 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 382 / 399, Shape: [32, 768], Time: 278ms
Rank 6: Batch 387 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 385 / 399, Shape: [32, 768], Time: 289ms
Rank 5: Batch 385 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 383 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 387 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 389 / 399, Shape: [32, 768], Time: 285ms
Rank 7: Batch 381 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 383 / 399, Shape: [32, 768], Time: 283ms
Rank 6: Batch 388 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 386 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 386 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 384 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 388 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 390 / 399, Shape: [32, 768], Time: 287ms
Rank 7: Batch 382 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 384 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 389 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 387 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 387 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 385 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 389 / 399, Shape: [32, 768], Time: 285ms
Rank 4: Batch 391 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 383 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 385 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 390 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 388 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 388 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 386 / 399, Shape: [32, 768], Time: 290ms
Rank 3: Batch 390 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 392 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 384 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 386 / 399, Shape: [32, 768], Time: 284ms
Memory: Rank 0: 2.74 GB | Rank 1: 3.10 GB | Rank 2: 2.99 GB | Rank 3: 3.00 GB | Rank 4: 3.08 GB | Rank 5: 3.09 GB | Rank 6: 3.04 GB | Rank 7: 3.02 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 391 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 389 / 399, Shape: [32, 768], Time: 288ms
Rank 5: Batch 389 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 387 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 391 / 399, Shape: [32, 768], Time: 283ms
Rank 4: Batch 393 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 385 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 387 / 399, Shape: [32, 768], Time: 280ms
Rank 6: Batch 392 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 390 / 399, Shape: [32, 768], Time: 292ms
Rank 5: Batch 390 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 388 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 392 / 399, Shape: [32, 768], Time: 284ms
Rank 4: Batch 394 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 386 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 388 / 399, Shape: [32, 768], Time: 282ms
Rank 6: Batch 393 / 399, Shape: [32, 768], Time: 286ms
Rank 1: Batch 391 / 399, Shape: [32, 768], Time: 285ms
Rank 5: Batch 391 / 399, Shape: [32, 768], Time: 286ms
Rank 3: Batch 393 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 389 / 399, Shape: [32, 768], Time: 290ms
Rank 4: Batch 395 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 387 / 399, Shape: [32, 768], Time: 290ms
Rank 0: Batch 389 / 399, Shape: [32, 768], Time: 284ms
Rank 6: Batch 394 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 392 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 392 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 394 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 390 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 396 / 399, Shape: [32, 768], Time: 284ms
Rank 0: Batch 390 / 399, Shape: [32, 768], Time: 284ms
Rank 7: Batch 388 / 399, Shape: [32, 768], Time: 290ms
Rank 6: Batch 395 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 393 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 393 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 395 / 399, Shape: [32, 768], Time: 284ms
Rank 2: Batch 391 / 399, Shape: [32, 768], Time: 289ms
Rank 4: Batch 397 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 391 / 399, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 2.75 GB | Rank 1: 3.10 GB | Rank 2: 3.00 GB | Rank 3: 3.00 GB | Rank 4: 3.08 GB | Rank 5: 3.09 GB | Rank 6: 3.04 GB | Rank 7: 3.02 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 389 / 399, Shape: [32, 768], Time: 288ms
Rank 6: Batch 396 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 394 / 399, Shape: [32, 768], Time: 288ms
Rank 5: Batch 394 / 399, Shape: [32, 768], Time: 288ms
Rank 3: Batch 396 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 392 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 398 / 399, Shape: [32, 768], Time: 283ms
Rank 0: Batch 392 / 399, Shape: [32, 768], Time: 280ms
Rank 7: Batch 390 / 399, Shape: [32, 768], Time: 290ms
Rank 6: Batch 397 / 399, Shape: [32, 768], Time: 285ms
Rank 1: Batch 395 / 399, Shape: [32, 768], Time: 288ms
Rank 5: Batch 395 / 399, Shape: [32, 768], Time: 289ms
Rank 3: Batch 397 / 399, Shape: [32, 768], Time: 280ms
Rank 2: Batch 393 / 399, Shape: [32, 768], Time: 287ms
Rank 4: Batch 399 / 399, Shape: [27, 768], Time: 287ms
Rank 0: Batch 393 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 391 / 399, Shape: [32, 768], Time: 295ms
Rank 6: Batch 398 / 399, Shape: [32, 768], Time: 287ms
Rank 1: Batch 396 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 398 / 399, Shape: [32, 768], Time: 284ms
Rank 5: Batch 396 / 399, Shape: [32, 768], Time: 289ms
Rank 2: Batch 394 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 394 / 399, Shape: [32, 768], Time: 283ms
Rank 7: Batch 392 / 399, Shape: [32, 768], Time: 286ms
Rank 6: Batch 399 / 399, Shape: [27, 768], Time: 286ms
Rank 1: Batch 397 / 399, Shape: [32, 768], Time: 287ms
Rank 3: Batch 399 / 399, Shape: [27, 768], Time: 284ms
Rank 5: Batch 397 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 395 / 399, Shape: [32, 768], Time: 288ms
Rank 0: Batch 395 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 393 / 399, Shape: [32, 768], Time: 289ms
Rank 1: Batch 398 / 399, Shape: [32, 768], Time: 287ms
Rank 5: Batch 398 / 399, Shape: [32, 768], Time: 287ms
Rank 2: Batch 396 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 396 / 399, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 2.75 GB | Rank 1: 3.10 GB | Rank 2: 3.00 GB | Rank 3: 3.00 GB | Rank 4: 3.08 GB | Rank 5: 3.09 GB | Rank 6: 3.04 GB | Rank 7: 3.02 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 394 / 399, Shape: [32, 768], Time: 290ms
Rank 1: Batch 399 / 399, Shape: [27, 768], Time: 287ms
Rank 5: Batch 399 / 399, Shape: [27, 768], Time: 289ms
Rank 2: Batch 397 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 397 / 399, Shape: [32, 768], Time: 279ms
Rank 7: Batch 395 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 398 / 399, Shape: [32, 768], Time: 289ms
Rank 0: Batch 398 / 399, Shape: [32, 768], Time: 282ms
Rank 7: Batch 396 / 399, Shape: [32, 768], Time: 290ms
Rank 2: Batch 399 / 399, Shape: [27, 768], Time: 290ms
Rank 0: Batch 399 / 399, Shape: [27, 768], Time: 285ms
Rank 7: Batch 397 / 399, Shape: [32, 768], Time: 289ms
Total embeddings: 102104
Original texts: 102097
Expected padding: 7
Actual padding: 7
Rank 7: Batch 398 / 399, Shape: [32, 768], Time: 292ms
Rank 7: Batch 399 / 399, Shape: [27, 768], Time: 291ms
Maximum difference between padding embeddings: 0.0
Padding embeddings verified as very similar.
Encoding completed (1m 56s)

Displaying samples from Validation data:
Validation[1381]: My primary care doctor left and unfortunately his new office is taking new patients so I stuck with Forte until I find someone new. - NEGATIVE
Tokens: ['my', 'primary', 'care', 'doctor', 'left', 'and', 'unfortunately', 'his', 'new', 'office', 'is', 'taking', 'new', 'patients', 'so', 'i', 'stuck', 'with', 'forte', 'until', 'i', 'find', 'someone', 'new', '.']
Embedding: [-0.03577539  0.1866337  -0.00972883  0.04267377  0.02039148  0.08064178] ...

Validation[2337]: He didn't even back off when we made it clear we were a couple. - NEGATIVE
Tokens: ['he', 'didn', "'", 't', 'even', 'back', 'off', 'when', 'we', 'made', 'it', 'clear', 'we', 'were', 'a', 'couple', '.']
Embedding: [-0.19298227  0.1846708   0.03479815 -0.59248054 -0.04275104  0.28450108] ...

Validation[5213]: The food was fantastic if you love cold greasy food. - NEGATIVE
Tokens: ['the', 'food', 'was', 'fantastic', 'if', 'you', 'love', 'cold', 'greasy', 'food', '.']
Embedding: [ 0.0348301  -0.06014011  0.14139125 -0.18323858 -0.0195738   0.09715316] ...


Encoding Validation data of 5421 texts distributed across 8 GPUs...
Batch Size: 32, Pooling: Mean, Empty Cache: False
Padding Validation data to 5424 texts for even distribution across 8 ranks...
Texts per rank: 678, Total batches: 170
Rank 4: Processing 678 texts (indices 2712 to 3389) in 22 batches...
Rank 3: Processing 678 texts (indices 2034 to 2711) in 22 batches...
Rank 0: Processing 678 texts (indices 0 to 677) in 22 batches...
Rank 6: Processing 678 texts (indices 4068 to 4745) in 22 batches...
Rank 5: Processing 678 texts (indices 3390 to 4067) in 22 batches...
Rank 2: Processing 678 texts (indices 1356 to 2033) in 22 batches...
Rank 7: Processing 678 texts (indices 4746 to 5423) in 22 batches...
Rank 1: Processing 678 texts (indices 678 to 1355) in 22 batches...
Rank 3: Batch  1 / 22, Shape: [32, 768], Time: 19ms
Rank 6: Batch  1 / 22, Shape: [32, 768], Time: 20ms
Rank 4: Batch  1 / 22, Shape: [32, 768], Time: 20ms
Rank 5: Batch  1 / 22, Shape: [32, 768], Time: 21ms
Rank 2: Batch  1 / 22, Shape: [32, 768], Time: 21ms
Rank 1: Batch  1 / 22, Shape: [32, 768], Time: 21ms
Rank 7: Batch  1 / 22, Shape: [32, 768], Time: 22ms
Rank 0: Batch  1 / 22, Shape: [32, 768], Time: 22ms
Memory: Rank 0: 3.00 GB | Rank 1: 3.50 GB | Rank 2: 3.40 GB | Rank 3: 3.40 GB | Rank 4: 3.48 GB | Rank 5: 3.50 GB | Rank 6: 3.45 GB | Rank 7: 3.42 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 4: Batch  2 / 22, Shape: [32, 768], Time: 279ms
Rank 3: Batch  2 / 22, Shape: [32, 768], Time: 280ms
Rank 6: Batch  2 / 22, Shape: [32, 768], Time: 279ms
Rank 0: Batch  2 / 22, Shape: [32, 768], Time: 270ms
Rank 1: Batch  2 / 22, Shape: [32, 768], Time: 281ms
Rank 5: Batch  2 / 22, Shape: [32, 768], Time: 281ms
Rank 2: Batch  2 / 22, Shape: [32, 768], Time: 283ms
Rank 7: Batch  2 / 22, Shape: [32, 768], Time: 285ms
Rank 4: Batch  3 / 22, Shape: [32, 768], Time: 282ms
Rank 0: Batch  3 / 22, Shape: [32, 768], Time: 282ms
Rank 3: Batch  3 / 22, Shape: [32, 768], Time: 284ms
Rank 6: Batch  3 / 22, Shape: [32, 768], Time: 285ms
Rank 1: Batch  3 / 22, Shape: [32, 768], Time: 285ms
Rank 5: Batch  3 / 22, Shape: [32, 768], Time: 287ms
Rank 2: Batch  3 / 22, Shape: [32, 768], Time: 287ms
Rank 7: Batch  3 / 22, Shape: [32, 768], Time: 291ms
Rank 4: Batch  4 / 22, Shape: [32, 768], Time: 283ms
Rank 0: Batch  4 / 22, Shape: [32, 768], Time: 283ms
Rank 3: Batch  4 / 22, Shape: [32, 768], Time: 284ms
Rank 6: Batch  4 / 22, Shape: [32, 768], Time: 284ms
Rank 1: Batch  4 / 22, Shape: [32, 768], Time: 285ms
Rank 5: Batch  4 / 22, Shape: [32, 768], Time: 286ms
Rank 2: Batch  4 / 22, Shape: [32, 768], Time: 289ms
Rank 7: Batch  4 / 22, Shape: [32, 768], Time: 289ms
Rank 4: Batch  5 / 22, Shape: [32, 768], Time: 285ms
Rank 0: Batch  5 / 22, Shape: [32, 768], Time: 283ms
Rank 3: Batch  5 / 22, Shape: [32, 768], Time: 283ms
Rank 6: Batch  5 / 22, Shape: [32, 768], Time: 283ms
Rank 1: Batch  5 / 22, Shape: [32, 768], Time: 286ms
Rank 5: Batch  5 / 22, Shape: [32, 768], Time: 286ms
Rank 2: Batch  5 / 22, Shape: [32, 768], Time: 288ms
Rank 7: Batch  5 / 22, Shape: [32, 768], Time: 291ms
Rank 4: Batch  6 / 22, Shape: [32, 768], Time: 283ms
Rank 0: Batch  6 / 22, Shape: [32, 768], Time: 283ms
Rank 3: Batch  6 / 22, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 3.06 GB | Rank 1: 3.50 GB | Rank 2: 3.40 GB | Rank 3: 3.40 GB | Rank 4: 3.48 GB | Rank 5: 3.50 GB | Rank 6: 3.45 GB | Rank 7: 3.42 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch  6 / 22, Shape: [32, 768], Time: 285ms
Rank 1: Batch  6 / 22, Shape: [32, 768], Time: 286ms
Rank 5: Batch  6 / 22, Shape: [32, 768], Time: 287ms
Rank 2: Batch  6 / 22, Shape: [32, 768], Time: 288ms
Rank 7: Batch  6 / 22, Shape: [32, 768], Time: 290ms
Rank 4: Batch  7 / 22, Shape: [32, 768], Time: 281ms
Rank 0: Batch  7 / 22, Shape: [32, 768], Time: 279ms
Rank 3: Batch  7 / 22, Shape: [32, 768], Time: 284ms
Rank 6: Batch  7 / 22, Shape: [32, 768], Time: 285ms
Rank 1: Batch  7 / 22, Shape: [32, 768], Time: 287ms
Rank 5: Batch  7 / 22, Shape: [32, 768], Time: 286ms
Rank 2: Batch  7 / 22, Shape: [32, 768], Time: 288ms
Rank 7: Batch  7 / 22, Shape: [32, 768], Time: 289ms
Rank 4: Batch  8 / 22, Shape: [32, 768], Time: 282ms
Rank 0: Batch  8 / 22, Shape: [32, 768], Time: 284ms
Rank 3: Batch  8 / 22, Shape: [32, 768], Time: 284ms
Rank 6: Batch  8 / 22, Shape: [32, 768], Time: 283ms
Rank 1: Batch  8 / 22, Shape: [32, 768], Time: 287ms
Rank 5: Batch  8 / 22, Shape: [32, 768], Time: 286ms
Rank 2: Batch  8 / 22, Shape: [32, 768], Time: 288ms
Rank 7: Batch  8 / 22, Shape: [32, 768], Time: 289ms
Rank 4: Batch  9 / 22, Shape: [32, 768], Time: 283ms
Rank 0: Batch  9 / 22, Shape: [32, 768], Time: 284ms
Rank 3: Batch  9 / 22, Shape: [32, 768], Time: 283ms
Rank 6: Batch  9 / 22, Shape: [32, 768], Time: 285ms
Rank 1: Batch  9 / 22, Shape: [32, 768], Time: 286ms
Rank 5: Batch  9 / 22, Shape: [32, 768], Time: 287ms
Rank 2: Batch  9 / 22, Shape: [32, 768], Time: 289ms
Rank 7: Batch  9 / 22, Shape: [32, 768], Time: 290ms
Rank 4: Batch 10 / 22, Shape: [32, 768], Time: 283ms
Rank 0: Batch 10 / 22, Shape: [32, 768], Time: 282ms
Rank 3: Batch 10 / 22, Shape: [32, 768], Time: 283ms
Rank 6: Batch 10 / 22, Shape: [32, 768], Time: 285ms
Rank 1: Batch 10 / 22, Shape: [32, 768], Time: 287ms
Rank 5: Batch 10 / 22, Shape: [32, 768], Time: 288ms
Rank 2: Batch 10 / 22, Shape: [32, 768], Time: 287ms
Rank 7: Batch 10 / 22, Shape: [32, 768], Time: 292ms
Rank 4: Batch 11 / 22, Shape: [32, 768], Time: 282ms
Rank 0: Batch 11 / 22, Shape: [32, 768], Time: 284ms
Rank 3: Batch 11 / 22, Shape: [32, 768], Time: 284ms
Memory: Rank 0: 3.06 GB | Rank 1: 3.50 GB | Rank 2: 3.40 GB | Rank 3: 3.40 GB | Rank 4: 3.48 GB | Rank 5: 3.50 GB | Rank 6: 3.45 GB | Rank 7: 3.42 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 11 / 22, Shape: [32, 768], Time: 285ms
Rank 1: Batch 11 / 22, Shape: [32, 768], Time: 286ms
Rank 5: Batch 11 / 22, Shape: [32, 768], Time: 286ms
Rank 2: Batch 11 / 22, Shape: [32, 768], Time: 289ms
Rank 7: Batch 11 / 22, Shape: [32, 768], Time: 289ms
Rank 4: Batch 12 / 22, Shape: [32, 768], Time: 284ms
Rank 0: Batch 12 / 22, Shape: [32, 768], Time: 277ms
Rank 3: Batch 12 / 22, Shape: [32, 768], Time: 284ms
Rank 6: Batch 12 / 22, Shape: [32, 768], Time: 284ms
Rank 1: Batch 12 / 22, Shape: [32, 768], Time: 288ms
Rank 5: Batch 12 / 22, Shape: [32, 768], Time: 287ms
Rank 2: Batch 12 / 22, Shape: [32, 768], Time: 287ms
Rank 7: Batch 12 / 22, Shape: [32, 768], Time: 290ms
Rank 4: Batch 13 / 22, Shape: [32, 768], Time: 284ms
Rank 0: Batch 13 / 22, Shape: [32, 768], Time: 283ms
Rank 3: Batch 13 / 22, Shape: [32, 768], Time: 284ms
Rank 6: Batch 13 / 22, Shape: [32, 768], Time: 286ms
Rank 1: Batch 13 / 22, Shape: [32, 768], Time: 286ms
Rank 5: Batch 13 / 22, Shape: [32, 768], Time: 287ms
Rank 2: Batch 13 / 22, Shape: [32, 768], Time: 287ms
Rank 7: Batch 13 / 22, Shape: [32, 768], Time: 290ms
Rank 4: Batch 14 / 22, Shape: [32, 768], Time: 284ms
Rank 0: Batch 14 / 22, Shape: [32, 768], Time: 284ms
Rank 3: Batch 14 / 22, Shape: [32, 768], Time: 285ms
Rank 6: Batch 14 / 22, Shape: [32, 768], Time: 284ms
Rank 1: Batch 14 / 22, Shape: [32, 768], Time: 288ms
Rank 5: Batch 14 / 22, Shape: [32, 768], Time: 288ms
Rank 2: Batch 14 / 22, Shape: [32, 768], Time: 289ms
Rank 7: Batch 14 / 22, Shape: [32, 768], Time: 289ms
Rank 4: Batch 15 / 22, Shape: [32, 768], Time: 283ms
Rank 0: Batch 15 / 22, Shape: [32, 768], Time: 283ms
Rank 3: Batch 15 / 22, Shape: [32, 768], Time: 283ms
Rank 6: Batch 15 / 22, Shape: [32, 768], Time: 286ms
Rank 1: Batch 15 / 22, Shape: [32, 768], Time: 287ms
Rank 5: Batch 15 / 22, Shape: [32, 768], Time: 286ms
Rank 2: Batch 15 / 22, Shape: [32, 768], Time: 289ms
Rank 7: Batch 15 / 22, Shape: [32, 768], Time: 290ms
Rank 4: Batch 16 / 22, Shape: [32, 768], Time: 282ms
Rank 0: Batch 16 / 22, Shape: [32, 768], Time: 284ms
Rank 3: Batch 16 / 22, Shape: [32, 768], Time: 284ms
Memory: Rank 0: 3.06 GB | Rank 1: 3.50 GB | Rank 2: 3.40 GB | Rank 3: 3.40 GB | Rank 4: 3.48 GB | Rank 5: 3.50 GB | Rank 6: 3.45 GB | Rank 7: 3.42 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 16 / 22, Shape: [32, 768], Time: 285ms
Rank 1: Batch 16 / 22, Shape: [32, 768], Time: 287ms
Rank 5: Batch 16 / 22, Shape: [32, 768], Time: 287ms
Rank 2: Batch 16 / 22, Shape: [32, 768], Time: 289ms
Rank 7: Batch 16 / 22, Shape: [32, 768], Time: 290ms
Rank 4: Batch 17 / 22, Shape: [32, 768], Time: 281ms
Rank 0: Batch 17 / 22, Shape: [32, 768], Time: 278ms
Rank 3: Batch 17 / 22, Shape: [32, 768], Time: 285ms
Rank 6: Batch 17 / 22, Shape: [32, 768], Time: 286ms
Rank 1: Batch 17 / 22, Shape: [32, 768], Time: 285ms
Rank 5: Batch 17 / 22, Shape: [32, 768], Time: 287ms
Rank 2: Batch 17 / 22, Shape: [32, 768], Time: 287ms
Rank 7: Batch 17 / 22, Shape: [32, 768], Time: 289ms
Rank 4: Batch 18 / 22, Shape: [32, 768], Time: 283ms
Rank 0: Batch 18 / 22, Shape: [32, 768], Time: 283ms
Rank 3: Batch 18 / 22, Shape: [32, 768], Time: 284ms
Rank 6: Batch 18 / 22, Shape: [32, 768], Time: 285ms
Rank 1: Batch 18 / 22, Shape: [32, 768], Time: 287ms
Rank 5: Batch 18 / 22, Shape: [32, 768], Time: 286ms
Rank 2: Batch 18 / 22, Shape: [32, 768], Time: 287ms
Rank 7: Batch 18 / 22, Shape: [32, 768], Time: 289ms
Rank 4: Batch 19 / 22, Shape: [32, 768], Time: 283ms
Rank 0: Batch 19 / 22, Shape: [32, 768], Time: 286ms
Rank 3: Batch 19 / 22, Shape: [32, 768], Time: 284ms
Rank 6: Batch 19 / 22, Shape: [32, 768], Time: 286ms
Rank 1: Batch 19 / 22, Shape: [32, 768], Time: 289ms
Rank 5: Batch 19 / 22, Shape: [32, 768], Time: 287ms
Rank 2: Batch 19 / 22, Shape: [32, 768], Time: 288ms
Rank 7: Batch 19 / 22, Shape: [32, 768], Time: 291ms
Rank 4: Batch 20 / 22, Shape: [32, 768], Time: 282ms
Rank 0: Batch 20 / 22, Shape: [32, 768], Time: 283ms
Rank 3: Batch 20 / 22, Shape: [32, 768], Time: 283ms
Rank 6: Batch 20 / 22, Shape: [32, 768], Time: 284ms
Rank 1: Batch 20 / 22, Shape: [32, 768], Time: 288ms
Rank 5: Batch 20 / 22, Shape: [32, 768], Time: 288ms
Rank 2: Batch 20 / 22, Shape: [32, 768], Time: 289ms
Rank 7: Batch 20 / 22, Shape: [32, 768], Time: 290ms
Rank 4: Batch 21 / 22, Shape: [32, 768], Time: 284ms
Rank 0: Batch 21 / 22, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 3.06 GB | Rank 1: 3.50 GB | Rank 2: 3.40 GB | Rank 3: 3.40 GB | Rank 4: 3.48 GB | Rank 5: 3.50 GB | Rank 6: 3.45 GB | Rank 7: 3.42 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch 21 / 22, Shape: [32, 768], Time: 284ms
Rank 6: Batch 21 / 22, Shape: [32, 768], Time: 285ms
Rank 1: Batch 21 / 22, Shape: [32, 768], Time: 286ms
Rank 5: Batch 21 / 22, Shape: [32, 768], Time: 286ms
Rank 2: Batch 21 / 22, Shape: [32, 768], Time: 287ms
Rank 7: Batch 21 / 22, Shape: [32, 768], Time: 290ms
Rank 4: Batch 22 / 22, Shape: [6, 768], Time: 285ms
Rank 0: Batch 22 / 22, Shape: [6, 768], Time: 279ms
Total embeddings: 5424
Original texts: 5421
Expected padding: 3
Actual padding: 3
Rank 3: Batch 22 / 22, Shape: [6, 768], Time: 284ms
Rank 6: Batch 22 / 22, Shape: [6, 768], Time: 284ms
Rank 1: Batch 22 / 22, Shape: [6, 768], Time: 288ms
Rank 5: Batch 22 / 22, Shape: [6, 768], Time: 286ms
Rank 2: Batch 22 / 22, Shape: [6, 768], Time: 288ms
Rank 7: Batch 22 / 22, Shape: [6, 768], Time: 288ms
Maximum difference between padding embeddings: 0.0
Padding embeddings verified as very similar.
Encoding completed (6s)

Displaying samples from Evaluation data:
Evaluation[934]: Only open Monday - Friday, though, which is a bummer. - NEGATIVE
Tokens: ['only', 'open', 'monday', '-', 'friday', ',', 'though', ',', 'which', 'is', 'a', 'bum', '##mer', '.']
Embedding: [-0.16179574  0.0889227   0.04988103 -0.3482111  -0.07171748  0.18010229] ...

Evaluation[5091]: The bun was good. - POSITIVE
Tokens: ['the', 'bun', 'was', 'good', '.']
Embedding: [ 0.2744711   0.03801029  0.2346346  -0.5525171   0.17762573  0.24929269] ...

Evaluation[6511]: The wings are good. - POSITIVE
Tokens: ['the', 'wings', 'are', 'good', '.']
Embedding: [ 0.15618756  0.13955769  0.1894244  -0.45839962  0.26597428  0.11285578] ...


Encoding Evaluation data of 6530 texts distributed across 8 GPUs...
Batch Size: 32, Pooling: Mean, Empty Cache: False
Padding Evaluation data to 6536 texts for even distribution across 8 ranks...
Texts per rank: 817, Total batches: 205
Rank 1: Processing 817 texts (indices 817 to 1633) in 26 batches...
Rank 2: Processing 817 texts (indices 1634 to 2450) in 26 batches...
Rank 5: Processing 817 texts (indices 4085 to 4901) in 26 batches...
Rank 0: Processing 817 texts (indices 0 to 816) in 26 batches...
Rank 3: Processing 817 texts (indices 2451 to 3267) in 26 batches...
Rank 7: Processing 817 texts (indices 5719 to 6535) in 26 batches...
Rank 6: Processing 817 texts (indices 4902 to 5718) in 26 batches...
Rank 4: Processing 817 texts (indices 3268 to 4084) in 26 batches...
Rank 3: Batch  1 / 26, Shape: [32, 768], Time: 17ms
Rank 1: Batch  1 / 26, Shape: [32, 768], Time: 18ms
Rank 4: Batch  1 / 26, Shape: [32, 768], Time: 18ms
Rank 2: Batch  1 / 26, Shape: [32, 768], Time: 18ms
Rank 5: Batch  1 / 26, Shape: [32, 768], Time: 18ms
Rank 7: Batch  1 / 26, Shape: [32, 768], Time: 19ms
Rank 6: Batch  1 / 26, Shape: [32, 768], Time: 19ms
Rank 0: Batch  1 / 26, Shape: [32, 768], Time: 21ms
Memory: Rank 0: 3.06 GB | Rank 1: 3.50 GB | Rank 2: 3.40 GB | Rank 3: 3.40 GB | Rank 4: 3.48 GB | Rank 5: 3.50 GB | Rank 6: 3.45 GB | Rank 7: 3.42 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 4: Batch  2 / 26, Shape: [32, 768], Time: 280ms
Rank 3: Batch  2 / 26, Shape: [32, 768], Time: 281ms
Rank 6: Batch  2 / 26, Shape: [32, 768], Time: 281ms
Rank 0: Batch  2 / 26, Shape: [32, 768], Time: 273ms
Rank 1: Batch  2 / 26, Shape: [32, 768], Time: 285ms
Rank 2: Batch  2 / 26, Shape: [32, 768], Time: 285ms
Rank 5: Batch  2 / 26, Shape: [32, 768], Time: 285ms
Rank 7: Batch  2 / 26, Shape: [32, 768], Time: 288ms
Rank 4: Batch  3 / 26, Shape: [32, 768], Time: 284ms
Rank 3: Batch  3 / 26, Shape: [32, 768], Time: 284ms
Rank 0: Batch  3 / 26, Shape: [32, 768], Time: 281ms
Rank 6: Batch  3 / 26, Shape: [32, 768], Time: 284ms
Rank 1: Batch  3 / 26, Shape: [32, 768], Time: 287ms
Rank 2: Batch  3 / 26, Shape: [32, 768], Time: 288ms
Rank 5: Batch  3 / 26, Shape: [32, 768], Time: 288ms
Rank 7: Batch  3 / 26, Shape: [32, 768], Time: 290ms
Rank 4: Batch  4 / 26, Shape: [32, 768], Time: 282ms
Rank 3: Batch  4 / 26, Shape: [32, 768], Time: 284ms
Rank 0: Batch  4 / 26, Shape: [32, 768], Time: 284ms
Rank 6: Batch  4 / 26, Shape: [32, 768], Time: 285ms
Rank 1: Batch  4 / 26, Shape: [32, 768], Time: 287ms
Rank 5: Batch  4 / 26, Shape: [32, 768], Time: 287ms
Rank 2: Batch  4 / 26, Shape: [32, 768], Time: 288ms
Rank 7: Batch  4 / 26, Shape: [32, 768], Time: 291ms
Rank 4: Batch  5 / 26, Shape: [32, 768], Time: 283ms
Rank 3: Batch  5 / 26, Shape: [32, 768], Time: 284ms
Rank 0: Batch  5 / 26, Shape: [32, 768], Time: 283ms
Rank 6: Batch  5 / 26, Shape: [32, 768], Time: 284ms
Rank 1: Batch  5 / 26, Shape: [32, 768], Time: 287ms
Rank 5: Batch  5 / 26, Shape: [32, 768], Time: 287ms
Rank 2: Batch  5 / 26, Shape: [32, 768], Time: 289ms
Rank 7: Batch  5 / 26, Shape: [32, 768], Time: 289ms
Rank 4: Batch  6 / 26, Shape: [32, 768], Time: 285ms
Rank 0: Batch  6 / 26, Shape: [32, 768], Time: 282ms
Rank 3: Batch  6 / 26, Shape: [32, 768], Time: 284ms
Memory: Rank 0: 3.11 GB | Rank 1: 3.50 GB | Rank 2: 3.40 GB | Rank 3: 3.40 GB | Rank 4: 3.48 GB | Rank 5: 3.50 GB | Rank 6: 3.45 GB | Rank 7: 3.42 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch  6 / 26, Shape: [32, 768], Time: 285ms
Rank 1: Batch  6 / 26, Shape: [32, 768], Time: 287ms
Rank 5: Batch  6 / 26, Shape: [32, 768], Time: 288ms
Rank 2: Batch  6 / 26, Shape: [32, 768], Time: 288ms
Rank 7: Batch  6 / 26, Shape: [32, 768], Time: 290ms
Rank 0: Batch  7 / 26, Shape: [32, 768], Time: 279ms
Rank 4: Batch  7 / 26, Shape: [32, 768], Time: 283ms
Rank 3: Batch  7 / 26, Shape: [32, 768], Time: 285ms
Rank 6: Batch  7 / 26, Shape: [32, 768], Time: 286ms
Rank 1: Batch  7 / 26, Shape: [32, 768], Time: 287ms
Rank 5: Batch  7 / 26, Shape: [32, 768], Time: 288ms
Rank 2: Batch  7 / 26, Shape: [32, 768], Time: 288ms
Rank 7: Batch  7 / 26, Shape: [32, 768], Time: 290ms
Rank 4: Batch  8 / 26, Shape: [32, 768], Time: 282ms
Rank 0: Batch  8 / 26, Shape: [32, 768], Time: 283ms
Rank 3: Batch  8 / 26, Shape: [32, 768], Time: 283ms
Rank 6: Batch  8 / 26, Shape: [32, 768], Time: 286ms
Rank 1: Batch  8 / 26, Shape: [32, 768], Time: 286ms
Rank 5: Batch  8 / 26, Shape: [32, 768], Time: 287ms
Rank 2: Batch  8 / 26, Shape: [32, 768], Time: 288ms
Rank 7: Batch  8 / 26, Shape: [32, 768], Time: 290ms
Rank 0: Batch  9 / 26, Shape: [32, 768], Time: 283ms
Rank 3: Batch  9 / 26, Shape: [32, 768], Time: 284ms
Rank 4: Batch  9 / 26, Shape: [32, 768], Time: 289ms
Rank 6: Batch  9 / 26, Shape: [32, 768], Time: 285ms
Rank 1: Batch  9 / 26, Shape: [32, 768], Time: 290ms
Rank 5: Batch  9 / 26, Shape: [32, 768], Time: 291ms
Rank 2: Batch  9 / 26, Shape: [32, 768], Time: 291ms
Rank 7: Batch  9 / 26, Shape: [32, 768], Time: 289ms
Rank 0: Batch 10 / 26, Shape: [32, 768], Time: 282ms
Rank 4: Batch 10 / 26, Shape: [32, 768], Time: 279ms
Rank 3: Batch 10 / 26, Shape: [32, 768], Time: 284ms
Rank 6: Batch 10 / 26, Shape: [32, 768], Time: 285ms
Rank 1: Batch 10 / 26, Shape: [32, 768], Time: 283ms
Rank 5: Batch 10 / 26, Shape: [32, 768], Time: 287ms
Rank 2: Batch 10 / 26, Shape: [32, 768], Time: 288ms
Rank 7: Batch 10 / 26, Shape: [32, 768], Time: 291ms
Rank 0: Batch 11 / 26, Shape: [32, 768], Time: 282ms
Rank 4: Batch 11 / 26, Shape: [32, 768], Time: 282ms
Rank 3: Batch 11 / 26, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 3.11 GB | Rank 1: 3.50 GB | Rank 2: 3.40 GB | Rank 3: 3.40 GB | Rank 4: 3.48 GB | Rank 5: 3.50 GB | Rank 6: 3.45 GB | Rank 7: 3.42 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 11 / 26, Shape: [32, 768], Time: 285ms
Rank 1: Batch 11 / 26, Shape: [32, 768], Time: 287ms
Rank 5: Batch 11 / 26, Shape: [32, 768], Time: 287ms
Rank 2: Batch 11 / 26, Shape: [32, 768], Time: 289ms
Rank 7: Batch 11 / 26, Shape: [32, 768], Time: 290ms
Rank 0: Batch 12 / 26, Shape: [32, 768], Time: 269ms
Rank 4: Batch 12 / 26, Shape: [32, 768], Time: 283ms
Rank 3: Batch 12 / 26, Shape: [32, 768], Time: 285ms
Rank 6: Batch 12 / 26, Shape: [32, 768], Time: 284ms
Rank 1: Batch 12 / 26, Shape: [32, 768], Time: 288ms
Rank 5: Batch 12 / 26, Shape: [32, 768], Time: 287ms
Rank 2: Batch 12 / 26, Shape: [32, 768], Time: 288ms
Rank 7: Batch 12 / 26, Shape: [32, 768], Time: 290ms
Rank 0: Batch 13 / 26, Shape: [32, 768], Time: 282ms
Rank 4: Batch 13 / 26, Shape: [32, 768], Time: 283ms
Rank 3: Batch 13 / 26, Shape: [32, 768], Time: 285ms
Rank 6: Batch 13 / 26, Shape: [32, 768], Time: 284ms
Rank 1: Batch 13 / 26, Shape: [32, 768], Time: 287ms
Rank 5: Batch 13 / 26, Shape: [32, 768], Time: 288ms
Rank 2: Batch 13 / 26, Shape: [32, 768], Time: 288ms
Rank 7: Batch 13 / 26, Shape: [32, 768], Time: 292ms
Rank 0: Batch 14 / 26, Shape: [32, 768], Time: 283ms
Rank 4: Batch 14 / 26, Shape: [32, 768], Time: 282ms
Rank 3: Batch 14 / 26, Shape: [32, 768], Time: 285ms
Rank 6: Batch 14 / 26, Shape: [32, 768], Time: 285ms
Rank 1: Batch 14 / 26, Shape: [32, 768], Time: 287ms
Rank 5: Batch 14 / 26, Shape: [32, 768], Time: 287ms
Rank 2: Batch 14 / 26, Shape: [32, 768], Time: 288ms
Rank 7: Batch 14 / 26, Shape: [32, 768], Time: 289ms
Rank 0: Batch 15 / 26, Shape: [32, 768], Time: 283ms
Rank 4: Batch 15 / 26, Shape: [32, 768], Time: 283ms
Rank 3: Batch 15 / 26, Shape: [32, 768], Time: 284ms
Rank 6: Batch 15 / 26, Shape: [32, 768], Time: 284ms
Rank 1: Batch 15 / 26, Shape: [32, 768], Time: 286ms
Rank 5: Batch 15 / 26, Shape: [32, 768], Time: 287ms
Rank 2: Batch 15 / 26, Shape: [32, 768], Time: 287ms
Rank 7: Batch 15 / 26, Shape: [32, 768], Time: 291ms
Rank 0: Batch 16 / 26, Shape: [32, 768], Time: 282ms
Rank 4: Batch 16 / 26, Shape: [32, 768], Time: 283ms
Memory: Rank 0: 3.11 GB | Rank 1: 3.50 GB | Rank 2: 3.40 GB | Rank 3: 3.40 GB | Rank 4: 3.48 GB | Rank 5: 3.50 GB | Rank 6: 3.45 GB | Rank 7: 3.42 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch 16 / 26, Shape: [32, 768], Time: 284ms
Rank 6: Batch 16 / 26, Shape: [32, 768], Time: 284ms
Rank 1: Batch 16 / 26, Shape: [32, 768], Time: 287ms
Rank 5: Batch 16 / 26, Shape: [32, 768], Time: 287ms
Rank 2: Batch 16 / 26, Shape: [32, 768], Time: 288ms
Rank 7: Batch 16 / 26, Shape: [32, 768], Time: 289ms
Rank 4: Batch 17 / 26, Shape: [32, 768], Time: 283ms
Rank 0: Batch 17 / 26, Shape: [32, 768], Time: 280ms
Rank 3: Batch 17 / 26, Shape: [32, 768], Time: 284ms
Rank 6: Batch 17 / 26, Shape: [32, 768], Time: 287ms
Rank 1: Batch 17 / 26, Shape: [32, 768], Time: 287ms
Rank 5: Batch 17 / 26, Shape: [32, 768], Time: 287ms
Rank 2: Batch 17 / 26, Shape: [32, 768], Time: 288ms
Rank 7: Batch 17 / 26, Shape: [32, 768], Time: 291ms
Rank 0: Batch 18 / 26, Shape: [32, 768], Time: 283ms
Rank 4: Batch 18 / 26, Shape: [32, 768], Time: 284ms
Rank 3: Batch 18 / 26, Shape: [32, 768], Time: 284ms
Rank 6: Batch 18 / 26, Shape: [32, 768], Time: 285ms
Rank 1: Batch 18 / 26, Shape: [32, 768], Time: 287ms
Rank 5: Batch 18 / 26, Shape: [32, 768], Time: 287ms
Rank 2: Batch 18 / 26, Shape: [32, 768], Time: 288ms
Rank 7: Batch 18 / 26, Shape: [32, 768], Time: 289ms
Rank 4: Batch 19 / 26, Shape: [32, 768], Time: 281ms
Rank 0: Batch 19 / 26, Shape: [32, 768], Time: 284ms
Rank 3: Batch 19 / 26, Shape: [32, 768], Time: 285ms
Rank 6: Batch 19 / 26, Shape: [32, 768], Time: 285ms
Rank 1: Batch 19 / 26, Shape: [32, 768], Time: 287ms
Rank 5: Batch 19 / 26, Shape: [32, 768], Time: 287ms
Rank 2: Batch 19 / 26, Shape: [32, 768], Time: 288ms
Rank 7: Batch 19 / 26, Shape: [32, 768], Time: 290ms
Rank 4: Batch 20 / 26, Shape: [32, 768], Time: 283ms
Rank 0: Batch 20 / 26, Shape: [32, 768], Time: 283ms
Rank 3: Batch 20 / 26, Shape: [32, 768], Time: 285ms
Rank 6: Batch 20 / 26, Shape: [32, 768], Time: 284ms
Rank 1: Batch 20 / 26, Shape: [32, 768], Time: 288ms
Rank 5: Batch 20 / 26, Shape: [32, 768], Time: 287ms
Rank 2: Batch 20 / 26, Shape: [32, 768], Time: 288ms
Rank 7: Batch 20 / 26, Shape: [32, 768], Time: 291ms
Rank 4: Batch 21 / 26, Shape: [32, 768], Time: 282ms
Rank 0: Batch 21 / 26, Shape: [32, 768], Time: 282ms
Memory: Rank 0: 3.11 GB | Rank 1: 3.50 GB | Rank 2: 3.40 GB | Rank 3: 3.40 GB | Rank 4: 3.48 GB | Rank 5: 3.50 GB | Rank 6: 3.45 GB | Rank 7: 3.42 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch 21 / 26, Shape: [32, 768], Time: 284ms
Rank 6: Batch 21 / 26, Shape: [32, 768], Time: 285ms
Rank 1: Batch 21 / 26, Shape: [32, 768], Time: 287ms
Rank 5: Batch 21 / 26, Shape: [32, 768], Time: 287ms
Rank 2: Batch 21 / 26, Shape: [32, 768], Time: 288ms
Rank 7: Batch 21 / 26, Shape: [32, 768], Time: 289ms
Rank 4: Batch 22 / 26, Shape: [32, 768], Time: 284ms
Rank 0: Batch 22 / 26, Shape: [32, 768], Time: 280ms
Rank 3: Batch 22 / 26, Shape: [32, 768], Time: 284ms
Rank 6: Batch 22 / 26, Shape: [32, 768], Time: 285ms
Rank 1: Batch 22 / 26, Shape: [32, 768], Time: 287ms
Rank 5: Batch 22 / 26, Shape: [32, 768], Time: 288ms
Rank 2: Batch 22 / 26, Shape: [32, 768], Time: 289ms
Rank 7: Batch 22 / 26, Shape: [32, 768], Time: 290ms
Rank 4: Batch 23 / 26, Shape: [32, 768], Time: 283ms
Rank 0: Batch 23 / 26, Shape: [32, 768], Time: 283ms
Rank 3: Batch 23 / 26, Shape: [32, 768], Time: 284ms
Rank 6: Batch 23 / 26, Shape: [32, 768], Time: 284ms
Rank 1: Batch 23 / 26, Shape: [32, 768], Time: 286ms
Rank 5: Batch 23 / 26, Shape: [32, 768], Time: 286ms
Rank 2: Batch 23 / 26, Shape: [32, 768], Time: 289ms
Rank 7: Batch 23 / 26, Shape: [32, 768], Time: 290ms
Rank 4: Batch 24 / 26, Shape: [32, 768], Time: 282ms
Rank 0: Batch 24 / 26, Shape: [32, 768], Time: 283ms
Rank 3: Batch 24 / 26, Shape: [32, 768], Time: 284ms
Rank 6: Batch 24 / 26, Shape: [32, 768], Time: 285ms
Rank 1: Batch 24 / 26, Shape: [32, 768], Time: 288ms
Rank 5: Batch 24 / 26, Shape: [32, 768], Time: 289ms
Rank 2: Batch 24 / 26, Shape: [32, 768], Time: 289ms
Rank 7: Batch 24 / 26, Shape: [32, 768], Time: 290ms
Rank 4: Batch 25 / 26, Shape: [32, 768], Time: 283ms
Rank 0: Batch 25 / 26, Shape: [32, 768], Time: 282ms
Rank 3: Batch 25 / 26, Shape: [32, 768], Time: 284ms
Rank 6: Batch 25 / 26, Shape: [32, 768], Time: 285ms
Rank 1: Batch 25 / 26, Shape: [32, 768], Time: 287ms
Rank 5: Batch 25 / 26, Shape: [32, 768], Time: 287ms
Rank 2: Batch 25 / 26, Shape: [32, 768], Time: 288ms
Rank 7: Batch 25 / 26, Shape: [32, 768], Time: 289ms
Rank 4: Batch 26 / 26, Shape: [17, 768], Time: 284ms
Rank 0: Batch 26 / 26, Shape: [17, 768], Time: 284ms
Memory: Rank 0: 3.11 GB | Rank 1: 3.50 GB | Rank 2: 3.40 GB | Rank 3: 3.40 GB | Rank 4: 3.48 GB | Rank 5: 3.50 GB | Rank 6: 3.45 GB | Rank 7: 3.42 GB (Max: 16.93 GB, Total: 135.43 GB)
Total embeddings: 6536
Original texts: 6530
Expected padding: 6
Actual padding: 6
Rank 3: Batch 26 / 26, Shape: [17, 768], Time: 284ms
Rank 6: Batch 26 / 26, Shape: [17, 768], Time: 285ms
Rank 1: Batch 26 / 26, Shape: [17, 768], Time: 288ms
Rank 5: Batch 26 / 26, Shape: [17, 768], Time: 287ms
Rank 2: Batch 26 / 26, Shape: [17, 768], Time: 289ms
Rank 7: Batch 26 / 26, Shape: [17, 768], Time: 290ms
Maximum difference between padding embeddings: 0.0
Padding embeddings verified as very similar.
Encoding completed (7s)

Data saved to: saves/data_8_gpu_20241029-040436.npz
X Train shape: [102097, 768], X Validation shape: [5421, 768], X Test shape: [6530, 768]
y Train shape: [102097], y Validation shape: [5421], y Test shape: [6530]
Data processed (2m 25s)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 1, Max Grad Norm: None
Batch Size: 32, Max Epochs: 100, LR: 1e-05, Early Stop: score, Fine-tune BERT: False, Fine-tune Layers: 1, Freeze BERT: False, Target Score: None, Interactive: False
Classifier initialized (31ms)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 10 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Classes: ['negative', 'neutral', 'positive'], Number of classes: 3
Initializing model, graph, optimizer...

Model Architecture Summary:
Classifier(
  (layers): Sequential(
    (0): Linear(in_features=768, out_features=1024, bias=True)
    (1): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=1024, out_features=1024, bias=True)
    (4): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (5): Dropout(p=0.3, inplace=False)
    (6): Linear(in_features=1024, out_features=3, bias=True)
  )
)

Model Parameters:
  layers.0: 787,456 (trainable)
  layers.1.projection: 2,099,200 (trainable)
  layers.3: 1,049,600 (trainable)
  layers.6: 3,075 (trainable)

Total layers: 4
Trainable layers: 4
Total parameters: 3,939,331
Trainable parameters: 3,939,331
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 1, Max Iterations: 100, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 10
Epoch 1/100: 100%|█████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 78.74it/s, loss=1.0328, lr=9.06e-06, grad=M:0.0531, status=Epoch complete]
█ Epoch   1: Loss T 1.032821 V 1.104772 | F1 T 0.218192 V 0.160664 B 0.160664 SC 0/10 | Acc T 0.481597 V 0.309366 | LR 9.06e-06 | Grad ↓ 0.002658 ↑ 0.139786 M 0.053100 | 5s
Epoch 2/100: 100%|█████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 80.45it/s, loss=0.9236, lr=6.59e-06, grad=M:0.1416, status=Epoch complete]
█ Epoch   2: Loss T 0.923607 V 1.049243 | F1 T 0.392835 V 0.341861 B 0.341861 SC 0/10 | Acc T 0.561712 V 0.424594 | LR 6.59e-06 | Grad ↓ 0.009198 ↑ 0.549781 M 0.141584 | 4s
Epoch 3/100: 100%|█████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 81.85it/s, loss=0.8865, lr=3.53e-06, grad=M:0.2056, status=Epoch complete]
█ Epoch   3: Loss T 0.886499 V 1.000051 | F1 T 0.426853 V 0.386606 B 0.386606 SC 0/10 | Acc T 0.585335 V 0.474926 | LR 3.53e-06 | Grad ↓ 0.011626 ↑ 0.817411 M 0.205598 | 4s
Epoch 4/100: 100%|█████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 83.37it/s, loss=0.8648, lr=1.05e-06, grad=M:0.1517, status=Epoch complete]
█ Epoch   4: Loss T 0.864781 V 0.984773 | F1 T 0.432246 V 0.393080 B 0.393080 SC 0/10 | Acc T 0.591338 V 0.482854 | LR 1.05e-06 | Grad ↓ 0.006307 ↑ 0.476798 M 0.151737 | 4s
Epoch 5/100: 100%|█████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 81.79it/s, loss=0.8589, lr=1.00e-07, grad=M:0.1515, status=Epoch complete]
█ Epoch   5: Loss T 0.858937 V 0.990887 | F1 T 0.432751 V 0.391714 B 0.393080 SC 1/10 | Acc T 0.593894 V 0.481748 | LR 1.00e-07 | Grad ↓ 0.006878 ↑ 0.435682 M 0.151459 | 4s
Saved model state: checkpoints/checkpoint_epoch_5_20241029-040520.pth
Epoch 6/100: 100%|█████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 78.81it/s, loss=0.8441, lr=9.06e-06, grad=M:0.1991, status=Epoch complete]
█ Epoch   6: Loss T 0.844055 V 0.993022 | F1 T 0.442942 V 0.395408 B 0.395408 SC 0/10 | Acc T 0.613365 V 0.490229 | LR 9.06e-06 | Grad ↓ 0.009561 ↑ 0.597139 M 0.199073 | 5s
Epoch 7/100: 100%|█████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 83.71it/s, loss=0.8168, lr=6.59e-06, grad=M:0.3698, status=Epoch complete]
█ Epoch   7: Loss T 0.816762 V 0.951268 | F1 T 0.454113 V 0.404568 B 0.404568 SC 0/10 | Acc T 0.624461 V 0.500000 | LR 6.59e-06 | Grad ↓ 0.023510 ↑ 1.121228 M 0.369771 | 4s
Epoch 8/100: 100%|█████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 78.51it/s, loss=0.7946, lr=3.53e-06, grad=M:0.6067, status=Epoch complete]
█ Epoch   8: Loss T 0.794625 V 0.930495 | F1 T 0.503186 V 0.456356 B 0.456356 SC 0/10 | Acc T 0.638614 V 0.523046 | LR 3.53e-06 | Grad ↓ 0.029053 ↑ 2.179559 M 0.606653 | 5s
Epoch 9/100: 100%|█████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 81.93it/s, loss=0.7773, lr=1.05e-06, grad=M:0.2497, status=Epoch complete]
█ Epoch   9: Loss T 0.777293 V 0.916092 | F1 T 0.548479 V 0.507664 B 0.507664 SC 0/10 | Acc T 0.651982 V 0.547566 | LR 1.05e-06 | Grad ↓ 0.009520 ↑ 0.784116 M 0.249652 | 4s
Epoch 10/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 77.92it/s, loss=0.7704, lr=1.00e-07, grad=M:0.3106, status=Epoch complete]
█ Epoch  10: Loss T 0.770389 V 0.905553 | F1 T 0.566286 V 0.528909 B 0.528909 SC 0/10 | Acc T 0.657007 V 0.559919 | LR 1.00e-07 | Grad ↓ 0.013294 ↑ 0.991228 M 0.310588 | 5s
Memory: Rank 0: 7.69 GB | Rank 1: 3.51 GB | Rank 2: 3.41 GB | Rank 3: 3.41 GB | Rank 4: 3.49 GB | Rank 5: 3.51 GB | Rank 6: 3.46 GB | Rank 7: 3.43 GB (Max: 16.93 GB, Total: 135.43 GB)
Saved model state: checkpoints/checkpoint_epoch_10_20241029-040545.pth
Epoch 11/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 79.49it/s, loss=0.7481, lr=9.06e-06, grad=M:0.3270, status=Epoch complete]
█ Epoch  11: Loss T 0.748078 V 0.845677 | F1 T 0.660045 V 0.631524 B 0.631524 SC 0/10 | Acc T 0.697710 V 0.633850 | LR 9.06e-06 | Grad ↓ 0.010354 ↑ 1.131746 M 0.326991 | 5s
Epoch 12/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 79.71it/s, loss=0.7152, lr=6.59e-06, grad=M:0.6801, status=Epoch complete]
█ Epoch  12: Loss T 0.715244 V 0.819172 | F1 T 0.678058 V 0.648747 B 0.648747 SC 0/10 | Acc T 0.707592 V 0.649152 | LR 6.59e-06 | Grad ↓ 0.035732 ↑ 2.124490 M 0.680121 | 5s
Epoch 13/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 76.73it/s, loss=0.7008, lr=3.53e-06, grad=M:0.7347, status=Epoch complete]
█ Epoch  13: Loss T 0.700836 V 0.813914 | F1 T 0.683132 V 0.652456 B 0.652456 SC 0/10 | Acc T 0.711441 V 0.653024 | LR 3.53e-06 | Grad ↓ 0.044752 ↑ 2.434731 M 0.734747 | 5s
Epoch 14/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 79.86it/s, loss=0.6941, lr=1.05e-06, grad=M:0.5894, status=Epoch complete]
█ Epoch  14: Loss T 0.694129 V 0.812470 | F1 T 0.684862 V 0.653563 B 0.653563 SC 0/10 | Acc T 0.712646 V 0.653761 | LR 1.05e-06 | Grad ↓ 0.027702 ↑ 1.913824 M 0.589449 | 5s
Epoch 15/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 78.57it/s, loss=0.6916, lr=1.00e-07, grad=M:0.4731, status=Epoch complete]
█ Epoch  15: Loss T 0.691563 V 0.810305 | F1 T 0.686173 V 0.654012 B 0.654012 SC 0/10 | Acc T 0.713371 V 0.654314 | LR 1.00e-07 | Grad ↓ 0.020611 ↑ 1.527685 M 0.473144 | 5s
Saved model state: checkpoints/checkpoint_epoch_15_20241029-040611.pth
Epoch 16/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 81.81it/s, loss=0.6896, lr=9.06e-06, grad=M:0.4507, status=Epoch complete]
█ Epoch  16: Loss T 0.689557 V 0.808310 | F1 T 0.689635 V 0.654298 B 0.654298 SC 0/10 | Acc T 0.716750 V 0.654499 | LR 9.06e-06 | Grad ↓ 0.013580 ↑ 1.562729 M 0.450696 | 4s
Epoch 17/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 82.54it/s, loss=0.6805, lr=6.59e-06, grad=M:0.5152, status=Epoch complete]
█ Epoch  17: Loss T 0.680525 V 0.787611 | F1 T 0.696258 V 0.664353 B 0.664353 SC 0/10 | Acc T 0.719590 V 0.663901 | LR 6.59e-06 | Grad ↓ 0.029649 ↑ 1.692826 M 0.515168 | 4s
Epoch 18/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 81.71it/s, loss=0.6751, lr=3.53e-06, grad=M:0.4156, status=Epoch complete]
█ Epoch  18: Loss T 0.675058 V 0.796952 | F1 T 0.694979 V 0.660785 B 0.664353 SC 1/10 | Acc T 0.720510 V 0.660398 | LR 3.53e-06 | Grad ↓ 0.007429 ↑ 1.547382 M 0.415630 | 4s
Epoch 19/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 81.19it/s, loss=0.6712, lr=1.05e-06, grad=M:1.1379, status=Epoch complete]
█ Epoch  19: Loss T 0.671236 V 0.795039 | F1 T 0.696964 V 0.661736 B 0.664353 SC 2/10 | Acc T 0.721911 V 0.661689 | LR 1.05e-06 | Grad ↓ 0.053757 ↑ 3.885336 M 1.137915 | 4s
Epoch 20/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 80.40it/s, loss=0.6701, lr=1.00e-07, grad=M:0.4444, status=Epoch complete]
█ Epoch  20: Loss T 0.670072 V 0.793959 | F1 T 0.698042 V 0.663573 B 0.664353 SC 3/10 | Acc T 0.722577 V 0.663717 | LR 1.00e-07 | Grad ↓ 0.024652 ↑ 1.426491 M 0.444412 | 4s
Memory: Rank 0: 7.69 GB | Rank 1: 3.51 GB | Rank 2: 3.41 GB | Rank 3: 3.41 GB | Rank 4: 3.49 GB | Rank 5: 3.51 GB | Rank 6: 3.46 GB | Rank 7: 3.43 GB (Max: 16.93 GB, Total: 135.43 GB)
Saved model state: checkpoints/checkpoint_epoch_20_20241029-040635.pth
Epoch 21/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 83.95it/s, loss=0.6715, lr=9.06e-06, grad=M:0.8406, status=Epoch complete]
█ Epoch  21: Loss T 0.671489 V 0.790488 | F1 T 0.700602 V 0.662638 B 0.664353 SC 4/10 | Acc T 0.724839 V 0.662795 | LR 9.06e-06 | Grad ↓ 0.049104 ↑ 3.121963 M 0.840612 | 4s
Epoch 22/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 80.90it/s, loss=0.6656, lr=6.59e-06, grad=M:0.8726, status=Epoch complete]
█ Epoch  22: Loss T 0.665584 V 0.798003 | F1 T 0.700922 V 0.664281 B 0.664353 SC 5/10 | Acc T 0.724898 V 0.665929 | LR 6.59e-06 | Grad ↓ 0.037495 ↑ 2.908779 M 0.872602 | 4s
Epoch 23/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 83.30it/s, loss=0.6622, lr=3.53e-06, grad=M:1.3323, status=Epoch complete]
█ Epoch  23: Loss T 0.662164 V 0.779782 | F1 T 0.704340 V 0.667739 B 0.667739 SC 0/10 | Acc T 0.726779 V 0.667588 | LR 3.53e-06 | Grad ↓ 0.058460 ↑ 4.549452 M 1.332309 | 4s
Epoch 24/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 83.20it/s, loss=0.6588, lr=1.05e-06, grad=M:0.6772, status=Epoch complete]
█ Epoch  24: Loss T 0.658775 V 0.790762 | F1 T 0.702472 V 0.665122 B 0.667739 SC 1/10 | Acc T 0.727161 V 0.665007 | LR 1.05e-06 | Grad ↓ 0.030788 ↑ 2.285776 M 0.677179 | 4s
Epoch 25/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 84.11it/s, loss=0.6577, lr=1.00e-07, grad=M:0.5316, status=Epoch complete]
█ Epoch  25: Loss T 0.657711 V 0.786361 | F1 T 0.704123 V 0.667538 B 0.667739 SC 2/10 | Acc T 0.727719 V 0.667588 | LR 1.00e-07 | Grad ↓ 0.025675 ↑ 1.794897 M 0.531603 | 4s
Saved model state: checkpoints/checkpoint_epoch_25_20241029-040700.pth
Epoch 26/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 79.29it/s, loss=0.6593, lr=9.06e-06, grad=M:1.1112, status=Epoch complete]
█ Epoch  26: Loss T 0.659290 V 0.801266 | F1 T 0.700286 V 0.657859 B 0.667739 SC 3/10 | Acc T 0.727072 V 0.658001 | LR 9.06e-06 | Grad ↓ 0.047747 ↑ 3.894236 M 1.111248 | 5s
Epoch 27/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 79.82it/s, loss=0.6567, lr=6.59e-06, grad=M:0.4837, status=Epoch complete]
█ Epoch  27: Loss T 0.656689 V 0.786966 | F1 T 0.704692 V 0.667505 B 0.667739 SC 4/10 | Acc T 0.728923 V 0.667220 | LR 6.59e-06 | Grad ↓ 0.022715 ↑ 1.586157 M 0.483656 | 5s
Epoch 28/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 77.49it/s, loss=0.6528, lr=3.53e-06, grad=M:0.3866, status=Epoch complete]
█ Epoch  28: Loss T 0.652806 V 0.784997 | F1 T 0.707179 V 0.668263 B 0.668263 SC 0/10 | Acc T 0.730872 V 0.668326 | LR 3.53e-06 | Grad ↓ 0.016915 ↑ 1.289406 M 0.386615 | 5s
Epoch 29/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 79.65it/s, loss=0.6523, lr=1.05e-06, grad=M:0.6632, status=Epoch complete]
█ Epoch  29: Loss T 0.652322 V 0.778803 | F1 T 0.708934 V 0.670314 B 0.670314 SC 0/10 | Acc T 0.731685 V 0.670354 | LR 1.05e-06 | Grad ↓ 0.017266 ↑ 2.400537 M 0.663238 | 5s
Epoch 30/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 79.81it/s, loss=0.6504, lr=1.00e-07, grad=M:0.4798, status=Epoch complete]
█ Epoch  30: Loss T 0.650435 V 0.780537 | F1 T 0.708469 V 0.669420 B 0.670314 SC 1/10 | Acc T 0.731548 V 0.669432 | LR 1.00e-07 | Grad ↓ 0.005422 ↑ 1.787913 M 0.479801 | 5s
Memory: Rank 0: 7.69 GB | Rank 1: 3.51 GB | Rank 2: 3.41 GB | Rank 3: 3.41 GB | Rank 4: 3.49 GB | Rank 5: 3.51 GB | Rank 6: 3.46 GB | Rank 7: 3.43 GB (Max: 16.93 GB, Total: 135.43 GB)
Saved model state: checkpoints/checkpoint_epoch_30_20241029-040725.pth
Epoch 31/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 79.72it/s, loss=0.6540, lr=9.06e-06, grad=M:1.1320, status=Epoch complete]
█ Epoch  31: Loss T 0.653994 V 0.783802 | F1 T 0.706535 V 0.667074 B 0.670314 SC 2/10 | Acc T 0.731000 V 0.667035 | LR 9.06e-06 | Grad ↓ 0.047983 ↑ 3.792162 M 1.132019 | 5s
Epoch 32/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 83.15it/s, loss=0.6506, lr=6.59e-06, grad=M:0.7978, status=Epoch complete]
█ Epoch  32: Loss T 0.650641 V 0.775187 | F1 T 0.708300 V 0.671353 B 0.671353 SC 0/10 | Acc T 0.731421 V 0.671091 | LR 6.59e-06 | Grad ↓ 0.029200 ↑ 2.743241 M 0.797847 | 4s
Epoch 33/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 78.64it/s, loss=0.6477, lr=3.53e-06, grad=M:0.9087, status=Epoch complete]
█ Epoch  33: Loss T 0.647722 V 0.783110 | F1 T 0.708192 V 0.668642 B 0.671353 SC 1/10 | Acc T 0.732371 V 0.668879 | LR 3.53e-06 | Grad ↓ 0.030575 ↑ 3.167047 M 0.908708 | 5s
Epoch 34/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 82.66it/s, loss=0.6463, lr=1.05e-06, grad=M:0.9515, status=Epoch complete]
█ Epoch  34: Loss T 0.646341 V 0.788646 | F1 T 0.706903 V 0.666726 B 0.671353 SC 2/10 | Acc T 0.732018 V 0.666851 | LR 1.05e-06 | Grad ↓ 0.049160 ↑ 3.487969 M 0.951480 | 4s
Epoch 35/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 87.11it/s, loss=0.6448, lr=1.00e-07, grad=M:0.9295, status=Epoch complete]
█ Epoch  35: Loss T 0.644799 V 0.776018 | F1 T 0.710608 V 0.672111 B 0.672111 SC 0/10 | Acc T 0.733311 V 0.672198 | LR 1.00e-07 | Grad ↓ 0.043929 ↑ 3.487795 M 0.929531 | 4s
Saved model state: checkpoints/checkpoint_epoch_35_20241029-040750.pth
Epoch 36/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 79.85it/s, loss=0.6476, lr=9.06e-06, grad=M:0.8208, status=Epoch complete]
█ Epoch  36: Loss T 0.647608 V 0.782422 | F1 T 0.708520 V 0.666970 B 0.672111 SC 1/10 | Acc T 0.732890 V 0.667404 | LR 9.06e-06 | Grad ↓ 0.029927 ↑ 2.874298 M 0.820824 | 5s
Epoch 37/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 77.36it/s, loss=0.6457, lr=6.59e-06, grad=M:1.4149, status=Epoch complete]
█ Epoch  37: Loss T 0.645701 V 0.780928 | F1 T 0.708072 V 0.668709 B 0.672111 SC 2/10 | Acc T 0.732391 V 0.668510 | LR 6.59e-06 | Grad ↓ 0.057109 ↑ 5.130360 M 1.414910 | 5s
Epoch 38/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 82.71it/s, loss=0.6440, lr=3.53e-06, grad=M:0.4667, status=Epoch complete]
█ Epoch  38: Loss T 0.643972 V 0.780658 | F1 T 0.709631 V 0.666903 B 0.672111 SC 3/10 | Acc T 0.734085 V 0.667773 | LR 3.53e-06 | Grad ↓ 0.018184 ↑ 1.605910 M 0.466657 | 4s
Epoch 39/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 78.18it/s, loss=0.6425, lr=1.05e-06, grad=M:0.6716, status=Epoch complete]
█ Epoch  39: Loss T 0.642540 V 0.771837 | F1 T 0.711655 V 0.671510 B 0.672111 SC 4/10 | Acc T 0.734418 V 0.671645 | LR 1.05e-06 | Grad ↓ 0.024647 ↑ 2.334362 M 0.671617 | 5s
Epoch 40/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 79.22it/s, loss=0.6410, lr=1.00e-07, grad=M:0.4210, status=Epoch complete]
█ Epoch  40: Loss T 0.641039 V 0.771366 | F1 T 0.712015 V 0.673004 B 0.673004 SC 0/10 | Acc T 0.734526 V 0.673119 | LR 1.00e-07 | Grad ↓ 0.008899 ↑ 1.433155 M 0.421024 | 5s
Memory: Rank 0: 7.69 GB | Rank 1: 3.51 GB | Rank 2: 3.41 GB | Rank 3: 3.41 GB | Rank 4: 3.49 GB | Rank 5: 3.51 GB | Rank 6: 3.46 GB | Rank 7: 3.43 GB (Max: 16.93 GB, Total: 135.43 GB)
Saved model state: checkpoints/checkpoint_epoch_40_20241029-040815.pth
Epoch 41/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 80.63it/s, loss=0.6442, lr=9.06e-06, grad=M:0.4180, status=Epoch complete]
█ Epoch  41: Loss T 0.644206 V 0.783655 | F1 T 0.709206 V 0.664521 B 0.673004 SC 1/10 | Acc T 0.734055 V 0.665007 | LR 9.06e-06 | Grad ↓ 0.007476 ↑ 1.584713 M 0.417965 | 4s
Epoch 42/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 81.25it/s, loss=0.6433, lr=6.59e-06, grad=M:1.6381, status=Epoch complete]
█ Epoch  42: Loss T 0.643259 V 0.771109 | F1 T 0.712925 V 0.673457 B 0.673457 SC 0/10 | Acc T 0.735711 V 0.674041 | LR 6.59e-06 | Grad ↓ 0.060625 ↑ 5.624638 M 1.638127 | 4s
Epoch 43/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 77.36it/s, loss=0.6404, lr=3.53e-06, grad=M:0.7806, status=Epoch complete]
█ Epoch  43: Loss T 0.640392 V 0.778918 | F1 T 0.710922 V 0.669478 B 0.673457 SC 1/10 | Acc T 0.735123 V 0.670170 | LR 3.53e-06 | Grad ↓ 0.029749 ↑ 2.895650 M 0.780619 | 5s
Epoch 44/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 76.92it/s, loss=0.6390, lr=1.05e-06, grad=M:0.4243, status=Epoch complete]
█ Epoch  44: Loss T 0.639028 V 0.769930 | F1 T 0.713535 V 0.674098 B 0.674098 SC 0/10 | Acc T 0.735975 V 0.674410 | LR 1.05e-06 | Grad ↓ 0.011337 ↑ 1.541531 M 0.424284 | 5s
Epoch 45/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 82.71it/s, loss=0.6377, lr=1.00e-07, grad=M:0.5889, status=Epoch complete]
█ Epoch  45: Loss T 0.637657 V 0.772088 | F1 T 0.713124 V 0.673514 B 0.674098 SC 1/10 | Acc T 0.735887 V 0.673857 | LR 1.00e-07 | Grad ↓ 0.021920 ↑ 2.072909 M 0.588939 | 4s
Saved model state: checkpoints/checkpoint_epoch_45_20241029-040841.pth
Epoch 46/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 74.84it/s, loss=0.6417, lr=9.06e-06, grad=M:0.9285, status=Epoch complete]
█ Epoch  46: Loss T 0.641721 V 0.766072 | F1 T 0.711338 V 0.674687 B 0.674687 SC 0/10 | Acc T 0.733272 V 0.674041 | LR 9.06e-06 | Grad ↓ 0.030919 ↑ 3.567096 M 0.928547 | 5s
Epoch 47/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 83.24it/s, loss=0.6408, lr=6.59e-06, grad=M:0.7314, status=Epoch complete]
█ Epoch  47: Loss T 0.640775 V 0.753866 | F1 T 0.716277 V 0.679761 B 0.679761 SC 0/10 | Acc T 0.735544 V 0.680310 | LR 6.59e-06 | Grad ↓ 0.023831 ↑ 2.547413 M 0.731390 | 4s
Epoch 48/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 78.00it/s, loss=0.6384, lr=3.53e-06, grad=M:0.6278, status=Epoch complete]
█ Epoch  48: Loss T 0.638366 V 0.773407 | F1 T 0.712124 V 0.670859 B 0.679761 SC 1/10 | Acc T 0.735681 V 0.670723 | LR 3.53e-06 | Grad ↓ 0.016227 ↑ 2.261521 M 0.627775 | 5s
Epoch 49/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 83.51it/s, loss=0.6371, lr=1.05e-06, grad=M:0.8767, status=Epoch complete]
█ Epoch  49: Loss T 0.637085 V 0.768169 | F1 T 0.714320 V 0.673350 B 0.679761 SC 2/10 | Acc T 0.736651 V 0.673488 | LR 1.05e-06 | Grad ↓ 0.028340 ↑ 3.219014 M 0.876728 | 4s
Epoch 50/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 81.39it/s, loss=0.6355, lr=1.00e-07, grad=M:0.6945, status=Epoch complete]
█ Epoch  50: Loss T 0.635511 V 0.765672 | F1 T 0.715165 V 0.675436 B 0.679761 SC 3/10 | Acc T 0.737072 V 0.675701 | LR 1.00e-07 | Grad ↓ 0.016680 ↑ 2.429785 M 0.694515 | 4s
Memory: Rank 0: 7.69 GB | Rank 1: 3.51 GB | Rank 2: 3.41 GB | Rank 3: 3.41 GB | Rank 4: 3.49 GB | Rank 5: 3.51 GB | Rank 6: 3.46 GB | Rank 7: 3.43 GB (Max: 16.93 GB, Total: 135.43 GB)
Saved model state: checkpoints/checkpoint_epoch_50_20241029-040906.pth
Epoch 51/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 81.48it/s, loss=0.6388, lr=9.06e-06, grad=M:1.2897, status=Epoch complete]
█ Epoch  51: Loss T 0.638817 V 0.771545 | F1 T 0.713860 V 0.672359 B 0.679761 SC 4/10 | Acc T 0.736435 V 0.672382 | LR 9.06e-06 | Grad ↓ 0.053056 ↑ 5.248724 M 1.289674 | 4s
Epoch 52/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 81.52it/s, loss=0.6374, lr=6.59e-06, grad=M:0.9475, status=Epoch complete]
█ Epoch  52: Loss T 0.637373 V 0.767173 | F1 T 0.714996 V 0.675161 B 0.679761 SC 5/10 | Acc T 0.736749 V 0.676438 | LR 6.59e-06 | Grad ↓ 0.026550 ↑ 3.518653 M 0.947512 | 4s
Epoch 53/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 79.17it/s, loss=0.6361, lr=3.53e-06, grad=M:1.1242, status=Epoch complete]
█ Epoch  53: Loss T 0.636053 V 0.778258 | F1 T 0.711791 V 0.667943 B 0.679761 SC 6/10 | Acc T 0.736504 V 0.668510 | LR 3.53e-06 | Grad ↓ 0.047838 ↑ 4.511461 M 1.124167 | 5s
Epoch 54/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 78.91it/s, loss=0.6346, lr=1.05e-06, grad=M:0.9246, status=Epoch complete]
█ Epoch  54: Loss T 0.634618 V 0.760369 | F1 T 0.715754 V 0.675982 B 0.679761 SC 7/10 | Acc T 0.737111 V 0.675885 | LR 1.05e-06 | Grad ↓ 0.024308 ↑ 3.315652 M 0.924636 | 5s
Epoch 55/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 78.45it/s, loss=0.6334, lr=1.00e-07, grad=M:0.4901, status=Epoch complete]
█ Epoch  55: Loss T 0.633388 V 0.764588 | F1 T 0.715409 V 0.674686 B 0.679761 SC 8/10 | Acc T 0.737444 V 0.674963 | LR 1.00e-07 | Grad ↓ 0.019908 ↑ 1.596539 M 0.490121 | 5s
Saved model state: checkpoints/checkpoint_epoch_55_20241029-040931.pth
Epoch 56/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:05<00:00, 79.02it/s, loss=0.6368, lr=9.06e-06, grad=M:0.7102, status=Epoch complete]
█ Epoch  56: Loss T 0.636849 V 0.763407 | F1 T 0.715618 V 0.675160 B 0.679761 SC 9/10 | Acc T 0.737444 V 0.675332 | LR 9.06e-06 | Grad ↓ 0.026104 ↑ 2.435028 M 0.710178 | 5s
Epoch 57/100: 100%|████████████████████████████████████████████████████████████████████| 399/399 [00:04<00:00, 85.83it/s, loss=0.6358, lr=6.59e-06, grad=M:0.5513, status=Epoch complete]
█ Epoch  57: Loss T 0.635848 V 0.758063 | F1 T 0.716221 V 0.676665 B 0.679761 SC 10/10 | Acc T 0.737287 V 0.677729 | LR 6.59e-06 | Grad ↓ 0.020663 ↑ 1.877592 M 0.551307 | 4s
Stopping early after 57 epochs due to no improvement in validation score in last 10 iterations.
Saving model in ONNX format...
Model saved in ONNX format to saves/model_20241029-040942.onnx and uploaded to Weights & Biases.
Saved model state: checkpoints/final_model_20241029-040942.pth
Saved model pickle: checkpoints/final_model_20241029-040942.pkl
Training completed (4m 51s)

Evaluating model...

B1-EB-Merged Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.756857  0.610119  0.675612      2352
     neutral   0.585344  0.716238  0.644209      1829
    positive   0.734558  0.749255  0.741834      2349

    accuracy                       0.689893      6530
   macro avg   0.692253  0.691871  0.687218      6530
weighted avg   0.700796  0.689893  0.690638      6530

ROC AUC: 0.863547

Predicted  negative  neutral  positive
Actual                                
negative       1435      537       380
neutral         263     1310       256
positive        198      391      1760

Saved predictions: saves/predictions_20241029-040943.csv

Macro F1 Score: 0.69

Evaluation completed (889ms)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            eval/macro_f1_score ▁
wandb:             gradients/max_norm ▁▂▁▁▁▃▁▃▃▂▃▆▂▄▇▆▃▂▄▃▄▅▅▅▄▃▄▂▃█▂▃▅▄▃▄▅▆▅▃
wandb:            gradients/mean_norm ▁▁▂▂▂▄▂▄▅▃▃▃▇▃▅█▄▃▃▄▇▅▅▆▆█▃▃▃▅▄▅▄▄▅▇▇▅▃▄
wandb:             gradients/min_norm ▁▂▂▁▂▄▂▂▂▅▃▂▄▂▄█▄▄▆▃▁▆▄▇▆█▃▂▂█▂▃▄▄▃▃▆▄▃▃
wandb:                    other/epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██
wandb:            other/learning_rate █▆▄▂▁▆▄▂▁█▄▂▁▆▄▁█▆▄▁▆▄▂█▆█▆▄▂█▄▂▆▄▂█▆▄▁▆
wandb:               other/stop_limit ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 train/accuracy ▁▄▄▅▅▆▆▇▇▇██████████████████████████████
wandb:                     train/loss █▆▅▅▅▄▄▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:           train/macro_f1_score ▁▄▄▄▄▅▆▇▇███████████████████████████████
wandb:               train/stop_count ▁▁▁▁▂▁▁▁▁▁▁▁▂▂▃▅▁▂▂▃▁▂▁▂▂▃▄▁▂▂▂▁▁▂▂▄▅▅▇█
wandb:            validation/accuracy ▁▃▄▄▄▅▅▆▇▇██████████████████████████████
wandb: validation/best_macro_f1_score ▁▃▄▄▄▄▅▆▆███████████████████████████████
wandb:                validation/loss █▇▆▆▆▅▅▄▄▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      validation/macro_f1_score ▁▃▄▄▄▄▆▆▇███████████████████████████████
wandb: 
wandb: Run summary:
wandb:            eval/macro_f1_score 0.68722
wandb:             gradients/max_norm 1.87759
wandb:            gradients/mean_norm 0.55131
wandb:             gradients/min_norm 0.02066
wandb:                    other/epoch 57
wandb:            other/learning_rate 1e-05
wandb:               other/stop_limit 10
wandb:                 train/accuracy 0.73729
wandb:                     train/loss 0.63585
wandb:           train/macro_f1_score 0.71622
wandb:               train/stop_count 10
wandb:            validation/accuracy 0.67773
wandb: validation/best_macro_f1_score 0.67976
wandb:                validation/loss 0.75806
wandb:      validation/macro_f1_score 0.67667
wandb: 
wandb: 🚀 View run B1-EB-Merged at: https://wandb.ai/jimbeno/Electra%20Base/runs/k6b9mvmn
wandb: ⭐️ View project at: https://wandb.ai/jimbeno/Electra%20Base
wandb: Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20241029_040221-k6b9mvmn/logs
TOTAL Time: 7m 35s
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --weights_name 'google/electra-base-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --epochs 149 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 32 --l2_strength 0.01  --checkpoint_interval 5 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --wandb --wandb_project 'Electra Base' --wandb_run 'B1-EB-Merged' --data_file 'saves/data_8_gpu_20241029-040436.npz' --model_file 'checkpoint_epoch_50_20241029-040906.pth' --interactive

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 2 - Device: cuda:2
Rank 7 - Device: cuda:7
Rank 5 - Device: cuda:5
Rank 6 - Device: cuda:6
Rank 3 - Device: cuda:3
Rank 4 - Device: cuda:4
Rank 1 - Device: cuda:1
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /home/ubuntu/nlp-test/sentiment/wandb/run-20241029_041426-xi7gddwg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run B1-EB-Merged
wandb: ⭐️ View project at https://wandb.ai/jimbeno/Electra%20Base
wandb: 🚀 View run at https://wandb.ai/jimbeno/Electra%20Base/runs/xi7gddwg
Wand run initialized.

Initializing 'google/electra-base-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (3s)

Loading archived data from: saves/data_8_gpu_20241029-040436.npz...
X Train shape: [102097, 768], y Train shape: [102097]
X Validation shape: [5421, 768], y Validation shape: [5421]
X Test shape: [6530, 768], y Dev shape: [6530]
X Test Sentences shape: [6530]
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Test label distribution:
	      Negative: 2352
	       Neutral: 1829
	      Positive: 2349
Archived data loaded (3.41s)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 1, Max Grad Norm: None
Batch Size: 32, Max Epochs: 149, LR: 1e-05, Early Stop: score, Fine-tune BERT: False, Fine-tune Layers: 1, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_50_20241029-040906.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_50_20241029-040906.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (279ms)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 10 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Classes: ['negative', 'neutral', 'positive'], Number of classes: 3
Initializing model, graph, optimizer...

Model Architecture Summary:
Classifier(
  (layers): Sequential(
    (0): Linear(in_features=768, out_features=1024, bias=True)
    (1): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=1024, out_features=1024, bias=True)
    (4): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (5): Dropout(p=0.3, inplace=False)
    (6): Linear(in_features=1024, out_features=3, bias=True)
  )
)

Model Parameters:
  layers.0: 787,456 (trainable)
  layers.1.projection: 2,099,200 (trainable)
  layers.3: 1,049,600 (trainable)
  layers.6: 3,075 (trainable)

Total layers: 4
Trainable layers: 4
Total parameters: 3,939,331
Trainable parameters: 3,939,331
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 51, Max Iterations: 149, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 10

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: h

Choose from the following commands:
[Enter]            Continue to the next epoch, for the next prompt.
[#]                Skip the specified number of epochs before the next prompt. Example: '5'
[A]dvance    ____  Set the number of epochs to skip. Current: 1. Example: 'a 5'
[C]heckpoint ____  Set the checkpoint interval. Current: 5. Example: 'c 5'
[D]ebug            Toggle debug mode. Current: False
[E]pochs     ____  Set the maximum number of epochs. Current: 149. Example: 'e 1000'
[G]PUs       ____  Show current GPU memory usage.
[H]elp             Display this help message
[L]earning   ____  Set the learning rate. Current: 0.000010. Example: 'l 0.001'
[M]emory     ____  Set the interval to display GPU memory usage. Example: 'm 5'
[N]umber     ____  Set the number of iterations no change. Current: 10. Example: 'n 10'
[P]rogress         Toggle progress bar display. Current: False
[Q]uit             Quit the training loop
[S]ave             Save the model as a checkpoint with DDP wrapper and optionally as a pickle file.
[T]olerance  ____  Set the tolerance for early stopping. Current: 1e-05. Example: 't 1e-03'
[V]al Score  ____  Set the target validation score for early stopping. Current: None. Example: 'v 0.95'
[X]it              Exit interactive mode. You won't be prompted for input again.

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: c 1
Updated checkpoint interval to: 1.0

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: n 20
Updated number of iterations no change to: 20

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: 49
Skipping 49 epochs.
█ Epoch  51: Loss T 0.638534 V 0.773408 | F1 T 0.713164 V 0.672176 B 0.672176 SC 0/20 | Acc T 0.736249 V 0.672198 | LR 9.06e-06 | Grad ↓ 0.049246 ↑ 4.594142 M 1.126577 | 56s
Saved model state: checkpoints/checkpoint_epoch_51_20241029-041533.pth
█ Epoch  52: Loss T 0.637286 V 0.771042 | F1 T 0.714096 V 0.673239 B 0.673239 SC 0/20 | Acc T 0.736680 V 0.674594 | LR 6.59e-06 | Grad ↓ 0.026425 ↑ 3.744754 M 1.000866 | 4s
Saved model state: checkpoints/checkpoint_epoch_52_20241029-041537.pth
█ Epoch  53: Loss T 0.636457 V 0.776917 | F1 T 0.712503 V 0.668941 B 0.673239 SC 1/20 | Acc T 0.736925 V 0.669617 | LR 3.53e-06 | Grad ↓ 0.043231 ↑ 4.126585 M 1.021062 | 4s
Saved model state: checkpoints/checkpoint_epoch_53_20241029-041542.pth
█ Epoch  54: Loss T 0.634245 V 0.758556 | F1 T 0.716014 V 0.677807 B 0.677807 SC 0/20 | Acc T 0.736906 V 0.677729 | LR 1.05e-06 | Grad ↓ 0.024552 ↑ 2.235610 M 0.613187 | 4s
Saved model state: checkpoints/checkpoint_epoch_54_20241029-041547.pth
█ Epoch  55: Loss T 0.633816 V 0.764688 | F1 T 0.715275 V 0.674914 B 0.677807 SC 1/20 | Acc T 0.737336 V 0.675147 | LR 1.00e-07 | Grad ↓ 0.024278 ↑ 2.191035 M 0.624648 | 4s
Saved model state: checkpoints/checkpoint_epoch_55_20241029-041551.pth
█ Epoch  56: Loss T 0.637177 V 0.762914 | F1 T 0.715662 V 0.674868 B 0.677807 SC 2/20 | Acc T 0.737405 V 0.674963 | LR 9.06e-06 | Grad ↓ 0.024322 ↑ 1.954643 M 0.572483 | 4s
Saved model state: checkpoints/checkpoint_epoch_56_20241029-041556.pth
█ Epoch  57: Loss T 0.635211 V 0.756572 | F1 T 0.716902 V 0.677568 B 0.677807 SC 3/20 | Acc T 0.737552 V 0.678282 | LR 6.59e-06 | Grad ↓ 0.015065 ↑ 1.714256 M 0.456264 | 4s
Saved model state: checkpoints/checkpoint_epoch_57_20241029-041601.pth
█ Epoch  58: Loss T 0.633676 V 0.776606 | F1 T 0.712945 V 0.668931 B 0.677807 SC 4/20 | Acc T 0.737356 V 0.669617 | LR 3.53e-06 | Grad ↓ 0.037970 ↑ 4.073885 M 1.173492 | 4s
Saved model state: checkpoints/checkpoint_epoch_58_20241029-041605.pth
█ Epoch  59: Loss T 0.632056 V 0.765569 | F1 T 0.715813 V 0.673628 B 0.677807 SC 5/20 | Acc T 0.738120 V 0.674410 | LR 1.05e-06 | Grad ↓ 0.044425 ↑ 4.384980 M 1.151606 | 4s
Saved model state: checkpoints/checkpoint_epoch_59_20241029-041610.pth
█ Epoch  60: Loss T 0.630815 V 0.762681 | F1 T 0.716466 V 0.674545 B 0.677807 SC 6/20 | Acc T 0.738326 V 0.674963 | LR 1.00e-07 | Grad ↓ 0.020622 ↑ 2.210851 M 0.635635 | 4s
Memory: Rank 0: 2.21 GB | Rank 1: 2.23 GB | Rank 2: 2.13 GB | Rank 3: 2.13 GB | Rank 4: 2.21 GB | Rank 5: 2.23 GB | Rank 6: 2.18 GB | Rank 7: 2.15 GB (Max: 16.93 GB, Total: 135.43 GB)
Saved model state: checkpoints/checkpoint_epoch_60_20241029-041616.pth
█ Epoch  61: Loss T 0.634527 V 0.779027 | F1 T 0.710215 V 0.665425 B 0.677807 SC 7/20 | Acc T 0.735417 V 0.665007 | LR 9.06e-06 | Grad ↓ 0.034936 ↑ 4.192071 M 1.151427 | 4s
Saved model state: checkpoints/checkpoint_epoch_61_20241029-041621.pth
█ Epoch  62: Loss T 0.634165 V 0.762721 | F1 T 0.713282 V 0.674194 B 0.677807 SC 8/20 | Acc T 0.735936 V 0.673488 | LR 6.59e-06 | Grad ↓ 0.016250 ↑ 1.557367 M 0.436947 | 4s
Saved model state: checkpoints/checkpoint_epoch_62_20241029-041625.pth
█ Epoch  63: Loss T 0.632018 V 0.763879 | F1 T 0.716641 V 0.673621 B 0.677807 SC 9/20 | Acc T 0.738815 V 0.674594 | LR 3.53e-06 | Grad ↓ 0.014822 ↑ 2.126772 M 0.550156 | 4s
Saved model state: checkpoints/checkpoint_epoch_63_20241029-041630.pth
█ Epoch  64: Loss T 0.630765 V 0.763270 | F1 T 0.716013 V 0.672316 B 0.677807 SC 10/20 | Acc T 0.738414 V 0.672751 | LR 1.05e-06 | Grad ↓ 0.042417 ↑ 4.513175 M 1.178620 | 4s
Saved model state: checkpoints/checkpoint_epoch_64_20241029-041635.pth
█ Epoch  65: Loss T 0.629500 V 0.761576 | F1 T 0.716809 V 0.673730 B 0.677807 SC 11/20 | Acc T 0.738806 V 0.674041 | LR 1.00e-07 | Grad ↓ 0.020557 ↑ 2.021111 M 0.560736 | 4s
Saved model state: checkpoints/checkpoint_epoch_65_20241029-041639.pth
█ Epoch  66: Loss T 0.632481 V 0.764368 | F1 T 0.715784 V 0.672650 B 0.677807 SC 12/20 | Acc T 0.738570 V 0.672935 | LR 9.06e-06 | Grad ↓ 0.027535 ↑ 2.920752 M 0.710697 | 4s
Saved model state: checkpoints/checkpoint_epoch_66_20241029-041644.pth
█ Epoch  67: Loss T 0.631329 V 0.751805 | F1 T 0.718754 V 0.681177 B 0.681177 SC 0/20 | Acc T 0.738825 V 0.681600 | LR 6.59e-06 | Grad ↓ 0.030047 ↑ 3.232561 M 0.891154 | 4s
Saved model state: checkpoints/checkpoint_epoch_67_20241029-041649.pth
█ Epoch  68: Loss T 0.630234 V 0.763800 | F1 T 0.716524 V 0.672590 B 0.681177 SC 1/20 | Acc T 0.739080 V 0.673304 | LR 3.53e-06 | Grad ↓ 0.016658 ↑ 1.589427 M 0.460971 | 4s
Saved model state: checkpoints/checkpoint_epoch_68_20241029-041653.pth
█ Epoch  69: Loss T 0.628782 V 0.766979 | F1 T 0.715477 V 0.670769 B 0.681177 SC 2/20 | Acc T 0.738747 V 0.671460 | LR 1.05e-06 | Grad ↓ 0.018882 ↑ 2.188100 M 0.617893 | 4s
Saved model state: checkpoints/checkpoint_epoch_69_20241029-041658.pth
█ Epoch  70: Loss T 0.627876 V 0.758137 | F1 T 0.717466 V 0.676051 B 0.681177 SC 3/20 | Acc T 0.739041 V 0.676438 | LR 1.00e-07 | Grad ↓ 0.012307 ↑ 2.004897 M 0.554430 | 4s
Memory: Rank 0: 2.21 GB | Rank 1: 2.23 GB | Rank 2: 2.13 GB | Rank 3: 2.13 GB | Rank 4: 2.21 GB | Rank 5: 2.23 GB | Rank 6: 2.18 GB | Rank 7: 2.15 GB (Max: 16.93 GB, Total: 135.43 GB)
Saved model state: checkpoints/checkpoint_epoch_70_20241029-041703.pth
█ Epoch  71: Loss T 0.631707 V 0.749765 | F1 T 0.717757 V 0.679389 B 0.681177 SC 4/20 | Acc T 0.738502 V 0.679204 | LR 9.06e-06 | Grad ↓ 0.014979 ↑ 1.546265 M 0.431192 | 4s
Saved model state: checkpoints/checkpoint_epoch_71_20241029-041708.pth
█ Epoch  72: Loss T 0.629252 V 0.750393 | F1 T 0.719391 V 0.681284 B 0.681284 SC 0/20 | Acc T 0.739217 V 0.681785 | LR 6.59e-06 | Grad ↓ 0.007254 ↑ 2.096627 M 0.572907 | 4s
Saved model state: checkpoints/checkpoint_epoch_72_20241029-041713.pth
█ Epoch  73: Loss T 0.628909 V 0.752591 | F1 T 0.718828 V 0.677931 B 0.681284 SC 1/20 | Acc T 0.739677 V 0.678466 | LR 3.53e-06 | Grad ↓ 0.020402 ↑ 1.706744 M 0.507257 | 4s
Saved model state: checkpoints/checkpoint_epoch_73_20241029-041717.pth
█ Epoch  74: Loss T 0.627069 V 0.751854 | F1 T 0.718902 V 0.679756 B 0.681284 SC 2/20 | Acc T 0.739785 V 0.680125 | LR 1.05e-06 | Grad ↓ 0.040883 ↑ 4.588170 M 1.225868 | 4s
Saved model state: checkpoints/checkpoint_epoch_74_20241029-041722.pth
█ Epoch  75: Loss T 0.625806 V 0.759330 | F1 T 0.717751 V 0.674076 B 0.681284 SC 3/20 | Acc T 0.739648 V 0.674594 | LR 1.00e-07 | Grad ↓ 0.033817 ↑ 3.957869 M 0.945187 | 4s
Saved model state: checkpoints/checkpoint_epoch_75_20241029-041727.pth
█ Epoch  76: Loss T 0.629801 V 0.749567 | F1 T 0.718770 V 0.679856 B 0.681284 SC 4/20 | Acc T 0.739432 V 0.679757 | LR 9.06e-06 | Grad ↓ 0.020727 ↑ 3.039528 M 0.850274 | 4s
Saved model state: checkpoints/checkpoint_epoch_76_20241029-041732.pth
█ Epoch  77: Loss T 0.627752 V 0.743571 | F1 T 0.719218 V 0.684910 B 0.684910 SC 0/20 | Acc T 0.738619 V 0.684919 | LR 6.59e-06 | Grad ↓ 0.019499 ↑ 2.620182 M 0.724254 | 4s
Saved model state: checkpoints/checkpoint_epoch_77_20241029-041736.pth
█ Epoch  78: Loss T 0.626995 V 0.745092 | F1 T 0.719857 V 0.683386 B 0.684910 SC 1/20 | Acc T 0.739207 V 0.683813 | LR 3.53e-06 | Grad ↓ 0.046466 ↑ 5.737352 M 1.481377 | 4s
Saved model state: checkpoints/checkpoint_epoch_78_20241029-041741.pth
█ Epoch  79: Loss T 0.625673 V 0.754526 | F1 T 0.718363 V 0.678136 B 0.684910 SC 2/20 | Acc T 0.739795 V 0.678097 | LR 1.05e-06 | Grad ↓ 0.017323 ↑ 2.995509 M 0.777292 | 4s
Saved model state: checkpoints/checkpoint_epoch_79_20241029-041746.pth
█ Epoch  80: Loss T 0.625195 V 0.760025 | F1 T 0.717638 V 0.674263 B 0.684910 SC 3/20 | Acc T 0.740000 V 0.674779 | LR 1.00e-07 | Grad ↓ 0.021311 ↑ 2.117629 M 0.548613 | 4s
Memory: Rank 0: 2.21 GB | Rank 1: 2.23 GB | Rank 2: 2.13 GB | Rank 3: 2.13 GB | Rank 4: 2.21 GB | Rank 5: 2.23 GB | Rank 6: 2.18 GB | Rank 7: 2.15 GB (Max: 16.93 GB, Total: 135.43 GB)
Saved model state: checkpoints/checkpoint_epoch_80_20241029-041751.pth
█ Epoch  81: Loss T 0.627324 V 0.773010 | F1 T 0.714280 V 0.664521 B 0.684910 SC 4/20 | Acc T 0.739011 V 0.666851 | LR 9.06e-06 | Grad ↓ 0.029697 ↑ 4.254677 M 1.107888 | 4s
Saved model state: checkpoints/checkpoint_epoch_81_20241029-041756.pth
█ Epoch  82: Loss T 0.627140 V 0.752815 | F1 T 0.719304 V 0.677952 B 0.684910 SC 5/20 | Acc T 0.740480 V 0.678466 | LR 6.59e-06 | Grad ↓ 0.016075 ↑ 2.242058 M 0.621246 | 4s
Saved model state: checkpoints/checkpoint_epoch_82_20241029-041800.pth
█ Epoch  83: Loss T 0.625514 V 0.754210 | F1 T 0.718872 V 0.675667 B 0.684910 SC 6/20 | Acc T 0.740617 V 0.676069 | LR 3.53e-06 | Grad ↓ 0.038169 ↑ 4.147260 M 1.130074 | 4s
Saved model state: checkpoints/checkpoint_epoch_83_20241029-041805.pth
█ Epoch  84: Loss T 0.624327 V 0.757954 | F1 T 0.718593 V 0.675052 B 0.684910 SC 7/20 | Acc T 0.740647 V 0.675516 | LR 1.05e-06 | Grad ↓ 0.045509 ↑ 5.730569 M 1.474803 | 4s
Saved model state: checkpoints/checkpoint_epoch_84_20241029-041810.pth
█ Epoch  85: Loss T 0.622928 V 0.757144 | F1 T 0.718539 V 0.675101 B 0.684910 SC 8/20 | Acc T 0.740559 V 0.675516 | LR 1.00e-07 | Grad ↓ 0.028434 ↑ 4.278016 M 1.099587 | 4s
Saved model state: checkpoints/checkpoint_epoch_85_20241029-041815.pth
█ Epoch  86: Loss T 0.626714 V 0.758594 | F1 T 0.719495 V 0.679413 B 0.684910 SC 9/20 | Acc T 0.740030 V 0.680678 | LR 9.06e-06 | Grad ↓ 0.027036 ↑ 3.868141 M 1.008896 | 4s
Saved model state: checkpoints/checkpoint_epoch_86_20241029-041819.pth
█ Epoch  87: Loss T 0.625738 V 0.753784 | F1 T 0.718798 V 0.677045 B 0.684910 SC 10/20 | Acc T 0.740578 V 0.678282 | LR 6.59e-06 | Grad ↓ 0.030812 ↑ 3.666895 M 0.954205 | 4s
Saved model state: checkpoints/checkpoint_epoch_87_20241029-041824.pth
█ Epoch  88: Loss T 0.624119 V 0.751113 | F1 T 0.719345 V 0.677065 B 0.684910 SC 11/20 | Acc T 0.740657 V 0.677544 | LR 3.53e-06 | Grad ↓ 0.011037 ↑ 1.737011 M 0.498202 | 4s
Saved model state: checkpoints/checkpoint_epoch_88_20241029-041829.pth
█ Epoch  89: Loss T 0.622865 V 0.758047 | F1 T 0.718763 V 0.675258 B 0.684910 SC 12/20 | Acc T 0.740911 V 0.676069 | LR 1.05e-06 | Grad ↓ 0.018582 ↑ 2.808495 M 0.741163 | 4s
Saved model state: checkpoints/checkpoint_epoch_89_20241029-041833.pth
█ Epoch  90: Loss T 0.621465 V 0.756539 | F1 T 0.718905 V 0.675747 B 0.684910 SC 13/20 | Acc T 0.740882 V 0.676438 | LR 1.00e-07 | Grad ↓ 0.011983 ↑ 1.397751 M 0.402904 | 4s
Memory: Rank 0: 2.21 GB | Rank 1: 2.23 GB | Rank 2: 2.13 GB | Rank 3: 2.13 GB | Rank 4: 2.21 GB | Rank 5: 2.23 GB | Rank 6: 2.18 GB | Rank 7: 2.15 GB (Max: 16.93 GB, Total: 135.43 GB)
Saved model state: checkpoints/checkpoint_epoch_90_20241029-041838.pth
█ Epoch  91: Loss T 0.625983 V 0.769646 | F1 T 0.716261 V 0.670700 B 0.684910 SC 14/20 | Acc T 0.740020 V 0.670907 | LR 9.06e-06 | Grad ↓ 0.046406 ↑ 6.201341 M 1.575799 | 4s
Saved model state: checkpoints/checkpoint_epoch_91_20241029-041842.pth
█ Epoch  92: Loss T 0.624966 V 0.764319 | F1 T 0.716713 V 0.673655 B 0.684910 SC 15/20 | Acc T 0.739971 V 0.674963 | LR 6.59e-06 | Grad ↓ 0.023160 ↑ 3.240058 M 0.857800 | 4s
Saved model state: checkpoints/checkpoint_epoch_92_20241029-041847.pth
█ Epoch  93: Loss T 0.623045 V 0.760207 | F1 T 0.716943 V 0.671243 B 0.684910 SC 16/20 | Acc T 0.740451 V 0.671645 | LR 3.53e-06 | Grad ↓ 0.017464 ↑ 2.779995 M 0.753452 | 4s
Saved model state: checkpoints/checkpoint_epoch_93_20241029-041852.pth
█ Epoch  94: Loss T 0.621379 V 0.750433 | F1 T 0.719886 V 0.679286 B 0.684910 SC 17/20 | Acc T 0.740833 V 0.679941 | LR 1.05e-06 | Grad ↓ 0.039933 ↑ 5.611817 M 1.379274 | 4s
Saved model state: checkpoints/checkpoint_epoch_94_20241029-041857.pth
█ Epoch  95: Loss T 0.620457 V 0.754440 | F1 T 0.719540 V 0.675498 B 0.684910 SC 18/20 | Acc T 0.741244 V 0.676069 | LR 1.00e-07 | Grad ↓ 0.013917 ↑ 1.941782 M 0.523070 | 4s
Saved model state: checkpoints/checkpoint_epoch_95_20241029-041901.pth
█ Epoch  96: Loss T 0.624287 V 0.750643 | F1 T 0.717537 V 0.679851 B 0.684910 SC 19/20 | Acc T 0.739139 V 0.679388 | LR 9.06e-06 | Grad ↓ 0.017591 ↑ 2.029586 M 0.565750 | 4s
Saved model state: checkpoints/checkpoint_epoch_96_20241029-041906.pth
█ Epoch  97: Loss T 0.623316 V 0.751925 | F1 T 0.719237 V 0.675329 B 0.684910 SC 20/20 | Acc T 0.741117 V 0.675885 | LR 6.59e-06 | Grad ↓ 0.020122 ↑ 3.170462 M 0.814356 | 4s
Stopping early after 97 epochs due to no improvement in validation score in last 20 iterations.
Saved model state: checkpoints/checkpoint_epoch_97_20241029-041911.pth
Saving model in ONNX format...
Model saved in ONNX format to saves/model_20241029-041912.onnx and uploaded to Weights & Biases.
Saved model state: checkpoints/final_model_20241029-041912.pth
Saved model pickle: checkpoints/final_model_20241029-041912.pkl
Training completed (4m 37s)

Evaluating model...

B1-EB-Merged Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.750000  0.621173  0.679535      2352
     neutral   0.577158  0.723893  0.642251      1829
    positive   0.749563  0.730098  0.739702      2349

    accuracy                       0.689127      6530
   macro avg   0.692240  0.691721  0.687163      6530
weighted avg   0.701431  0.689127  0.690736      6530

ROC AUC: 0.866317

Predicted  negative  neutral  positive
Actual                                
negative       1461      558       333
neutral         265     1324       240
positive        222      412      1715

Saved predictions: saves/predictions_20241029-041913.csv

Macro F1 Score: 0.69

Evaluation completed (1s)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            eval/macro_f1_score ▁
wandb:             gradients/max_norm ▆▄▅▂▂▂▅▅▂▅▁▂▂▃▄▁▂▁▂▁▆▅▃▇▃▂▅▂▇▅▄▄▁▃█▄▃▇▂▃
wandb:            gradients/mean_norm ▅▄▅▂▂▂▆▅▂▅▁▂▂▃▄▁▂▁▂▁▆▄▄▇▃▂▅▂▇▅▅▄▁▃█▄▃▇▂▃
wandb:             gradients/min_norm █▄▇▄▄▄▆▇▃▆▂▂▃▄▅▃▃▂▁▃▇▅▃█▃▃▅▂▇▅▄▅▂▃█▄▃▆▂▃
wandb:                    other/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:            other/learning_rate █▆▄▂▁█▄▂▁█▆▄▁█▆▄▂█▆▄▂▁█▄▂▁█▆▂▁█▆▄▂█▆▄▂▁▆
wandb:               other/stop_limit ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 train/accuracy ▂▃▃▃▃▃▃▄▄▁▂▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▅▇▇▇▇▇▇█▇▆▇███
wandb:                     train/loss ██▇▆▆▇▆▅▅▆▆▅▅▆▅▅▄▅▄▄▄▃▅▄▃▃▄▄▂▂▃▃▂▂▃▃▂▁▁▂
wandb:           train/macro_f1_score ▃▄▃▅▅▅▃▅▆▁▃▆▆▅▇▆▅▆█▇▇▆▇█▇▆▄█▇▇█▇█▇▅▆▆███
wandb:               train/stop_count ▁▁▁▁▁▂▂▃▃▃▄▄▅▅▁▁▂▂▁▁▂▂▂▁▂▂▂▃▃▄▄▅▅▅▆▆▇▇▇█
wandb:            validation/accuracy ▄▅▃▆▅▅▃▅▅▁▄▅▄▄▇▄▃▆▇▆▇▅▆█▆▅▂▆▅▅▇▆▆▅▃▅▃▇▅▅
wandb: validation/best_macro_f1_score ▁▂▂▄▄▄▄▄▄▄▄▄▄▄▆▆▆▆▆▆▆▆▆█████████████████
wandb:                validation/loss ▇▆█▄▅▅█▅▅█▅▅▄▅▂▅▆▂▂▃▂▄▂▁▃▄▇▃▄▃▄▃▂▄▆▅▄▂▃▂
wandb:      validation/macro_f1_score ▄▄▃▆▅▅▃▄▅▁▅▄▄▄▇▄▃▇▇▆▇▅▇█▆▅▁▆▅▅▇▆▆▅▃▄▃▆▅▅
wandb: 
wandb: Run summary:
wandb:            eval/macro_f1_score 0.68716
wandb:             gradients/max_norm 3.17046
wandb:            gradients/mean_norm 0.81436
wandb:             gradients/min_norm 0.02012
wandb:                    other/epoch 97
wandb:            other/learning_rate 1e-05
wandb:               other/stop_limit 20
wandb:                 train/accuracy 0.74112
wandb:                     train/loss 0.62332
wandb:           train/macro_f1_score 0.71924
wandb:               train/stop_count 20
wandb:            validation/accuracy 0.67588
wandb: validation/best_macro_f1_score 0.68491
wandb:                validation/loss 0.75193
wandb:      validation/macro_f1_score 0.67533
wandb: 
wandb: 🚀 View run B1-EB-Merged at: https://wandb.ai/jimbeno/Electra%20Base/runs/xi7gddwg
wandb: ⭐️ View project at: https://wandb.ai/jimbeno/Electra%20Base
wandb: Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20241029_041426-xi7gddwg/logs
TOTAL Time: 5m 1s
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --weights_name 'google/electra-base-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --epochs 123 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 32 --l2_strength 0.01  --checkpoint_interval 5 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 10 --wandb --wandb_project 'Electra Base' --wandb_run 'B1-EB-Merged' --data_file 'saves/data_8_gpu_20241029-040436.npz' --model_file 'checkpoint_epoch_77_20241029-041736.pth' --interactive^C
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --weights_name 'google/electra-base-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --epochs 123 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 32 --l2_strength 0.01  --checkpoint_interval 5 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 20 --wandb --wandb_project 'Electra Base' --wandb_run 'B1-EB-Merged' --data_file 'saves/data_8_gpu_20241029-040436.npz' --model_file 'checkpoint_epoch_77_20241029-041736.pth' --interactive
^CTraceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/./ddp_sentiment_finetune.py", line 33, in <module>
    from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1354, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1364, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/__init__.py", line 15, in <module>
    from . import (
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap_external>", line 975, in get_code
  File "<frozen importlib._bootstrap_external>", line 1074, in get_data
KeyboardInterrupt

(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --weights_name 'google/electra-base-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --epochs 123 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 32 --l2_strength 0.01  --checkpoint_interval 5 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 20 --wandb --wandb_project 'Electra Base' --wandb_run 'B1-EB-Merged' --data_file 'saves/data_8_gpu_20241029-040436.npz' --model_file 'checkpoint_epoch_77_20241029-041736.pth' --interactive^C
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --weights_name 'google/electra-base-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --epochs 123 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 32 --l2_strength 0.01  --checkpoint_interval 5 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 20 --wandb --wandb_project 'Electra Base' --wandb_run 'B1-EB-Merged_epoch_77' --data_file 'saves/data_8_gpu_20241029-040436.npz' --model_file 'checkpoint_epoch_77_20241029-041736.pth' --interactive

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 3 - Device: cuda:3
Rank 4 - Device: cuda:4
Rank 5 - Device: cuda:5
Rank 1 - Device: cuda:1
Rank 2 - Device: cuda:2
Rank 6 - Device: cuda:6
Rank 7 - Device: cuda:7
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /home/ubuntu/nlp-test/sentiment/wandb/run-20241029_042341-zvcpwuso
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run B1-EB-Merged_epoch_77
wandb: ⭐️ View project at https://wandb.ai/jimbeno/Electra%20Base
wandb: 🚀 View run at https://wandb.ai/jimbeno/Electra%20Base/runs/zvcpwuso
Wand run initialized.

Initializing 'google/electra-base-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (3s)

Loading archived data from: saves/data_8_gpu_20241029-040436.npz...
X Train shape: [102097, 768], y Train shape: [102097]
X Validation shape: [5421, 768], y Validation shape: [5421]
X Test shape: [6530, 768], y Dev shape: [6530]
X Test Sentences shape: [6530]
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Test label distribution:
	      Negative: 2352
	       Neutral: 1829
	      Positive: 2349
Archived data loaded (2.55s)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 1, Max Grad Norm: None
Batch Size: 32, Max Epochs: 123, LR: 1e-05, Early Stop: score, Fine-tune BERT: False, Fine-tune Layers: 1, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_77_20241029-041736.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_77_20241029-041736.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (259ms)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 20 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Classes: ['negative', 'neutral', 'positive'], Number of classes: 3
Initializing model, graph, optimizer...

Model Architecture Summary:
Classifier(
  (layers): Sequential(
    (0): Linear(in_features=768, out_features=1024, bias=True)
    (1): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=1024, out_features=1024, bias=True)
    (4): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (5): Dropout(p=0.3, inplace=False)
    (6): Linear(in_features=1024, out_features=3, bias=True)
  )
)

Model Parameters:
  layers.0: 787,456 (trainable)
  layers.1.projection: 2,099,200 (trainable)
  layers.3: 1,049,600 (trainable)
  layers.6: 3,075 (trainable)

Total layers: 4
Trainable layers: 4
Total parameters: 3,939,331
Trainable parameters: 3,939,331
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 78, Max Iterations: 123, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 20

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: q
Quitting training...
Saving model in ONNX format...
Model saved in ONNX format to saves/model_20241029-042355.onnx and uploaded to Weights & Biases.
Saved model state: checkpoints/final_model_20241029-042355.pth
Saved model pickle: checkpoints/final_model_20241029-042355.pkl
Training completed (7s)

Evaluating model...

B1-EB-Merged_epoch_77 Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.743172  0.659439  0.698806      2352
     neutral   0.592443  0.720066  0.650049      1829
    positive   0.762162  0.720307  0.740643      2349

    accuracy                       0.698315      6530
   macro avg   0.699259  0.699937  0.696500      6530
weighted avg   0.707785  0.698315  0.700200      6530

ROC AUC: 0.865288

Predicted  negative  neutral  positive
Actual                                
negative       1551      503       298
neutral         282     1317       230
positive        254      403      1692

Saved predictions: saves/predictions_20241029-042355.csv

Macro F1 Score: 0.70

Evaluation completed (877ms)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: eval/macro_f1_score ▁
wandb: 
wandb: Run summary:
wandb: eval/macro_f1_score 0.6965
wandb: 
wandb: 🚀 View run B1-EB-Merged_epoch_77 at: https://wandb.ai/jimbeno/Electra%20Base/runs/zvcpwuso
wandb: ⭐️ View project at: https://wandb.ai/jimbeno/Electra%20Base
wandb: Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20241029_042341-zvcpwuso/logs
TOTAL Time: 30s
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --weights_name 'google/electra-base-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --epochs 103 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 32 --l2_strength 0.01  --checkpoint_interval 5 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 20 --wandb --wandb_project 'Electra Base' --wandb_run 'B1-EB-Merged' --data_file 'saves/data_8_gpu_20241029-040436.npz' --model_file 'checkpoint_epoch_97_20241029-041911.pth' --interactive

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 2 - Device: cuda:2
Rank 5 - Device: cuda:5
Rank 7 - Device: cuda:7
Rank 6 - Device: cuda:6
Rank 3 - Device: cuda:3
Rank 4 - Device: cuda:4
Rank 1 - Device: cuda:1
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /home/ubuntu/nlp-test/sentiment/wandb/run-20241029_043031-rlvdiw7r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run B1-EB-Merged
wandb: ⭐️ View project at https://wandb.ai/jimbeno/Electra%20Base
wandb: 🚀 View run at https://wandb.ai/jimbeno/Electra%20Base/runs/rlvdiw7r
Wand run initialized.

Initializing 'google/electra-base-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (3s)

Loading archived data from: saves/data_8_gpu_20241029-040436.npz...
X Train shape: [102097, 768], y Train shape: [102097]
X Validation shape: [5421, 768], y Validation shape: [5421]
X Test shape: [6530, 768], y Dev shape: [6530]
X Test Sentences shape: [6530]
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Test label distribution:
	      Negative: 2352
	       Neutral: 1829
	      Positive: 2349
Archived data loaded (2.53s)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 1, Max Grad Norm: None
Batch Size: 32, Max Epochs: 103, LR: 1e-05, Early Stop: score, Fine-tune BERT: False, Fine-tune Layers: 1, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_97_20241029-041911.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_97_20241029-041911.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (255ms)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 20 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Classes: ['negative', 'neutral', 'positive'], Number of classes: 3
Initializing model, graph, optimizer...

Model Architecture Summary:
Classifier(
  (layers): Sequential(
    (0): Linear(in_features=768, out_features=1024, bias=True)
    (1): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=1024, out_features=1024, bias=True)
    (4): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (5): Dropout(p=0.3, inplace=False)
    (6): Linear(in_features=1024, out_features=3, bias=True)
  )
)

Model Parameters:
  layers.0: 787,456 (trainable)
  layers.1.projection: 2,099,200 (trainable)
  layers.3: 1,049,600 (trainable)
  layers.6: 3,075 (trainable)

Total layers: 4
Trainable layers: 4
Total parameters: 3,939,331
Trainable parameters: 3,939,331
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 98, Max Iterations: 103, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 20

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: 
Skipping 1 epochs.
█ Epoch  98: Loss T 0.622485 V 0.748727 | F1 T 0.720627 V 0.681779 B 0.681779 SC 0/20 | Acc T 0.741352 V 0.681969 | LR 3.53e-06 | Grad ↓ 0.004193 ↑ 1.255385 M 0.364665 | 14s

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: 
Skipping 1 epochs.
█ Epoch  99: Loss T 0.620133 V 0.749063 | F1 T 0.720441 V 0.680955 B 0.681779 SC 1/20 | Acc T 0.741332 V 0.681232 | LR 1.05e-06 | Grad ↓ 0.026346 ↑ 3.925904 M 1.034274 | 6s

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: 
Skipping 1 epochs.
█ Epoch 100: Loss T 0.619420 V 0.752922 | F1 T 0.719995 V 0.676520 B 0.681779 SC 2/20 | Acc T 0.741616 V 0.676991 | LR 1.00e-07 | Grad ↓ 0.022482 ↑ 3.125573 M 0.795877 | 5s
Memory: Rank 0: 2.21 GB | Rank 1: 2.23 GB | Rank 2: 2.13 GB | Rank 3: 2.13 GB | Rank 4: 2.21 GB | Rank 5: 2.23 GB | Rank 6: 2.18 GB | Rank 7: 2.15 GB (Max: 16.93 GB, Total: 135.43 GB)
Saved model state: checkpoints/checkpoint_epoch_100_20241029-043109.pth

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: h

Choose from the following commands:
[Enter]            Continue to the next epoch, for the next prompt.
[#]                Skip the specified number of epochs before the next prompt. Example: '5'
[A]dvance    ____  Set the number of epochs to skip. Current: 1. Example: 'a 5'
[C]heckpoint ____  Set the checkpoint interval. Current: 5. Example: 'c 5'
[D]ebug            Toggle debug mode. Current: False
[E]pochs     ____  Set the maximum number of epochs. Current: 103. Example: 'e 1000'
[G]PUs       ____  Show current GPU memory usage.
[H]elp             Display this help message
[L]earning   ____  Set the learning rate. Current: 0.000010. Example: 'l 0.001'
[M]emory     ____  Set the interval to display GPU memory usage. Example: 'm 5'
[N]umber     ____  Set the number of iterations no change. Current: 20. Example: 'n 10'
[P]rogress         Toggle progress bar display. Current: False
[Q]uit             Quit the training loop
[S]ave             Save the model as a checkpoint with DDP wrapper and optionally as a pickle file.
[T]olerance  ____  Set the tolerance for early stopping. Current: 1e-05. Example: 't 1e-03'
[V]al Score  ____  Set the target validation score for early stopping. Current: None. Example: 'v 0.95'
[X]it              Exit interactive mode. You won't be prompted for input again.

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: c 1
Updated checkpoint interval to: 1.0

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: 5 
Skipping 5 epochs.
█ Epoch 101: Loss T 0.622395 V 0.770652 | F1 T 0.713744 V 0.667509 B 0.681779 SC 3/20 | Acc T 0.738894 V 0.667404 | LR 9.06e-06 | Grad ↓ 0.015634 ↑ 2.319847 M 0.566280 | 59s
Saved model state: checkpoints/checkpoint_epoch_101_20241029-043209.pth
█ Epoch 102: Loss T 0.622016 V 0.735433 | F1 T 0.720441 V 0.688030 B 0.688030 SC 0/20 | Acc T 0.738081 V 0.688975 | LR 6.59e-06 | Grad ↓ 0.023969 ↑ 3.075403 M 0.749963 | 4s
Saved model state: checkpoints/checkpoint_epoch_102_20241029-043213.pth
█ Epoch 103: Loss T 0.620488 V 0.753734 | F1 T 0.718851 V 0.674831 B 0.688030 SC 1/20 | Acc T 0.741323 V 0.675147 | LR 3.53e-06 | Grad ↓ 0.025092 ↑ 3.951403 M 0.907493 | 4s
Saved model state: checkpoints/checkpoint_epoch_103_20241029-043218.pth
█ Epoch 104: Loss T 0.618742 V 0.747130 | F1 T 0.720589 V 0.679825 B 0.688030 SC 2/20 | Acc T 0.741616 V 0.680494 | LR 1.05e-06 | Grad ↓ 0.027598 ↑ 3.707573 M 0.960417 | 4s
Saved model state: checkpoints/checkpoint_epoch_104_20241029-043223.pth
█ Epoch 105: Loss T 0.618926 V 0.753807 | F1 T 0.719602 V 0.676388 B 0.688030 SC 3/20 | Acc T 0.741587 V 0.676991 | LR 1.00e-07 | Grad ↓ 0.019956 ↑ 3.755443 M 0.889228 | 4s
Saved model state: checkpoints/checkpoint_epoch_105_20241029-043228.pth

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: a 5
Advance epochs set to: 5.0

[Enter] to continue for 5 epochs, [Q]uit, [S]ave, [H]elp: 
Skipping 5 epochs.
█ Epoch 106: Loss T 0.622650 V 0.755952 | F1 T 0.718564 V 0.674025 B 0.688030 SC 4/20 | Acc T 0.741411 V 0.674041 | LR 9.06e-06 | Grad ↓ 0.024557 ↑ 2.729368 M 0.707063 | 18s
Saved model state: checkpoints/checkpoint_epoch_106_20241029-043247.pth
█ Epoch 107: Loss T 0.621746 V 0.741742 | F1 T 0.721532 V 0.684611 B 0.688030 SC 5/20 | Acc T 0.740657 V 0.684919 | LR 6.59e-06 | Grad ↓ 0.015625 ↑ 2.188322 M 0.599536 | 4s
Saved model state: checkpoints/checkpoint_epoch_107_20241029-043252.pth
█ Epoch 108: Loss T 0.619689 V 0.756580 | F1 T 0.718181 V 0.672366 B 0.688030 SC 6/20 | Acc T 0.741460 V 0.673488 | LR 3.53e-06 | Grad ↓ 0.017242 ↑ 2.126625 M 0.580962 | 4s
Saved model state: checkpoints/checkpoint_epoch_108_20241029-043257.pth
█ Epoch 109: Loss T 0.618164 V 0.752198 | F1 T 0.719645 V 0.674374 B 0.688030 SC 7/20 | Acc T 0.741812 V 0.675147 | LR 1.05e-06 | Grad ↓ 0.025502 ↑ 3.267365 M 0.862597 | 4s
Saved model state: checkpoints/checkpoint_epoch_109_20241029-043302.pth
█ Epoch 110: Loss T 0.617162 V 0.751220 | F1 T 0.720156 V 0.677655 B 0.688030 SC 8/20 | Acc T 0.741793 V 0.678282 | LR 1.00e-07 | Grad ↓ 0.011449 ↑ 1.613213 M 0.457348 | 4s
Memory: Rank 0: 2.21 GB | Rank 1: 2.23 GB | Rank 2: 2.13 GB | Rank 3: 2.13 GB | Rank 4: 2.21 GB | Rank 5: 2.23 GB | Rank 6: 2.18 GB | Rank 7: 2.15 GB (Max: 16.93 GB, Total: 135.43 GB)
Saved model state: checkpoints/checkpoint_epoch_110_20241029-043307.pth

[Enter] to continue for 5 epochs, [Q]uit, [S]ave, [H]elp: q
Quitting training...
Saving model in ONNX format...
Model saved in ONNX format to saves/model_20241029-043318.onnx and uploaded to Weights & Biases.
Saved model state: checkpoints/final_model_20241029-043319.pth
Saved model pickle: checkpoints/final_model_20241029-043319.pkl
Training completed (2m 40s)

Evaluating model...

B1-EB-Merged Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.752049  0.624150  0.682156      2352
     neutral   0.581507  0.725533  0.645585      1829
    positive   0.749129  0.732226  0.740581      2349

    accuracy                       0.691424      6530
   macro avg   0.694229  0.693970  0.689441      6530
weighted avg   0.703231  0.691424  0.692930      6530

ROC AUC: 0.867241

Predicted  negative  neutral  positive
Actual                                
negative       1468      545       339
neutral         265     1327       237
positive        219      410      1720

Saved predictions: saves/predictions_20241029-043319.csv

Macro F1 Score: 0.69

Evaluation completed (929ms)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            eval/macro_f1_score ▁
wandb:             gradients/max_norm ▁█▆▄▆█▇▇▅▃▃▆▂
wandb:            gradients/mean_norm ▁█▆▃▅▇▇▆▅▃▃▆▂
wandb:             gradients/min_norm ▁█▆▄▇▇█▆▇▄▅▇▃
wandb:                    other/epoch ▁▂▂▃▃▄▅▅▆▆▇▇█
wandb:            other/learning_rate ▄▂▁█▆▄▂▁█▆▄▂▁
wandb:               other/stop_limit ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 train/accuracy ▇▇█▃▁▇██▇▆▇██
wandb:                     train/loss █▅▄█▇▅▃▃█▇▄▂▁
wandb:           train/macro_f1_score ▇▇▇▁▇▆▇▆▅█▅▆▇
wandb:               train/stop_count ▁▂▃▄▁▂▃▄▅▅▆▇█
wandb:            validation/accuracy ▆▅▄▁█▄▅▄▃▇▃▄▅
wandb: validation/best_macro_f1_score ▁▁▁▁█████████
wandb:                validation/loss ▄▄▄█▁▅▃▅▅▂▅▄▄
wandb:      validation/macro_f1_score ▆▆▄▁█▃▅▄▃▇▃▃▄
wandb: 
wandb: Run summary:
wandb:            eval/macro_f1_score 0.68944
wandb:             gradients/max_norm 1.61321
wandb:            gradients/mean_norm 0.45735
wandb:             gradients/min_norm 0.01145
wandb:                    other/epoch 110
wandb:            other/learning_rate 0.0
wandb:               other/stop_limit 20
wandb:                 train/accuracy 0.74179
wandb:                     train/loss 0.61716
wandb:           train/macro_f1_score 0.72016
wandb:               train/stop_count 8
wandb:            validation/accuracy 0.67828
wandb: validation/best_macro_f1_score 0.68803
wandb:                validation/loss 0.75122
wandb:      validation/macro_f1_score 0.67766
wandb: 
wandb: 🚀 View run B1-EB-Merged at: https://wandb.ai/jimbeno/Electra%20Base/runs/rlvdiw7r
wandb: ⭐️ View project at: https://wandb.ai/jimbeno/Electra%20Base
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20241029_043031-rlvdiw7r/logs
TOTAL Time: 3m 1s
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --weights_name 'google/electra-base-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --epochs 103 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 32 --l2_strength 0.01  --checkpoint_interval 5 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 20 --wandb --wandb_project 'Electra Base' --wandb_run 'B1-EB-Merged_epoch_50' --data_file 'saves/data_8_gpu_20241029-040436.npz' --model_file 'checkpoint_epoch_50_20241029-040906.pth' --interactive

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 1 - Device: cuda:1
Rank 5 - Device: cuda:5
Rank 6 - Device: cuda:6
Rank 2 - Device: cuda:2
Rank 3 - Device: cuda:3
Rank 7 - Device: cuda:7
Rank 4 - Device: cuda:4
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /home/ubuntu/nlp-test/sentiment/wandb/run-20241029_044144-44dxm8hr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run B1-EB-Merged_epoch_50
wandb: ⭐️ View project at https://wandb.ai/jimbeno/Electra%20Base
wandb: 🚀 View run at https://wandb.ai/jimbeno/Electra%20Base/runs/44dxm8hr
Wand run initialized.

Initializing 'google/electra-base-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (3s)

Loading archived data from: saves/data_8_gpu_20241029-040436.npz...
X Train shape: [102097, 768], y Train shape: [102097]
X Validation shape: [5421, 768], y Validation shape: [5421]
X Test shape: [6530, 768], y Dev shape: [6530]
X Test Sentences shape: [6530]
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Test label distribution:
	      Negative: 2352
	       Neutral: 1829
	      Positive: 2349
Archived data loaded (2.54s)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 1, Max Grad Norm: None
Batch Size: 32, Max Epochs: 103, LR: 1e-05, Early Stop: score, Fine-tune BERT: False, Fine-tune Layers: 1, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_50_20241029-040906.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_50_20241029-040906.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (121ms)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 20 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Classes: ['negative', 'neutral', 'positive'], Number of classes: 3
Initializing model, graph, optimizer...

Model Architecture Summary:
Classifier(
  (layers): Sequential(
    (0): Linear(in_features=768, out_features=1024, bias=True)
    (1): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=1024, out_features=1024, bias=True)
    (4): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (5): Dropout(p=0.3, inplace=False)
    (6): Linear(in_features=1024, out_features=3, bias=True)
  )
)

Model Parameters:
  layers.0: 787,456 (trainable)
  layers.1.projection: 2,099,200 (trainable)
  layers.3: 1,049,600 (trainable)
  layers.6: 3,075 (trainable)

Total layers: 4
Trainable layers: 4
Total parameters: 3,939,331
Trainable parameters: 3,939,331
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 51, Max Iterations: 103, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 20

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: q
Quitting training...
Saving model in ONNX format...
Model saved in ONNX format to saves/model_20241029-044156.onnx and uploaded to Weights & Biases.
Saved model state: checkpoints/final_model_20241029-044156.pth
Saved model pickle: checkpoints/final_model_20241029-044156.pkl
Training completed (5s)

Evaluating model...

B1-EB-Merged_epoch_50 Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.747214  0.627126  0.681923      2352
     neutral   0.573379  0.734828  0.644141      1829
    positive   0.762206  0.717752  0.739312      2349

    accuracy                       0.689893      6530
   macro avg   0.694266  0.693235  0.688459      6530
weighted avg   0.703917  0.689893  0.691985      6530

ROC AUC: 0.862015

Predicted  negative  neutral  positive
Actual                                
negative       1475      568       309
neutral         268     1344       217
positive        231      432      1686

Saved predictions: saves/predictions_20241029-044157.csv

Macro F1 Score: 0.69

Evaluation completed (1s)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: eval/macro_f1_score ▁
wandb: 
wandb: Run summary:
wandb: eval/macro_f1_score 0.68846
wandb: 
wandb: 🚀 View run B1-EB-Merged_epoch_50 at: https://wandb.ai/jimbeno/Electra%20Base/runs/44dxm8hr
wandb: ⭐️ View project at: https://wandb.ai/jimbeno/Electra%20Base
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20241029_044144-44dxm8hr/logs
TOTAL Time: 26s
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --eval_dataset 'dynasent_r1' --weights_name 'google/electra-base-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --epochs 123 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 32 --l2_strength 0.01  --checkpoint_interval 5 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 20 --wandb --wandb_project 'Electra Base' --wandb_run 'B1-EB-DYN-R1' --data_file 'saves/data_8_gpu_20241029-040436.npz' --model_file 'checkpoint_epoch_77_20241029-041736.pth' --interactive

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 1 - Device: cuda:1
Rank 2 - Device: cuda:2
Rank 5 - Device: cuda:5
Rank 6 - Device: cuda:6
Rank 3 - Device: cuda:3
Rank 7 - Device: cuda:7
Rank 4 - Device: cuda:4
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /home/ubuntu/nlp-test/sentiment/wandb/run-20241029_044820-53gtf8m6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run B1-EB-DYN-R1
wandb: ⭐️ View project at https://wandb.ai/jimbeno/Electra%20Base
wandb: 🚀 View run at https://wandb.ai/jimbeno/Electra%20Base/runs/53gtf8m6
Wand run initialized.

Initializing 'google/electra-base-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (3s)

Loading archived data from: saves/data_8_gpu_20241029-040436.npz...
X Train shape: [102097, 768], y Train shape: [102097]
X Validation shape: [5421, 768], y Validation shape: [5421]
X Test shape: [6530, 768], y Dev shape: [6530]
X Test Sentences shape: [6530]
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Test label distribution:
	      Negative: 2352
	       Neutral: 1829
	      Positive: 2349
Archived data loaded (2.56s)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 1, Max Grad Norm: None
Batch Size: 32, Max Epochs: 123, LR: 1e-05, Early Stop: score, Fine-tune BERT: False, Fine-tune Layers: 1, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_77_20241029-041736.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_77_20241029-041736.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (122ms)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 20 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Classes: ['negative', 'neutral', 'positive'], Number of classes: 3
Initializing model, graph, optimizer...

Model Architecture Summary:
Classifier(
  (layers): Sequential(
    (0): Linear(in_features=768, out_features=1024, bias=True)
    (1): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=1024, out_features=1024, bias=True)
    (4): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (5): Dropout(p=0.3, inplace=False)
    (6): Linear(in_features=1024, out_features=3, bias=True)
  )
)

Model Parameters:
  layers.0: 787,456 (trainable)
  layers.1.projection: 2,099,200 (trainable)
  layers.3: 1,049,600 (trainable)
  layers.6: 3,075 (trainable)

Total layers: 4
Trainable layers: 4
Total parameters: 3,939,331
Trainable parameters: 3,939,331
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 78, Max Iterations: 123, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 20

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: ^C
KeyboardInterrupt received. Terminating all processes...
Ctrl+C received. Terminating all processes...
Rank 0 - Current process: MainProcess cleaning up...
Terminating all child processes of MainProcess...

Terminated child process: SpawnProcess-6
Terminated child process: SpawnProcess-2
Terminated child process: SpawnProcess-7
Terminated child process: SpawnProcess-4
Terminated child process: SpawnProcess-8
Terminated child process: SpawnProcess-5
Terminated child process: SpawnProcess-1
Terminated child process: SpawnProcess-3
Rank 0 - Exiting program...
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --eval_dataset 'dynasent_r1' --weights_name 'google/electra-base-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --epochs 123 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 1024 --l2_strength 0.01  --checkpoint_interval 5 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 20 --wandb --wandb_project 'Electra Base' --wandb_run 'B1-EB-DYN-R1' --model_file 'checkpoint_epoch_77_20241029-041736.pth' --interactive

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 1 - Device: cuda:1
Rank 4 - Device: cuda:4
Rank 5 - Device: cuda:5
Rank 7 - Device: cuda:7
Rank 6 - Device: cuda:6
Rank 2 - Device: cuda:2
Rank 3 - Device: cuda:3
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /home/ubuntu/nlp-test/sentiment/wandb/run-20241029_045011-w0rerpsc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run B1-EB-DYN-R1
wandb: ⭐️ View project at https://wandb.ai/jimbeno/Electra%20Base
wandb: 🚀 View run at https://wandb.ai/jimbeno/Electra%20Base/runs/w0rerpsc
Wand run initialized.

Initializing 'google/electra-base-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (3s)

Loading data...
Using different datasets for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using dynasent_r1 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: DynaSent Round 1 from Hugging Face: 'dynabench/dynasent'
Train size: 102097
Validation size: 5421
Evaluation size: 3600
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 1200
	       Neutral: 1200
	      Positive: 1200
Data loaded (1s)

Processing data...
(Batch size: 1024, Pooling: Mean, Fine Tune BERT: False, Chunk size: None)...
Extracting sentences and labels...

Displaying samples from Train data:
Train[84457]: she blurted. - NEUTRAL
Tokens: ['she', 'blurted', '.']
Embedding: [ 0.0033753   0.17586878  0.32458916 -0.54597694  0.37396565  0.2769468 ] ...

Train[33315]: $10 is the minimum tip they ask for. - NEUTRAL
Tokens: ['$', '10', 'is', 'the', 'minimum', 'tip', 'they', 'ask', 'for', '.']
Embedding: [-0.12106011  0.10919575  0.07619347 -0.54361504 -0.21918344  0.24642695] ...

Train[95755]: Once arriving home, we opened our food. - NEUTRAL
Tokens: ['once', 'arriving', 'home', ',', 'we', 'opened', 'our', 'food', '.']
Embedding: [ 0.23126072  0.0258252   0.22760166 -0.25229746  0.00471391  0.27857935] ...


Encoding Train data of 102097 texts distributed across 8 GPUs...
Batch Size: 1024, Pooling: Mean, Empty Cache: False
Padding Train data to 102104 texts for even distribution across 8 ranks...
Texts per rank: 12763, Total batches: 100
Rank 7: Processing 12763 texts (indices 89341 to 102103) in 13 batches...
Rank 3: Processing 12763 texts (indices 38289 to 51051) in 13 batches...
Rank 6: Processing 12763 texts (indices 76578 to 89340) in 13 batches...
Rank 4: Processing 12763 texts (indices 51052 to 63814) in 13 batches...
Rank 5: Processing 12763 texts (indices 63815 to 76577) in 13 batches...
Rank 0: Processing 12763 texts (indices 0 to 12762) in 13 batches...
Rank 1: Processing 12763 texts (indices 12763 to 25525) in 13 batches...
Rank 2: Processing 12763 texts (indices 25526 to 38288) in 13 batches...
An error occurred during training: CUDA out of memory. Tried to allocate 12.00 GiB. GPU 7 has a total capacty of 15.77 GiB of which 3.93 GiB is free. Including non-PyTorch memory, this process has 11.83 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
An error occurred during training: CUDA out of memory. Tried to allocate 12.00 GiB. GPU 1 has a total capacty of 15.77 GiB of which 3.86 GiB is free. Including non-PyTorch memory, this process has 11.91 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
An error occurred during training: CUDA out of memory. Tried to allocate 12.00 GiB. GPU 4 has a total capacty of 15.77 GiB of which 3.88 GiB is free. Including non-PyTorch memory, this process has 11.89 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
An error occurred during training: CUDA out of memory. Tried to allocate 12.00 GiB. GPU 6 has a total capacty of 15.77 GiB of which 3.91 GiB is free. Including non-PyTorch memory, this process has 11.86 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
An error occurred during training: CUDA out of memory. Tried to allocate 12.00 GiB. GPU 2 has a total capacty of 15.77 GiB of which 3.95 GiB is free. Including non-PyTorch memory, this process has 11.81 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
An error occurred during training: CUDA out of memory. Tried to allocate 12.00 GiB. GPU 5 has a total capacty of 15.77 GiB of which 3.86 GiB is free. Including non-PyTorch memory, this process has 11.90 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
An error occurred during training: CUDA out of memory. Tried to allocate 12.00 GiB. GPU 3 has a total capacty of 15.77 GiB of which 3.95 GiB is free. Including non-PyTorch memory, this process has 11.82 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
An error occurred during training: CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 5.38 GiB is free. Including non-PyTorch memory, this process has 10.39 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 37.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1203, in main
    X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent = process_data(tokenizer, transformer_model, pooling, world_size, train,
Traceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 540, in process_data
    X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 608, in process_data_chunks
    return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 762, in bert_phi
    outputs = model(input_ids, attention_mask=attention_mask)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
Traceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 913, in forward
    hidden_states = self.encoder(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 585, in forward
    layer_outputs = layer_module(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 474, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1203, in main
    X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent = process_data(tokenizer, transformer_model, pooling, world_size, train,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 401, in forward
    self_outputs = self.self(
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 540, in process_data
    X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 608, in process_data_chunks
    return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
Traceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 762, in bert_phi
    outputs = model(input_ids, attention_mask=attention_mask)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 297, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1203, in main
    X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent = process_data(tokenizer, transformer_model, pooling, world_size, train,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
Traceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 540, in process_data
    X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 GiB. GPU 1 has a total capacty of 15.77 GiB of which 3.86 GiB is free. Including non-PyTorch memory, this process has 11.91 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 608, in process_data_chunks
    return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 913, in forward
    hidden_states = self.encoder(
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 762, in bert_phi
    outputs = model(input_ids, attention_mask=attention_mask)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 585, in forward
    layer_outputs = layer_module(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 913, in forward
    hidden_states = self.encoder(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 474, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 585, in forward
    layer_outputs = layer_module(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
Traceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 401, in forward
    self_outputs = self.self(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 474, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
Traceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 297, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 401, in forward
    self_outputs = self.self(
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1203, in main
    X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent = process_data(tokenizer, transformer_model, pooling, world_size, train,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 540, in process_data
    X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 297, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 GiB. GPU 5 has a total capacty of 15.77 GiB of which 3.86 GiB is free. Including non-PyTorch memory, this process has 11.90 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 608, in process_data_chunks
    return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1203, in main
    X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent = process_data(tokenizer, transformer_model, pooling, world_size, train,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 GiB. GPU 6 has a total capacty of 15.77 GiB of which 3.91 GiB is free. Including non-PyTorch memory, this process has 11.86 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 762, in bert_phi
    outputs = model(input_ids, attention_mask=attention_mask)
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 540, in process_data
    X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 608, in process_data_chunks
    return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 762, in bert_phi
    outputs = model(input_ids, attention_mask=attention_mask)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1203, in main
    X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent = process_data(tokenizer, transformer_model, pooling, world_size, train,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 913, in forward
    hidden_states = self.encoder(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 540, in process_data
    X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 585, in forward
    layer_outputs = layer_module(
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 608, in process_data_chunks
    return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1203, in main
    X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent = process_data(tokenizer, transformer_model, pooling, world_size, train,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 762, in bert_phi
    outputs = model(input_ids, attention_mask=attention_mask)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 540, in process_data
    X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 608, in process_data_chunks
    return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 474, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 913, in forward
    hidden_states = self.encoder(
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 762, in bert_phi
    outputs = model(input_ids, attention_mask=attention_mask)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1203, in main
    X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent = process_data(tokenizer, transformer_model, pooling, world_size, train,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 585, in forward
    layer_outputs = layer_module(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 401, in forward
    self_outputs = self.self(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 474, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 297, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 401, in forward
    self_outputs = self.self(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 297, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 913, in forward
    hidden_states = self.encoder(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 GiB. GPU 3 has a total capacty of 15.77 GiB of which 3.95 GiB is free. Including non-PyTorch memory, this process has 11.82 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 585, in forward
    layer_outputs = layer_module(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 GiB. GPU 2 has a total capacty of 15.77 GiB of which 3.95 GiB is free. Including non-PyTorch memory, this process has 11.81 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 474, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 401, in forward
    self_outputs = self.self(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 297, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 GiB. GPU 4 has a total capacty of 15.77 GiB of which 3.88 GiB is free. Including non-PyTorch memory, this process has 11.89 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 540, in process_data
    X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 913, in forward
    hidden_states = self.encoder(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 585, in forward
    layer_outputs = layer_module(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 474, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 608, in process_data_chunks
    return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 401, in forward
    self_outputs = self.self(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 297, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 762, in bert_phi
    outputs = model(input_ids, attention_mask=attention_mask)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 GiB. GPU 7 has a total capacty of 15.77 GiB of which 3.93 GiB is free. Including non-PyTorch memory, this process has 11.83 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 913, in forward
    hidden_states = self.encoder(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 585, in forward
    layer_outputs = layer_module(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 474, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 401, in forward
    self_outputs = self.self(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 297, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 5.38 GiB is free. Including non-PyTorch memory, this process has 10.39 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 37.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: 🚀 View run B1-EB-DYN-R1 at: https://wandb.ai/jimbeno/Electra Base/runs/w0rerpsc
wandb: Find logs at: wandb/run-20241029_045011-w0rerpsc/logs
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ ^C
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --eval_dataset 'dynasent_r1' --weights_name 'google/electra-base-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --epochs 123 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 512 --l2_strength 0.01  --checkpoint_interval 5 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 20 --wandb --wandb_project 'Electra Base' --wandb_run 'B1-EB-DYN-R1' --model_file 'checkpoint_epoch_77_20241029-041736.pth' --interactive

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 4 - Device: cuda:4
Rank 6 - Device: cuda:6
Rank 1 - Device: cuda:1
Rank 5 - Device: cuda:5
Rank 7 - Device: cuda:7
Rank 2 - Device: cuda:2
Rank 3 - Device: cuda:3
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /home/ubuntu/nlp-test/sentiment/wandb/run-20241029_045110-bg17gk4w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run B1-EB-DYN-R1
wandb: ⭐️ View project at https://wandb.ai/jimbeno/Electra%20Base
wandb: 🚀 View run at https://wandb.ai/jimbeno/Electra%20Base/runs/bg17gk4w
Wand run initialized.

Initializing 'google/electra-base-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (3s)

Loading data...
Using different datasets for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using dynasent_r1 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: DynaSent Round 1 from Hugging Face: 'dynabench/dynasent'
Train size: 102097
Validation size: 5421
Evaluation size: 3600
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 1200
	       Neutral: 1200
	      Positive: 1200
Data loaded (1s)

Processing data...
(Batch size: 512, Pooling: Mean, Fine Tune BERT: False, Chunk size: None)...
Extracting sentences and labels...

Displaying samples from Train data:
Train[84457]: she blurted. - NEUTRAL
Tokens: ['she', 'blurted', '.']
Embedding: [ 0.0033753   0.17586878  0.32458916 -0.54597694  0.37396565  0.2769468 ] ...

Train[33315]: $10 is the minimum tip they ask for. - NEUTRAL
Tokens: ['$', '10', 'is', 'the', 'minimum', 'tip', 'they', 'ask', 'for', '.']
Embedding: [-0.12106011  0.10919575  0.07619347 -0.54361504 -0.21918344  0.24642695] ...

Train[95755]: Once arriving home, we opened our food. - NEUTRAL
Tokens: ['once', 'arriving', 'home', ',', 'we', 'opened', 'our', 'food', '.']
Embedding: [ 0.23126072  0.0258252   0.22760166 -0.25229746  0.00471391  0.27857935] ...


Encoding Train data of 102097 texts distributed across 8 GPUs...
Batch Size: 512, Pooling: Mean, Empty Cache: False
Padding Train data to 102104 texts for even distribution across 8 ranks...
Texts per rank: 12763, Total batches: 200
Rank 5: Processing 12763 texts (indices 63815 to 76577) in 25 batches...
Rank 6: Processing 12763 texts (indices 76578 to 89340) in 25 batches...
Rank 2: Processing 12763 texts (indices 25526 to 38288) in 25 batches...
Rank 7: Processing 12763 texts (indices 89341 to 102103) in 25 batches...
Rank 1: Processing 12763 texts (indices 12763 to 25525) in 25 batches...
Rank 3: Processing 12763 texts (indices 38289 to 51051) in 25 batches...
Rank 4: Processing 12763 texts (indices 51052 to 63814) in 25 batches...
Rank 0: Processing 12763 texts (indices 0 to 12762) in 25 batches...
An error occurred during training: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 5.37 GiB is free. Including non-PyTorch memory, this process has 10.39 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 44.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1203, in main
    X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent = process_data(tokenizer, transformer_model, pooling, world_size, train,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 540, in process_data
    X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size,
An error occurred during training: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 5 has a total capacty of 15.77 GiB of which 5.36 GiB is free. Including non-PyTorch memory, this process has 10.40 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 36.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 608, in process_data_chunks
    return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 762, in bert_phi
    outputs = model(input_ids, attention_mask=attention_mask)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 913, in forward
    hidden_states = self.encoder(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 585, in forward
    layer_outputs = layer_module(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 474, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 401, in forward
    self_outputs = self.self(
Traceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1203, in main
    X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent = process_data(tokenizer, transformer_model, pooling, world_size, train,
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 540, in process_data
    X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 608, in process_data_chunks
    return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 762, in bert_phi
    outputs = model(input_ids, attention_mask=attention_mask)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 913, in forward
    hidden_states = self.encoder(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 585, in forward
    layer_outputs = layer_module(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 474, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 401, in forward
    self_outputs = self.self(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 321, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 5 has a total capacty of 15.77 GiB of which 5.36 GiB is free. Including non-PyTorch memory, this process has 10.40 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 36.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 321, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 5.37 GiB is free. Including non-PyTorch memory, this process has 10.39 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 44.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
An error occurred during training: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 1 has a total capacty of 15.77 GiB of which 5.35 GiB is free. Including non-PyTorch memory, this process has 10.41 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 36.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1203, in main
    X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent = process_data(tokenizer, transformer_model, pooling, world_size, train,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 540, in process_data
    X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 608, in process_data_chunks
    return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 762, in bert_phi
    outputs = model(input_ids, attention_mask=attention_mask)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 913, in forward
    hidden_states = self.encoder(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 585, in forward
    layer_outputs = layer_module(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 474, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 401, in forward
    self_outputs = self.self(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 321, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 1 has a total capacty of 15.77 GiB of which 5.35 GiB is free. Including non-PyTorch memory, this process has 10.41 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 36.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
An error occurred during training: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 7 has a total capacty of 15.77 GiB of which 5.43 GiB is free. Including non-PyTorch memory, this process has 10.33 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 36.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1203, in main
    X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent = process_data(tokenizer, transformer_model, pooling, world_size, train,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 540, in process_data
    X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 608, in process_data_chunks
    return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 762, in bert_phi
    outputs = model(input_ids, attention_mask=attention_mask)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 913, in forward
    hidden_states = self.encoder(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 585, in forward
    layer_outputs = layer_module(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 474, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 401, in forward
    self_outputs = self.self(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 321, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 7 has a total capacty of 15.77 GiB of which 5.43 GiB is free. Including non-PyTorch memory, this process has 10.33 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 36.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
An error occurred during training: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 6 has a total capacty of 15.77 GiB of which 5.40 GiB is free. Including non-PyTorch memory, this process has 10.36 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 36.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1203, in main
    X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent = process_data(tokenizer, transformer_model, pooling, world_size, train,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 540, in process_data
    X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 608, in process_data_chunks
    return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 762, in bert_phi
    outputs = model(input_ids, attention_mask=attention_mask)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 913, in forward
    hidden_states = self.encoder(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 585, in forward
    layer_outputs = layer_module(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 474, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 401, in forward
    self_outputs = self.self(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 321, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 6 has a total capacty of 15.77 GiB of which 5.40 GiB is free. Including non-PyTorch memory, this process has 10.36 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 36.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
An error occurred during training: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 2 has a total capacty of 15.77 GiB of which 5.45 GiB is free. Including non-PyTorch memory, this process has 10.31 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 36.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1203, in main
    X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent = process_data(tokenizer, transformer_model, pooling, world_size, train,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 540, in process_data
    X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 608, in process_data_chunks
    return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 762, in bert_phi
    outputs = model(input_ids, attention_mask=attention_mask)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 913, in forward
    hidden_states = self.encoder(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 585, in forward
    layer_outputs = layer_module(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 474, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 401, in forward
    self_outputs = self.self(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 321, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 2 has a total capacty of 15.77 GiB of which 5.45 GiB is free. Including non-PyTorch memory, this process has 10.31 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 36.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
An error occurred during training: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 3 has a total capacty of 15.77 GiB of which 5.44 GiB is free. Including non-PyTorch memory, this process has 10.32 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 36.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1203, in main
    X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent = process_data(tokenizer, transformer_model, pooling, world_size, train,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 540, in process_data
    X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 608, in process_data_chunks
    return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 762, in bert_phi
    outputs = model(input_ids, attention_mask=attention_mask)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 913, in forward
    hidden_states = self.encoder(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 585, in forward
    layer_outputs = layer_module(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 474, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 401, in forward
    self_outputs = self.self(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 321, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 3 has a total capacty of 15.77 GiB of which 5.44 GiB is free. Including non-PyTorch memory, this process has 10.32 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 36.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
An error occurred during training: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 4 has a total capacty of 15.77 GiB of which 5.37 GiB is free. Including non-PyTorch memory, this process has 10.39 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 36.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 1203, in main
    X_train, X_val, X_test, y_train, y_val, y_test, X_test_sent = process_data(tokenizer, transformer_model, pooling, world_size, train,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 540, in process_data
    X_train = process_data_chunks(X_train_sent, bert_tokenizer, bert_model, pooling, world_size, device, batch_size,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 608, in process_data_chunks
    return bert_phi(texts, tokenizer, model, pooling, world_size, device, batch_size, sample_texts, rank, debug, split,
  File "/home/ubuntu/nlp-test/sentiment/ddp_sentiment_finetune.py", line 762, in bert_phi
    outputs = model(input_ids, attention_mask=attention_mask)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 913, in forward
    hidden_states = self.encoder(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 585, in forward
    layer_outputs = layer_module(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 474, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 401, in forward
    self_outputs = self.self(
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/nlp-test/sentiment/nlp/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 321, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 4 has a total capacty of 15.77 GiB of which 5.37 GiB is free. Including non-PyTorch memory, this process has 10.39 GiB memory in use. Of the allocated memory 9.42 GiB is allocated by PyTorch, and 36.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: 🚀 View run B1-EB-DYN-R1 at: https://wandb.ai/jimbeno/Electra Base/runs/bg17gk4w
wandb: Find logs at: wandb/run-20241029_045110-bg17gk4w/logs
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --eval_dataset 'dynasent_r1' --weights_name 'google/electra-base-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --epochs 123 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 256 --l2_strength 0.01  --checkpoint_interval 5 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 20 --wandb --wandb_project 'Electra Base' --wandb_run 'B1-EB-DYN-R1' --model_file 'checkpoint_epoch_77_20241029-041736.pth' --interactive

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 2 - Device: cuda:2
Rank 7 - Device: cuda:7
Rank 4 - Device: cuda:4
Rank 3 - Device: cuda:3
Rank 5 - Device: cuda:5
Rank 1 - Device: cuda:1
Rank 6 - Device: cuda:6
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /home/ubuntu/nlp-test/sentiment/wandb/run-20241029_045231-ous83tuq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run B1-EB-DYN-R1
wandb: ⭐️ View project at https://wandb.ai/jimbeno/Electra%20Base
wandb: 🚀 View run at https://wandb.ai/jimbeno/Electra%20Base/runs/ous83tuq
Wand run initialized.

Initializing 'google/electra-base-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (3s)

Loading data...
Using different datasets for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using dynasent_r1 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: DynaSent Round 1 from Hugging Face: 'dynabench/dynasent'
Train size: 102097
Validation size: 5421
Evaluation size: 3600
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 1200
	       Neutral: 1200
	      Positive: 1200
Data loaded (1s)

Processing data...
(Batch size: 256, Pooling: Mean, Fine Tune BERT: False, Chunk size: None)...
Extracting sentences and labels...

Displaying samples from Train data:
Train[84457]: she blurted. - NEUTRAL
Tokens: ['she', 'blurted', '.']
Embedding: [ 0.0033753   0.17586878  0.32458916 -0.54597694  0.37396565  0.2769468 ] ...

Train[33315]: $10 is the minimum tip they ask for. - NEUTRAL
Tokens: ['$', '10', 'is', 'the', 'minimum', 'tip', 'they', 'ask', 'for', '.']
Embedding: [-0.12106011  0.10919575  0.07619347 -0.54361504 -0.21918344  0.24642695] ...

Train[95755]: Once arriving home, we opened our food. - NEUTRAL
Tokens: ['once', 'arriving', 'home', ',', 'we', 'opened', 'our', 'food', '.']
Embedding: [ 0.23126072  0.0258252   0.22760166 -0.25229746  0.00471391  0.27857935] ...


Encoding Train data of 102097 texts distributed across 8 GPUs...
Batch Size: 256, Pooling: Mean, Empty Cache: False
Padding Train data to 102104 texts for even distribution across 8 ranks...
Texts per rank: 12763, Total batches: 399
Rank 3: Processing 12763 texts (indices 38289 to 51051) in 50 batches...
Rank 1: Processing 12763 texts (indices 12763 to 25525) in 50 batches...
Rank 7: Processing 12763 texts (indices 89341 to 102103) in 50 batches...
Rank 6: Processing 12763 texts (indices 76578 to 89340) in 50 batches...
Rank 5: Processing 12763 texts (indices 63815 to 76577) in 50 batches...
Rank 0: Processing 12763 texts (indices 0 to 12762) in 50 batches...
Rank 2: Processing 12763 texts (indices 25526 to 38288) in 50 batches...
Rank 4: Processing 12763 texts (indices 51052 to 63814) in 50 batches...
Rank 0: Batch  1 / 50, Shape: [256, 768], Time: 91ms
Rank 2: Batch  1 / 50, Shape: [256, 768], Time: 342ms
Rank 3: Batch  1 / 50, Shape: [256, 768], Time: 342ms
Rank 7: Batch  1 / 50, Shape: [256, 768], Time: 354ms
Rank 4: Batch  1 / 50, Shape: [256, 768], Time: 433ms
Rank 6: Batch  1 / 50, Shape: [256, 768], Time: 460ms
Rank 5: Batch  1 / 50, Shape: [256, 768], Time: 484ms
Rank 1: Batch  1 / 50, Shape: [256, 768], Time: 506ms
Rank 3: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  2 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 10.76 GB | Rank 1: 11.10 GB | Rank 2: 11.00 GB | Rank 3: 11.01 GB | Rank 4: 11.08 GB | Rank 5: 11.50 GB | Rank 6: 11.45 GB | Rank 7: 11.42 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 0: Batch  2 / 50, Shape: [256, 768], Time: 141ms
Rank 3: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  6 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.17 GB | Rank 1: 11.51 GB | Rank 2: 11.41 GB | Rank 3: 11.41 GB | Rank 4: 11.49 GB | Rank 5: 11.50 GB | Rank 6: 11.45 GB | Rank 7: 11.43 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 11 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.17 GB | Rank 1: 11.52 GB | Rank 2: 11.41 GB | Rank 3: 11.42 GB | Rank 4: 11.49 GB | Rank 5: 11.51 GB | Rank 6: 11.46 GB | Rank 7: 11.44 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 4: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 16 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.18 GB | Rank 1: 11.52 GB | Rank 2: 11.42 GB | Rank 3: 11.42 GB | Rank 4: 11.50 GB | Rank 5: 11.52 GB | Rank 6: 11.46 GB | Rank 7: 11.44 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 4: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 21 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.18 GB | Rank 1: 11.53 GB | Rank 2: 11.42 GB | Rank 3: 11.43 GB | Rank 4: 11.50 GB | Rank 5: 11.52 GB | Rank 6: 11.47 GB | Rank 7: 11.45 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 4: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 26 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.19 GB | Rank 1: 11.53 GB | Rank 2: 11.43 GB | Rank 3: 11.43 GB | Rank 4: 11.51 GB | Rank 5: 11.53 GB | Rank 6: 11.48 GB | Rank 7: 11.45 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 4: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 31 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.19 GB | Rank 1: 11.54 GB | Rank 2: 11.44 GB | Rank 3: 11.44 GB | Rank 4: 11.52 GB | Rank 5: 11.53 GB | Rank 6: 11.48 GB | Rank 7: 11.46 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 4: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 36 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.20 GB | Rank 1: 11.54 GB | Rank 2: 11.44 GB | Rank 3: 11.44 GB | Rank 4: 11.52 GB | Rank 5: 11.54 GB | Rank 6: 11.49 GB | Rank 7: 11.46 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 41 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.20 GB | Rank 1: 11.55 GB | Rank 2: 11.45 GB | Rank 3: 11.45 GB | Rank 4: 11.53 GB | Rank 5: 11.54 GB | Rank 6: 11.49 GB | Rank 7: 11.46 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 46 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.21 GB | Rank 1: 11.55 GB | Rank 2: 11.45 GB | Rank 3: 11.45 GB | Rank 4: 11.53 GB | Rank 5: 11.55 GB | Rank 6: 11.50 GB | Rank 7: 11.47 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 3: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 6: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 5: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 1: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 2: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 0: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 7: Batch 50 / 50, Shape: [219, 768], Time: 2s
Total embeddings: 102104
Original texts: 102097
Expected padding: 7
Actual padding: 7
Maximum difference between padding embeddings: 0.0
Padding embeddings verified as very similar.
Encoding completed (1m 55s)

Displaying samples from Validation data:
Validation[1381]: My primary care doctor left and unfortunately his new office is taking new patients so I stuck with Forte until I find someone new. - NEGATIVE
Tokens: ['my', 'primary', 'care', 'doctor', 'left', 'and', 'unfortunately', 'his', 'new', 'office', 'is', 'taking', 'new', 'patients', 'so', 'i', 'stuck', 'with', 'forte', 'until', 'i', 'find', 'someone', 'new', '.']
Embedding: [-0.03577539  0.1866337  -0.00972883  0.04267377  0.02039148  0.08064178] ...

Validation[2337]: He didn't even back off when we made it clear we were a couple. - NEGATIVE
Tokens: ['he', 'didn', "'", 't', 'even', 'back', 'off', 'when', 'we', 'made', 'it', 'clear', 'we', 'were', 'a', 'couple', '.']
Embedding: [-0.19298227  0.1846708   0.03479815 -0.59248054 -0.04275104  0.28450108] ...

Validation[5213]: The food was fantastic if you love cold greasy food. - NEGATIVE
Tokens: ['the', 'food', 'was', 'fantastic', 'if', 'you', 'love', 'cold', 'greasy', 'food', '.']
Embedding: [ 0.0348301  -0.06014011  0.14139125 -0.18323858 -0.0195738   0.09715316] ...


Encoding Validation data of 5421 texts distributed across 8 GPUs...
Batch Size: 256, Pooling: Mean, Empty Cache: False
Padding Validation data to 5424 texts for even distribution across 8 ranks...
Texts per rank: 678, Total batches: 22
Rank 1: Processing 678 texts (indices 678 to 1355) in 3 batches...
Rank 5: Processing 678 texts (indices 3390 to 4067) in 3 batches...
Rank 2: Processing 678 texts (indices 1356 to 2033) in 3 batches...
Rank 3: Processing 678 texts (indices 2034 to 2711) in 3 batches...
Rank 0: Processing 678 texts (indices 0 to 677) in 3 batches...
Rank 7: Processing 678 texts (indices 4746 to 5423) in 3 batches...
Rank 4: Processing 678 texts (indices 2712 to 3389) in 3 batches...
Rank 6: Processing 678 texts (indices 4068 to 4745) in 3 batches...
Rank 3: Batch  1 / 3, Shape: [256, 768], Time: 86ms
Rank 4: Batch  1 / 3, Shape: [256, 768], Time: 91ms
Rank 2: Batch  1 / 3, Shape: [256, 768], Time: 93ms
Rank 7: Batch  1 / 3, Shape: [256, 768], Time: 94ms
Rank 1: Batch  1 / 3, Shape: [256, 768], Time: 94ms
Rank 6: Batch  1 / 3, Shape: [256, 768], Time: 96ms
Rank 5: Batch  1 / 3, Shape: [256, 768], Time: 97ms
Rank 0: Batch  1 / 3, Shape: [256, 768], Time: 235ms
Memory: Rank 0: 11.16 GB | Rank 1: 11.55 GB | Rank 2: 11.45 GB | Rank 3: 11.46 GB | Rank 4: 11.53 GB | Rank 5: 11.55 GB | Rank 6: 11.50 GB | Rank 7: 11.48 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 4: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 3: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 6: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 1: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 5: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 2: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 7: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 0: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 4: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 3: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 6: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 1: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 5: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 2: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 0: Batch  3 / 3, Shape: [166, 768], Time: 2s
Total embeddings: 5424
Original texts: 5421
Expected padding: 3
Actual padding: 3
Rank 7: Batch  3 / 3, Shape: [166, 768], Time: 2s
Maximum difference between padding embeddings: 0.0
Padding embeddings verified as very similar.
Encoding completed (6s)

Displaying samples from Evaluation data:
Evaluation[2270]: She brought the breadsticks over to us late ...just lots of little things. - NEGATIVE
Tokens: ['she', 'brought', 'the', 'bread', '##stick', '##s', 'over', 'to', 'us', 'late', '.', '.', '.', 'just', 'lots', 'of', 'little', 'things', '.']
Embedding: [-0.1184809  -0.06222033 -0.05655206 -0.14355242  0.00800195  0.08705036] ...

Evaluation[735]: I've heard a couple of good comments about this pizza so I finally gave it a try. - NEUTRAL
Tokens: ['i', "'", 've', 'heard', 'a', 'couple', 'of', 'good', 'comments', 'about', 'this', 'pizza', 'so', 'i', 'finally', 'gave', 'it', 'a', 'try', '.']
Embedding: [-0.06812384  0.15084063  0.08960816 -0.17725493  0.04382836  0.275119  ] ...

Evaluation[1275]: If you're wondering what to do with your time and money in Las Vegas, go to Baz. - POSITIVE
Tokens: ['if', 'you', "'", 're', 'wondering', 'what', 'to', 'do', 'with', 'your', 'time', 'and', 'money', 'in', 'las', 'vegas', ',', 'go', 'to', 'ba', '##z', '.']
Embedding: [-0.09249381  0.03481807  0.05863693 -0.10625216  0.02719286  0.19827813] ...


Encoding Evaluation data of 3600 texts distributed across 8 GPUs...
Batch Size: 256, Pooling: Mean, Empty Cache: False
Texts per rank: 450, Total batches: 15
Rank 1: Processing 450 texts (indices 450 to 899) in 2 batches...
Rank 3: Processing 450 texts (indices 1350 to 1799) in 2 batches...
Rank 0: Processing 450 texts (indices 0 to 449) in 2 batches...
Rank 4: Processing 450 texts (indices 1800 to 2249) in 2 batches...
Rank 5: Processing 450 texts (indices 2250 to 2699) in 2 batches...
Rank 7: Processing 450 texts (indices 3150 to 3599) in 2 batches...
Rank 2: Processing 450 texts (indices 900 to 1349) in 2 batches...
Rank 6: Processing 450 texts (indices 2700 to 3149) in 2 batches...
Rank 3: Batch  1 / 2, Shape: [256, 768], Time: 86ms
Rank 7: Batch  1 / 2, Shape: [256, 768], Time: 87ms
Rank 1: Batch  1 / 2, Shape: [256, 768], Time: 88ms
Rank 5: Batch  1 / 2, Shape: [256, 768], Time: 88ms
Rank 4: Batch  1 / 2, Shape: [256, 768], Time: 90ms
Rank 2: Batch  1 / 2, Shape: [256, 768], Time: 91ms
Rank 6: Batch  1 / 2, Shape: [256, 768], Time: 92ms
Rank 0: Batch  1 / 2, Shape: [256, 768], Time: 243ms
Memory: Rank 0: 11.16 GB | Rank 1: 11.96 GB | Rank 2: 11.86 GB | Rank 3: 11.86 GB | Rank 4: 11.94 GB | Rank 5: 11.95 GB | Rank 6: 11.90 GB | Rank 7: 11.88 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 4: Batch  2 / 2, Shape: [194, 768], Time: 2s
Rank 3: Batch  2 / 2, Shape: [194, 768], Time: 2s
Rank 6: Batch  2 / 2, Shape: [194, 768], Time: 2s
Rank 5: Batch  2 / 2, Shape: [194, 768], Time: 2s
Rank 2: Batch  2 / 2, Shape: [194, 768], Time: 2s
Rank 1: Batch  2 / 2, Shape: [194, 768], Time: 2s
Rank 7: Batch  2 / 2, Shape: [194, 768], Time: 2s
Rank 0: Batch  2 / 2, Shape: [194, 768], Time: 2s
Total embeddings: 3600
Original texts: 3600
Expected padding: 0
Actual padding: 0
Encoding completed (4s)

Data saved to: saves/data_8_gpu_20241029-045443.npz
X Train shape: [102097, 768], X Validation shape: [5421, 768], X Test shape: [3600, 768]
y Train shape: [102097], y Validation shape: [5421], y Test shape: [3600]
Data processed (2m 20s)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 1, Max Grad Norm: None
Batch Size: 256, Max Epochs: 123, LR: 1e-05, Early Stop: score, Fine-tune BERT: False, Fine-tune Layers: 1, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_77_20241029-041736.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_77_20241029-041736.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (123ms)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 20 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Classes: ['negative', 'neutral', 'positive'], Number of classes: 3
Initializing model, graph, optimizer...

Model Architecture Summary:
Classifier(
  (layers): Sequential(
    (0): Linear(in_features=768, out_features=1024, bias=True)
    (1): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=1024, out_features=1024, bias=True)
    (4): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (5): Dropout(p=0.3, inplace=False)
    (6): Linear(in_features=1024, out_features=3, bias=True)
  )
)

Model Parameters:
  layers.0: 787,456 (trainable)
  layers.1.projection: 2,099,200 (trainable)
  layers.3: 1,049,600 (trainable)
  layers.6: 3,075 (trainable)

Total layers: 4
Trainable layers: 4
Total parameters: 3,939,331
Trainable parameters: 3,939,331
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 78, Max Iterations: 123, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 20

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: q
Quitting training...
Saving model in ONNX format...
Model saved in ONNX format to saves/model_20241029-045530.onnx and uploaded to Weights & Biases.
Saved model state: checkpoints/final_model_20241029-045530.pth
Saved model pickle: checkpoints/final_model_20241029-045530.pkl
Training completed (33s)

Evaluating model...

B1-EB-DYN-R1 Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.788779  0.597500  0.679943      1200
     neutral   0.626163  0.897500  0.737671      1200
    positive   0.795057  0.643333  0.711193      1200

    accuracy                       0.712778      3600
   macro avg   0.736666  0.712778  0.709602      3600
weighted avg   0.736666  0.712778  0.709602      3600

ROC AUC: 0.890687

Predicted  negative  neutral  positive
Actual                                
negative        717      341       142
neutral          66     1077        57
positive        126      302       772

Saved predictions: saves/predictions_20241029-045531.csv

Macro F1 Score: 0.71

Evaluation completed (674ms)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: eval/macro_f1_score ▁
wandb: 
wandb: Run summary:
wandb: eval/macro_f1_score 0.7096
wandb: 
wandb: 🚀 View run B1-EB-DYN-R1 at: https://wandb.ai/jimbeno/Electra%20Base/runs/ous83tuq
wandb: ⭐️ View project at: https://wandb.ai/jimbeno/Electra%20Base
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20241029_045231-ous83tuq/logs
TOTAL Time: 3m 12s
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --eval_dataset 'dynasent_r2' --weights_name 'google/electra-base-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --epochs 123 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 256 --l2_strength 0.01  --checkpoint_interval 5 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 20 --wandb --wandb_project 'Electra Base' --wandb_run 'B1-EB-DYN-R2' --model_file 'checkpoint_epoch_77_20241029-041736.pth' --interactive

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 7 - Device: cuda:7
Rank 1 - Device: cuda:1
Rank 6 - Device: cuda:6
Rank 5 - Device: cuda:5
Rank 2 - Device: cuda:2
Rank 3 - Device: cuda:3
Rank 0 - Device: cuda:0
Rank 4 - Device: cuda:4
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /home/ubuntu/nlp-test/sentiment/wandb/run-20241029_045709-lt8k7iz6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run B1-EB-DYN-R2
wandb: ⭐️ View project at https://wandb.ai/jimbeno/Electra%20Base
wandb: 🚀 View run at https://wandb.ai/jimbeno/Electra%20Base/runs/lt8k7iz6
Wand run initialized.

Initializing 'google/electra-base-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (3s)

Loading data...
Using different datasets for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using dynasent_r2 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: DynaSent Round 2 from Hugging Face: 'dynabench/dynasent'
Train size: 102097
Validation size: 5421
Evaluation size: 720
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 240
	       Neutral: 240
	      Positive: 240
Data loaded (1s)

Processing data...
(Batch size: 256, Pooling: Mean, Fine Tune BERT: False, Chunk size: None)...
Extracting sentences and labels...

Displaying samples from Train data:
Train[84457]: she blurted. - NEUTRAL
Tokens: ['she', 'blurted', '.']
Embedding: [ 0.0033753   0.17586878  0.32458916 -0.54597694  0.37396565  0.2769468 ] ...

Train[33315]: $10 is the minimum tip they ask for. - NEUTRAL
Tokens: ['$', '10', 'is', 'the', 'minimum', 'tip', 'they', 'ask', 'for', '.']
Embedding: [-0.12106011  0.10919575  0.07619347 -0.54361504 -0.21918344  0.24642695] ...

Train[95755]: Once arriving home, we opened our food. - NEUTRAL
Tokens: ['once', 'arriving', 'home', ',', 'we', 'opened', 'our', 'food', '.']
Embedding: [ 0.23126072  0.0258252   0.22760166 -0.25229746  0.00471391  0.27857935] ...


Encoding Train data of 102097 texts distributed across 8 GPUs...
Batch Size: 256, Pooling: Mean, Empty Cache: False
Padding Train data to 102104 texts for even distribution across 8 ranks...
Texts per rank: 12763, Total batches: 399
Rank 7: Processing 12763 texts (indices 89341 to 102103) in 50 batches...
Rank 5: Processing 12763 texts (indices 63815 to 76577) in 50 batches...
Rank 4: Processing 12763 texts (indices 51052 to 63814) in 50 batches...
Rank 6: Processing 12763 texts (indices 76578 to 89340) in 50 batches...
Rank 0: Processing 12763 texts (indices 0 to 12762) in 50 batches...
Rank 1: Processing 12763 texts (indices 12763 to 25525) in 50 batches...
Rank 2: Processing 12763 texts (indices 25526 to 38288) in 50 batches...
Rank 3: Processing 12763 texts (indices 38289 to 51051) in 50 batches...
Rank 0: Batch  1 / 50, Shape: [256, 768], Time: 100ms
Rank 7: Batch  1 / 50, Shape: [256, 768], Time: 426ms
Rank 3: Batch  1 / 50, Shape: [256, 768], Time: 435ms
Rank 5: Batch  1 / 50, Shape: [256, 768], Time: 440ms
Rank 1: Batch  1 / 50, Shape: [256, 768], Time: 442ms
Rank 4: Batch  1 / 50, Shape: [256, 768], Time: 454ms
Rank 6: Batch  1 / 50, Shape: [256, 768], Time: 532ms
Rank 2: Batch  1 / 50, Shape: [256, 768], Time: 539ms
Rank 5: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  2 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 10.76 GB | Rank 1: 11.11 GB | Rank 2: 11.01 GB | Rank 3: 11.01 GB | Rank 4: 11.09 GB | Rank 5: 11.51 GB | Rank 6: 11.46 GB | Rank 7: 11.43 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 0: Batch  2 / 50, Shape: [256, 768], Time: 147ms
Rank 4: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  6 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.17 GB | Rank 1: 11.52 GB | Rank 2: 11.42 GB | Rank 3: 11.42 GB | Rank 4: 11.50 GB | Rank 5: 11.51 GB | Rank 6: 11.46 GB | Rank 7: 11.44 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 4: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 11 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.17 GB | Rank 1: 11.52 GB | Rank 2: 11.42 GB | Rank 3: 11.43 GB | Rank 4: 11.50 GB | Rank 5: 11.52 GB | Rank 6: 11.47 GB | Rank 7: 11.44 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 4: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 16 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.18 GB | Rank 1: 11.53 GB | Rank 2: 11.43 GB | Rank 3: 11.43 GB | Rank 4: 11.51 GB | Rank 5: 11.52 GB | Rank 6: 11.47 GB | Rank 7: 11.45 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 4: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 21 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.18 GB | Rank 1: 11.53 GB | Rank 2: 11.43 GB | Rank 3: 11.44 GB | Rank 4: 11.51 GB | Rank 5: 11.53 GB | Rank 6: 11.48 GB | Rank 7: 11.45 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 4: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 26 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.19 GB | Rank 1: 11.54 GB | Rank 2: 11.44 GB | Rank 3: 11.44 GB | Rank 4: 11.52 GB | Rank 5: 11.53 GB | Rank 6: 11.48 GB | Rank 7: 11.46 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 4: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 31 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.19 GB | Rank 1: 11.54 GB | Rank 2: 11.44 GB | Rank 3: 11.45 GB | Rank 4: 11.52 GB | Rank 5: 11.54 GB | Rank 6: 11.49 GB | Rank 7: 11.46 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 4: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 36 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.20 GB | Rank 1: 11.55 GB | Rank 2: 11.45 GB | Rank 3: 11.45 GB | Rank 4: 11.53 GB | Rank 5: 11.54 GB | Rank 6: 11.49 GB | Rank 7: 11.47 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 41 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.20 GB | Rank 1: 11.55 GB | Rank 2: 11.45 GB | Rank 3: 11.46 GB | Rank 4: 11.53 GB | Rank 5: 11.55 GB | Rank 6: 11.50 GB | Rank 7: 11.47 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 46 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.21 GB | Rank 1: 11.56 GB | Rank 2: 11.46 GB | Rank 3: 11.46 GB | Rank 4: 11.54 GB | Rank 5: 11.55 GB | Rank 6: 11.50 GB | Rank 7: 11.48 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 3: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 6: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 5: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 1: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 0: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 2: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 7: Batch 50 / 50, Shape: [219, 768], Time: 2s
Total embeddings: 102104
Original texts: 102097
Expected padding: 7
Actual padding: 7
Maximum difference between padding embeddings: 0.0
Padding embeddings verified as very similar.
Encoding completed (1m 55s)

Displaying samples from Validation data:
Validation[1381]: My primary care doctor left and unfortunately his new office is taking new patients so I stuck with Forte until I find someone new. - NEGATIVE
Tokens: ['my', 'primary', 'care', 'doctor', 'left', 'and', 'unfortunately', 'his', 'new', 'office', 'is', 'taking', 'new', 'patients', 'so', 'i', 'stuck', 'with', 'forte', 'until', 'i', 'find', 'someone', 'new', '.']
Embedding: [-0.03577539  0.1866337  -0.00972883  0.04267377  0.02039148  0.08064178] ...

Validation[2337]: He didn't even back off when we made it clear we were a couple. - NEGATIVE
Tokens: ['he', 'didn', "'", 't', 'even', 'back', 'off', 'when', 'we', 'made', 'it', 'clear', 'we', 'were', 'a', 'couple', '.']
Embedding: [-0.19298227  0.1846708   0.03479815 -0.59248054 -0.04275104  0.28450108] ...

Validation[5213]: The food was fantastic if you love cold greasy food. - NEGATIVE
Tokens: ['the', 'food', 'was', 'fantastic', 'if', 'you', 'love', 'cold', 'greasy', 'food', '.']
Embedding: [ 0.0348301  -0.06014011  0.14139125 -0.18323858 -0.0195738   0.09715316] ...


Encoding Validation data of 5421 texts distributed across 8 GPUs...
Batch Size: 256, Pooling: Mean, Empty Cache: False
Padding Validation data to 5424 texts for even distribution across 8 ranks...
Texts per rank: 678, Total batches: 22
Rank 3: Processing 678 texts (indices 2034 to 2711) in 3 batches...
Rank 2: Processing 678 texts (indices 1356 to 2033) in 3 batches...
Rank 4: Processing 678 texts (indices 2712 to 3389) in 3 batches...
Rank 1: Processing 678 texts (indices 678 to 1355) in 3 batches...
Rank 6: Processing 678 texts (indices 4068 to 4745) in 3 batches...
Rank 5: Processing 678 texts (indices 3390 to 4067) in 3 batches...
Rank 7: Processing 678 texts (indices 4746 to 5423) in 3 batches...
Rank 0: Processing 678 texts (indices 0 to 677) in 3 batches...
Rank 4: Batch  1 / 3, Shape: [256, 768], Time: 87ms
Rank 2: Batch  1 / 3, Shape: [256, 768], Time: 87ms
Rank 3: Batch  1 / 3, Shape: [256, 768], Time: 88ms
Rank 5: Batch  1 / 3, Shape: [256, 768], Time: 89ms
Rank 6: Batch  1 / 3, Shape: [256, 768], Time: 91ms
Rank 1: Batch  1 / 3, Shape: [256, 768], Time: 92ms
Rank 7: Batch  1 / 3, Shape: [256, 768], Time: 148ms
Rank 0: Batch  1 / 3, Shape: [256, 768], Time: 217ms
Memory: Rank 0: 11.16 GB | Rank 1: 11.56 GB | Rank 2: 11.46 GB | Rank 3: 11.47 GB | Rank 4: 11.54 GB | Rank 5: 11.56 GB | Rank 6: 11.51 GB | Rank 7: 11.48 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 4: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 3: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 6: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 5: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 2: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 1: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 0: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 7: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 4: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 3: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 6: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 5: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 2: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 1: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 0: Batch  3 / 3, Shape: [166, 768], Time: 2s
Total embeddings: 5424
Original texts: 5421
Expected padding: 3
Actual padding: 3
Rank 7: Batch  3 / 3, Shape: [166, 768], Time: 2s
Maximum difference between padding embeddings: 0.0
Padding embeddings verified as very similar.
Encoding completed (6s)

Displaying samples from Evaluation data:
Evaluation[284]: Check in was hard, boring seeing all the Flamingos, fish, turtles outside. - NEGATIVE
Tokens: ['check', 'in', 'was', 'hard', ',', 'boring', 'seeing', 'all', 'the', 'flaming', '##os', ',', 'fish', ',', 'turtles', 'outside', '.']
Embedding: [ 0.04272212  0.02016286  0.03870789 -0.2276077   0.03190934  0.09706514] ...

Evaluation[9]: My car wasn't working. - NEGATIVE
Tokens: ['my', 'car', 'wasn', "'", 't', 'working', '.']
Embedding: [-0.02403473  0.10543826  0.09924571 -0.64583266  0.09486583  0.27188858] ...

Evaluation[34]: kids are allowed - POSITIVE
Tokens: ['kids', 'are', 'allowed']
Embedding: [ 0.06258728  0.0517352   0.22796193 -0.21547166  0.14358363  0.30749577] ...


Encoding Evaluation data of 720 texts distributed across 8 GPUs...
Batch Size: 256, Pooling: Mean, Empty Cache: False
Texts per rank: 90, Total batches: 3
Rank 2: Processing 90 texts (indices 180 to 269) in 1 batches...
Rank 7: Processing 90 texts (indices 630 to 719) in 1 batches...
Rank 1: Processing 90 texts (indices 90 to 179) in 1 batches...
Rank 0: Processing 90 texts (indices 0 to 89) in 1 batches...
Rank 4: Processing 90 texts (indices 360 to 449) in 1 batches...
Rank 3: Processing 90 texts (indices 270 to 359) in 1 batches...
Rank 6: Processing 90 texts (indices 540 to 629) in 1 batches...
Rank 5: Processing 90 texts (indices 450 to 539) in 1 batches...
Rank 7: Batch  1 / 1, Shape: [90, 768], Time: 36ms
Rank 4: Batch  1 / 1, Shape: [90, 768], Time: 36ms
Rank 2: Batch  1 / 1, Shape: [90, 768], Time: 37ms
Rank 5: Batch  1 / 1, Shape: [90, 768], Time: 38ms
Rank 6: Batch  1 / 1, Shape: [90, 768], Time: 38ms
Rank 3: Batch  1 / 1, Shape: [90, 768], Time: 39ms
Rank 1: Batch  1 / 1, Shape: [90, 768], Time: 39ms
Rank 0: Batch  1 / 1, Shape: [90, 768], Time: 39ms
Memory: Rank 0: 5.16 GB | Rank 1: 11.97 GB | Rank 2: 11.87 GB | Rank 3: 11.87 GB | Rank 4: 11.95 GB | Rank 5: 11.96 GB | Rank 6: 11.91 GB | Rank 7: 11.89 GB (Max: 16.93 GB, Total: 135.43 GB)
Total embeddings: 720
Original texts: 720
Expected padding: 0
Actual padding: 0
Encoding completed (903ms)

Data saved to: saves/data_8_gpu_20241029-045918.npz
X Train shape: [102097, 768], X Validation shape: [5421, 768], X Test shape: [720, 768]
y Train shape: [102097], y Validation shape: [5421], y Test shape: [720]
Data processed (2m 18s)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 1, Max Grad Norm: None
Batch Size: 256, Max Epochs: 123, LR: 1e-05, Early Stop: score, Fine-tune BERT: False, Fine-tune Layers: 1, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_77_20241029-041736.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_77_20241029-041736.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (114ms)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 20 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Classes: ['negative', 'neutral', 'positive'], Number of classes: 3
Initializing model, graph, optimizer...

Model Architecture Summary:
Classifier(
  (layers): Sequential(
    (0): Linear(in_features=768, out_features=1024, bias=True)
    (1): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=1024, out_features=1024, bias=True)
    (4): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (5): Dropout(p=0.3, inplace=False)
    (6): Linear(in_features=1024, out_features=3, bias=True)
  )
)

Model Parameters:
  layers.0: 787,456 (trainable)
  layers.1.projection: 2,099,200 (trainable)
  layers.3: 1,049,600 (trainable)
  layers.6: 3,075 (trainable)

Total layers: 4
Trainable layers: 4
Total parameters: 3,939,331
Trainable parameters: 3,939,331
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 78, Max Iterations: 123, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 20

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: q
Quitting training...
Saving model in ONNX format...
Model saved in ONNX format to saves/model_20241029-050042.onnx and uploaded to Weights & Biases.
Saved model state: checkpoints/final_model_20241029-050043.pth
Saved model pickle: checkpoints/final_model_20241029-050043.pkl
Training completed (1m 9s)

Evaluating model...

B1-EB-DYN-R2 Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.591928  0.550000  0.570194       240
     neutral   0.655022  0.625000  0.639659       240
    positive   0.604478  0.675000  0.637795       240

    accuracy                       0.616667       720
   macro avg   0.617143  0.616667  0.615883       720
weighted avg   0.617143  0.616667  0.615883       720

ROC AUC: 0.814112

Predicted  negative  neutral  positive
Actual                                
negative        132       44        64
neutral          48      150        42
positive         43       35       162

Saved predictions: saves/predictions_20241029-050043.csv

Macro F1 Score: 0.62

Evaluation completed (471ms)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: eval/macro_f1_score ▁
wandb: 
wandb: Run summary:
wandb: eval/macro_f1_score 0.61588
wandb: 
wandb: 🚀 View run B1-EB-DYN-R2 at: https://wandb.ai/jimbeno/Electra%20Base/runs/lt8k7iz6
wandb: ⭐️ View project at: https://wandb.ai/jimbeno/Electra%20Base
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20241029_045709-lt8k7iz6/logs
TOTAL Time: 3m 47s
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ python ./ddp_sentiment_finetune.py --dataset 'merged_local' --eval_dataset 'sst_local' --weights_name 'google/electra-base-discriminator' --save_data --save_model --save_pickle --save_preds --lr 0.00001 --epochs 123 --pooling 'mean' --dropout_rate 0.3 --num_layers 2 --hidden_dim 1024 --hidden_activation 'swishglu' --batch_size 256 --l2_strength 0.01  --checkpoint_interval 5 --use_zero --optimizer 'adamw' --scheduler 'cosine_warmup' --scheduler_kwargs '{"T_0":5, "T_mult":1, "eta_min":1e-7}' --decimal 6 --use_val_split --eval_split 'test' --early_stop 'score' --n_iter_no_change 20 --wandb --wandb_project 'Electra Base' --wandb_run 'B1-EB-SST' --model_file 'checkpoint_epoch_77_20241029-041736.pth' --interactive

Starting DDP PyTorch Training...
Device: cuda, Number of GPUs: 8
Spawning 8 processes...
Rank 7 - Device: cuda:7
Rank 4 - Device: cuda:4
Rank 5 - Device: cuda:5
Rank 2 - Device: cuda:2
Rank 6 - Device: cuda:6
Rank 1 - Device: cuda:1
Rank 3 - Device: cuda:3
Rank 0 - Device: cuda:0
8 process groups initialized with 'nccl' backend on localhost:12355
NCCL Timeout: 1 hr 0 min. NCCL Blocking Wait: Enabled
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbeno (jimbeno). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /home/ubuntu/nlp-test/sentiment/wandb/run-20241029_050315-im59p4eq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run B1-EB-SST
wandb: ⭐️ View project at https://wandb.ai/jimbeno/Electra%20Base
wandb: 🚀 View run at https://wandb.ai/jimbeno/Electra%20Base/runs/im59p4eq
Wand run initialized.

Initializing 'google/electra-base-discriminator' tokenizer and model...
Checking for local files...
Found all files in cache, skipping download
All ranks loading tokenizer from local files...
All ranks loading model from local files...
Tokenizer and model initialized (3s)

Loading data...
Using different datasets for training and evaluation
Splits:
- Train: Using merged_local 'train' split
- Validation: Using merged_local 'validation' split
- Evaluation: Using sst_local 'test' split
Train Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Validation Data: Merged DynaSent Round 1, Round 2 and SST from Local: 'data/merged'
Evaluation Data: Stanford Sentiment Treebank (SST) from Local: 'data/sentiment'
Train size: 102097
Validation size: 5421
Evaluation size: 2210
Train label distribution:
	      Negative: 21910
	       Neutral: 49148
	      Positive: 31039
Validation label distribution:
	      Negative: 1868
	       Neutral: 1669
	      Positive: 1884
Evaluation label distribution:
	      Negative: 912
	       Neutral: 389
	      Positive: 909
Data loaded (318ms)

Processing data...
(Batch size: 256, Pooling: Mean, Fine Tune BERT: False, Chunk size: None)...
Extracting sentences and labels...

Displaying samples from Train data:
Train[84457]: she blurted. - NEUTRAL
Tokens: ['she', 'blurted', '.']
Embedding: [ 0.0033753   0.17586878  0.32458916 -0.54597694  0.37396565  0.2769468 ] ...

Train[33315]: $10 is the minimum tip they ask for. - NEUTRAL
Tokens: ['$', '10', 'is', 'the', 'minimum', 'tip', 'they', 'ask', 'for', '.']
Embedding: [-0.12106011  0.10919575  0.07619347 -0.54361504 -0.21918344  0.24642695] ...

Train[95755]: Once arriving home, we opened our food. - NEUTRAL
Tokens: ['once', 'arriving', 'home', ',', 'we', 'opened', 'our', 'food', '.']
Embedding: [ 0.23126072  0.0258252   0.22760166 -0.25229746  0.00471391  0.27857935] ...


Encoding Train data of 102097 texts distributed across 8 GPUs...
Batch Size: 256, Pooling: Mean, Empty Cache: False
Padding Train data to 102104 texts for even distribution across 8 ranks...
Texts per rank: 12763, Total batches: 399
Rank 7: Processing 12763 texts (indices 89341 to 102103) in 50 batches...
Rank 3: Processing 12763 texts (indices 38289 to 51051) in 50 batches...
Rank 1: Processing 12763 texts (indices 12763 to 25525) in 50 batches...
Rank 6: Processing 12763 texts (indices 76578 to 89340) in 50 batches...
Rank 5: Processing 12763 texts (indices 63815 to 76577) in 50 batches...
Rank 2: Processing 12763 texts (indices 25526 to 38288) in 50 batches...
Rank 4: Processing 12763 texts (indices 51052 to 63814) in 50 batches...
Rank 0: Processing 12763 texts (indices 0 to 12762) in 50 batches...
Rank 0: Batch  1 / 50, Shape: [256, 768], Time: 90ms
Rank 2: Batch  1 / 50, Shape: [256, 768], Time: 334ms
Rank 1: Batch  1 / 50, Shape: [256, 768], Time: 346ms
Rank 6: Batch  1 / 50, Shape: [256, 768], Time: 371ms
Rank 3: Batch  1 / 50, Shape: [256, 768], Time: 376ms
Rank 7: Batch  1 / 50, Shape: [256, 768], Time: 470ms
Rank 5: Batch  1 / 50, Shape: [256, 768], Time: 470ms
Rank 4: Batch  1 / 50, Shape: [256, 768], Time: 509ms
Rank 6: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  2 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  2 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 10.76 GB | Rank 1: 11.11 GB | Rank 2: 11.01 GB | Rank 3: 11.01 GB | Rank 4: 11.09 GB | Rank 5: 11.51 GB | Rank 6: 11.46 GB | Rank 7: 11.43 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 0: Batch  2 / 50, Shape: [256, 768], Time: 143ms
Rank 6: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  3 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  4 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  5 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  6 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  6 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.17 GB | Rank 1: 11.52 GB | Rank 2: 11.42 GB | Rank 3: 11.42 GB | Rank 4: 11.50 GB | Rank 5: 11.51 GB | Rank 6: 11.46 GB | Rank 7: 11.44 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  7 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  8 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch  9 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 10 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 11 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 11 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.17 GB | Rank 1: 11.52 GB | Rank 2: 11.42 GB | Rank 3: 11.43 GB | Rank 4: 11.50 GB | Rank 5: 11.52 GB | Rank 6: 11.47 GB | Rank 7: 11.44 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 12 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 13 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 14 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 15 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 16 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 16 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.18 GB | Rank 1: 11.53 GB | Rank 2: 11.43 GB | Rank 3: 11.43 GB | Rank 4: 11.51 GB | Rank 5: 11.52 GB | Rank 6: 11.47 GB | Rank 7: 11.45 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 17 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 18 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 19 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 20 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 21 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 21 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.18 GB | Rank 1: 11.53 GB | Rank 2: 11.43 GB | Rank 3: 11.44 GB | Rank 4: 11.51 GB | Rank 5: 11.53 GB | Rank 6: 11.48 GB | Rank 7: 11.45 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 6: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 22 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 23 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 24 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 25 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 26 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.19 GB | Rank 1: 11.54 GB | Rank 2: 11.44 GB | Rank 3: 11.44 GB | Rank 4: 11.52 GB | Rank 5: 11.53 GB | Rank 6: 11.48 GB | Rank 7: 11.46 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 26 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 27 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 28 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 29 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 30 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 31 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.19 GB | Rank 1: 11.54 GB | Rank 2: 11.44 GB | Rank 3: 11.45 GB | Rank 4: 11.52 GB | Rank 5: 11.54 GB | Rank 6: 11.49 GB | Rank 7: 11.46 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 31 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 32 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 33 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 34 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 35 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 36 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.20 GB | Rank 1: 11.55 GB | Rank 2: 11.45 GB | Rank 3: 11.45 GB | Rank 4: 11.53 GB | Rank 5: 11.54 GB | Rank 6: 11.49 GB | Rank 7: 11.47 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 7: Batch 36 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 37 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 38 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 39 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 40 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 41 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.20 GB | Rank 1: 11.55 GB | Rank 2: 11.45 GB | Rank 3: 11.46 GB | Rank 4: 11.53 GB | Rank 5: 11.55 GB | Rank 6: 11.50 GB | Rank 7: 11.47 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 41 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 42 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 43 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 44 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 45 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 46 / 50, Shape: [256, 768], Time: 2s
Memory: Rank 0: 11.21 GB | Rank 1: 11.56 GB | Rank 2: 11.46 GB | Rank 3: 11.46 GB | Rank 4: 11.54 GB | Rank 5: 11.55 GB | Rank 6: 11.50 GB | Rank 7: 11.48 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 2: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 46 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 47 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 48 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 3: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 6: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 0: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 5: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 1: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 2: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 7: Batch 49 / 50, Shape: [256, 768], Time: 2s
Rank 4: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 3: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 6: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 0: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 5: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 1: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 2: Batch 50 / 50, Shape: [219, 768], Time: 2s
Rank 7: Batch 50 / 50, Shape: [219, 768], Time: 2s
Total embeddings: 102104
Original texts: 102097
Expected padding: 7
Actual padding: 7
Maximum difference between padding embeddings: 0.0
Padding embeddings verified as very similar.
Encoding completed (1m 55s)

Displaying samples from Validation data:
Validation[1381]: My primary care doctor left and unfortunately his new office is taking new patients so I stuck with Forte until I find someone new. - NEGATIVE
Tokens: ['my', 'primary', 'care', 'doctor', 'left', 'and', 'unfortunately', 'his', 'new', 'office', 'is', 'taking', 'new', 'patients', 'so', 'i', 'stuck', 'with', 'forte', 'until', 'i', 'find', 'someone', 'new', '.']
Embedding: [-0.03577539  0.1866337  -0.00972883  0.04267377  0.02039148  0.08064178] ...

Validation[2337]: He didn't even back off when we made it clear we were a couple. - NEGATIVE
Tokens: ['he', 'didn', "'", 't', 'even', 'back', 'off', 'when', 'we', 'made', 'it', 'clear', 'we', 'were', 'a', 'couple', '.']
Embedding: [-0.19298227  0.1846708   0.03479815 -0.59248054 -0.04275104  0.28450108] ...

Validation[5213]: The food was fantastic if you love cold greasy food. - NEGATIVE
Tokens: ['the', 'food', 'was', 'fantastic', 'if', 'you', 'love', 'cold', 'greasy', 'food', '.']
Embedding: [ 0.0348301  -0.06014011  0.14139125 -0.18323858 -0.0195738   0.09715316] ...


Encoding Validation data of 5421 texts distributed across 8 GPUs...
Batch Size: 256, Pooling: Mean, Empty Cache: False
Padding Validation data to 5424 texts for even distribution across 8 ranks...
Texts per rank: 678, Total batches: 22
Rank 6: Processing 678 texts (indices 4068 to 4745) in 3 batches...
Rank 1: Processing 678 texts (indices 678 to 1355) in 3 batches...
Rank 3: Processing 678 texts (indices 2034 to 2711) in 3 batches...
Rank 5: Processing 678 texts (indices 3390 to 4067) in 3 batches...
Rank 4: Processing 678 texts (indices 2712 to 3389) in 3 batches...
Rank 2: Processing 678 texts (indices 1356 to 2033) in 3 batches...
Rank 7: Processing 678 texts (indices 4746 to 5423) in 3 batches...
Rank 0: Processing 678 texts (indices 0 to 677) in 3 batches...
Rank 6: Batch  1 / 3, Shape: [256, 768], Time: 87ms
Rank 1: Batch  1 / 3, Shape: [256, 768], Time: 88ms
Rank 2: Batch  1 / 3, Shape: [256, 768], Time: 89ms
Rank 5: Batch  1 / 3, Shape: [256, 768], Time: 89ms
Rank 3: Batch  1 / 3, Shape: [256, 768], Time: 90ms
Rank 4: Batch  1 / 3, Shape: [256, 768], Time: 95ms
Rank 7: Batch  1 / 3, Shape: [256, 768], Time: 95ms
Rank 0: Batch  1 / 3, Shape: [256, 768], Time: 226ms
Memory: Rank 0: 11.16 GB | Rank 1: 11.56 GB | Rank 2: 11.46 GB | Rank 3: 11.47 GB | Rank 4: 11.54 GB | Rank 5: 11.56 GB | Rank 6: 11.51 GB | Rank 7: 11.48 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 4: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 6: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 5: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 1: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 2: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 7: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 0: Batch  2 / 3, Shape: [256, 768], Time: 2s
Rank 4: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 3: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 6: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 5: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 1: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 0: Batch  3 / 3, Shape: [166, 768], Time: 2s
Total embeddings: 5424
Original texts: 5421
Expected padding: 3
Actual padding: 3
Rank 2: Batch  3 / 3, Shape: [166, 768], Time: 2s
Rank 7: Batch  3 / 3, Shape: [166, 768], Time: 2s
Maximum difference between padding embeddings: 0.0
Padding embeddings verified as very similar.
Encoding completed (6s)

Displaying samples from Evaluation data:
Evaluation[257]: The director , Steven Shainberg , has succeeded by focusing intently on his characters , making them quirky individuals rather than figures of fun . - POSITIVE
Tokens: ['the', 'director', ',', 'steven', 'sha', '##in', '##berg', ',', 'has', 'succeeded', 'by', 'focusing', 'intently', 'on', 'his', 'characters', ',', 'making', 'them', 'qui', '##rky', 'individuals', 'rather', 'than', 'figures', 'of', 'fun', '.']
Embedding: [-0.12061016  0.11890399 -0.11705556 -0.40881985 -0.0739258  -0.00200709] ...

Evaluation[186]: The kind of nervous film that will either give you a mild headache or exhilarate you . - NEUTRAL
Tokens: ['the', 'kind', 'of', 'nervous', 'film', 'that', 'will', 'either', 'give', 'you', 'a', 'mild', 'headache', 'or', 'ex', '##hila', '##rate', 'you', '.']
Embedding: [-0.29576513 -0.00624341  0.03614863 -0.3884102   0.07588808  0.23328759] ...

Evaluation[1301]: Human Nature initially succeeds by allowing itself to go crazy , but ultimately fails by spinning out of control . - NEUTRAL
Tokens: ['human', 'nature', 'initially', 'succeeds', 'by', 'allowing', 'itself', 'to', 'go', 'crazy', ',', 'but', 'ultimately', 'fails', 'by', 'spinning', 'out', 'of', 'control', '.']
Embedding: [-0.18056896  0.14240852 -0.0251379  -0.6127703  -0.16722038  0.12992625] ...


Encoding Evaluation data of 2210 texts distributed across 8 GPUs...
Batch Size: 256, Pooling: Mean, Empty Cache: False
Padding Evaluation data to 2216 texts for even distribution across 8 ranks...
Texts per rank: 277, Total batches: 9
Rank 7: Processing 277 texts (indices 1939 to 2215) in 2 batches...
Rank 2: Processing 277 texts (indices 554 to 830) in 2 batches...
Rank 3: Processing 277 texts (indices 831 to 1107) in 2 batches...
Rank 6: Processing 277 texts (indices 1662 to 1938) in 2 batches...
Rank 5: Processing 277 texts (indices 1385 to 1661) in 2 batches...
Rank 1: Processing 277 texts (indices 277 to 553) in 2 batches...
Rank 4: Processing 277 texts (indices 1108 to 1384) in 2 batches...
Rank 0: Processing 277 texts (indices 0 to 276) in 2 batches...
Rank 6: Batch  1 / 2, Shape: [256, 768], Time: 86ms
Rank 2: Batch  1 / 2, Shape: [256, 768], Time: 89ms
Rank 5: Batch  1 / 2, Shape: [256, 768], Time: 90ms
Rank 3: Batch  1 / 2, Shape: [256, 768], Time: 90ms
Rank 1: Batch  1 / 2, Shape: [256, 768], Time: 93ms
Rank 7: Batch  1 / 2, Shape: [256, 768], Time: 97ms
Rank 4: Batch  1 / 2, Shape: [256, 768], Time: 99ms
Rank 0: Batch  1 / 2, Shape: [256, 768], Time: 209ms
Memory: Rank 0: 11.16 GB | Rank 1: 11.97 GB | Rank 2: 11.87 GB | Rank 3: 11.87 GB | Rank 4: 11.95 GB | Rank 5: 11.96 GB | Rank 6: 11.91 GB | Rank 7: 11.89 GB (Max: 16.93 GB, Total: 135.43 GB)
Rank 3: Batch  2 / 2, Shape: [21, 768], Time: 2s
Rank 6: Batch  2 / 2, Shape: [21, 768], Time: 2s
Rank 4: Batch  2 / 2, Shape: [21, 768], Time: 2s
Rank 1: Batch  2 / 2, Shape: [21, 768], Time: 2s
Rank 5: Batch  2 / 2, Shape: [21, 768], Time: 2s
Rank 2: Batch  2 / 2, Shape: [21, 768], Time: 2s
Rank 7: Batch  2 / 2, Shape: [21, 768], Time: 2s
Rank 0: Batch  2 / 2, Shape: [21, 768], Time: 2s
Total embeddings: 2216
Original texts: 2210
Expected padding: 6
Actual padding: 6
Maximum difference between padding embeddings: 0.0
Padding embeddings verified as very similar.
Encoding completed (2s)

Data saved to: saves/data_8_gpu_20241029-050524.npz
X Train shape: [102097, 768], X Validation shape: [5421, 768], X Test shape: [2210, 768]
y Train shape: [102097], y Validation shape: [5421], y Test shape: [2210]
Data processed (2m 18s)

Initializing DDP Neural Classifier...
Layers: 2, Hidden Dim: 1024, Hidden Act: SwishGLU, Dropout: 0.3, Optimizer: AdamW, L2 Strength: 0.01, Pooling: MEAN, Accumulation Steps: 1, Max Grad Norm: None
Batch Size: 256, Max Epochs: 123, LR: 1e-05, Early Stop: score, Fine-tune BERT: False, Fine-tune Layers: 1, Freeze BERT: False, Target Score: None, Interactive: True
Loading model from: checkpoints/checkpoint_epoch_77_20241029-041736.pth...
Loaded checkpoint: checkpoints/checkpoint_epoch_77_20241029-041736.pth
Retrieved model state dictionary.
Retrieved optimizer state dictionary.
Classifier initialized (116ms)

Fitting DDP Neural Classifier on training data...
Num Workers: 0, Prefetch: None
Score-based early stopping enabled (macro average F1 score against validation set). Validation fraction: 0.1
Training will stop early if the score does not improve by at least 0.00001 for 20 iterations.
Using provided validation set for early stopping.
Building dataset and dataloader...
Classes: ['negative', 'neutral', 'positive'], Number of classes: 3
Initializing model, graph, optimizer...

Model Architecture Summary:
Classifier(
  (layers): Sequential(
    (0): Linear(in_features=768, out_features=1024, bias=True)
    (1): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=1024, out_features=1024, bias=True)
    (4): SwishGLU(
      (projection): Linear(in_features=1024, out_features=2048, bias=True)
      (activation): SiLU()
    )
    (5): Dropout(p=0.3, inplace=False)
    (6): Linear(in_features=1024, out_features=3, bias=True)
  )
)

Model Parameters:
  layers.0: 787,456 (trainable)
  layers.1.projection: 2,099,200 (trainable)
  layers.3: 1,049,600 (trainable)
  layers.6: 3,075 (trainable)

Total layers: 4
Trainable layers: 4
Total parameters: 3,939,331
Trainable parameters: 3,939,331
Percentage of trainable parameters: 100.00%
Using optimizer: AdamW, Use Zero: True, Base Learning Rate: 1e-05, L2 strength: 0.01
Using scheduler: CosineAnnealingWarmRestarts
Scheduler arguments:
- T_0: 5
- T_mult: 1
- eta_min: 1e-07

Starting training loop...
Start epoch: 78, Max Iterations: 123, Early Stop: score, Tolerance: 0.00001, Number Iterations No Change: 20

[Enter] to continue for 1 epoch, [Q]uit, [S]ave, [H]elp: q
Quitting training...
Saving model in ONNX format...
Model saved in ONNX format to saves/model_20241029-050615.onnx and uploaded to Weights & Biases.
Saved model state: checkpoints/final_model_20241029-050615.pth
Saved model pickle: checkpoints/final_model_20241029-050615.pkl
Training completed (36s)

Evaluating model...

B1-EB-SST Multi-Class Classification Report

              precision    recall  f1-score   support

    negative   0.735079  0.769737  0.752009       912
     neutral   0.328467  0.231362  0.271493       389
    positive   0.772681  0.833883  0.802116       909

    accuracy                       0.701357      2210
   macro avg   0.612076  0.611661  0.608539      2210
weighted avg   0.678974  0.701357  0.688039      2210

ROC AUC: 0.814551

Predicted  negative  neutral  positive
Actual                                
negative        702      118        92
neutral         168       90       131
positive         85       66       758

Saved predictions: saves/predictions_20241029-050616.csv

Macro F1 Score: 0.61

Evaluation completed (679ms)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: eval/macro_f1_score ▁
wandb: 
wandb: Run summary:
wandb: eval/macro_f1_score 0.60854
wandb: 
wandb: 🚀 View run B1-EB-SST at: https://wandb.ai/jimbeno/Electra%20Base/runs/im59p4eq
wandb: ⭐️ View project at: https://wandb.ai/jimbeno/Electra%20Base
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20241029_050315-im59p4eq/logs
TOTAL Time: 3m 13s
(nlp) ubuntu@104-171-203-59:~/nlp-test/sentiment$ client_loop: send disconnect: Broken pipe
(base) Jims-MBP:electra_base jim$ ssh ubuntu@104.171.203.59
^C
(base) Jims-MBP:electra_base jim$ ssh ubuntu@104.171.203.59
ssh: connect to host 104.171.203.59 port 22: Operation timed out
(base) Jims-MBP:electra_base jim$ 
